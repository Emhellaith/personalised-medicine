Personalised medicine
Imagine being able to get a faster diagnosis of a condition based on your unique situation, be given personalised treatments based on what would be most effective for you and experience fewer, if any, side effects….or even move away from simply managing an illness once you are sick to promoting health by predicting certain conditions and preventing them developing in the first place.

This is the basis for personalised medicine and by understanding the role our DNA plays in our health, it can help us transform how we think about our healthcare and help us deliver the four Ps:

Prediction and Prevention of disease
More Precise diagnoses
Personalised and targeted interventions
A more Participatory role for patients
By combining and analysing information about our genome, with other clinical and diagnostic information, patterns can be identified that can help to determine our individual risk of developing disease; detect illness earlier and determine the most effective interventions to help improve our health; be they medicines, lifestyle choices, or even simple changes in diet.

This can help us move away from simply managing an illness and instead focusing on promoting health.

The concept of personalised medicine is not new. Clinicians have been working to personalise care, tailored to people’s individual health needs, throughout the history of medicine, but never before has it been possible to predict how each of our bodies will respond to specific interventions, or identify which of us is at risk of developing an illness. New possibilities are now emerging as we bring together novel approaches, such as whole genome sequencing, data and informatics, and wearable technology. It is the interconnections between these innovations that makes it possible to move to an era of truly personalised care.

Through the 100,000 Genomes Project, and now the NHS Genomic Medicine Service, the NHS is building partnerships with academia and industry to decode the human genome, in people with rare diseases and cancer and realise the potential of personalised medicine. This will help to predict the future development of disease, to make a diagnosis where one has not existed previously and to identify treatments where possible. Please see the genomics page for further information.

Source 
https://www.england.nhs.uk/healthcare-science/personalisedmedicine/

------end---------

How personalised medicine will transform healthcare by 2030: the ICPerMed vision

Abstract
This commentary presents the vision of the International Consortium for Personalised Medicine (ICPerMed) on how personalised medicine (PM) will lead to the next generation of healthcare by 2030. This vision focuses on five perspectives: individual and public engagement, involvement of health professionals, implementation within healthcare systems, health-related data, and the development of sustainable economic models that allow improved therapy, diagnostic and preventive approaches as new healthcare concepts for the benefit of the public. We further identify four pillars representing transversal issues that are crucial for the successful implementation of PM in all perspectives. The implementation of PM will result in more efficient and equitable healthcare, access to modern healthcare methods, and improved control by individuals of their own health data, as well as economic development in the health sector.

Personalised medicine (PM) represents an exciting opportunity to improve the future of individualised healthcare for all citizens (citizen herein equivalent to individuals in the society, reflecting the inclusive and fair nature of PM approaches), holding much promise for disease treatment and prevention. There are high expectations for the future, but will PM and its accompanying tools and approaches change healthcare and be widely implemented for the benefit of society and its citizens by 2030? Will scientists, innovators, healthcare providers, and others be able to provide the most suitable medicine, at the right dose, for the right person, at the right time, at a reasonable cost? Will the healthcare sector be able to find the incentives and create appropriate financial models to implement PM in daily clinical practice? These are questions that require immediate attention and coordinated action to achieve the goal of the comprehensive implementation of PM already by 2030.

The International Consortium for Personalised Medicine (ICPerMed) [1] believes that advancement of the biomedical, social, and economic sciences, together with technological development, is the driving force for PM. Strong investment in research and innovation is therefore a prerequisite for its successful implementation. Here, we present our vision of how PM will lead to the next generation of healthcare by 2030. Through five main perspectives, our vision affirms PM as a medical practice centred on the individual’s characteristics, leading to improved effectiveness of diagnostics, treatment and prevention, added economic value, and equitable access for all citizens.

ICPerMed envisages healthcare within the five core perspectives, further delineated in our white paper [2], to be implemented by 2030 as follows:

Perspective 1: Informed, empowered, engaged, and responsible citizens

Health-related data is controlled by the citizen, including input, monitoring, and access.

Easily accessible, reliable, and understandable sources of medical information are available.

Perspective 2: Informed, empowered, engaged, and responsible health providers

The safe, responsible, and optimal use of health information and research results required for PM is routine in clinical settings.

Clinical decisions requiremultidisciplinary teams, integrating novel health-related professions.

The education of healthcare professionals has adopted the interdisciplinary aspects of PM.

Clinicians and researchers work closely to support the rapid development and implementation of PM solutions.

Perspective 3: Healthcare systems enable personally tailored, optimised health promotion and disease prevention, diagnosis, and treatment for the benefit of patients

Equitable access to PM services for all citizens is a reality.

PM services are optimised in terms of effectiveness and equity.

The allocation of resources within healthcare systems is consistent with societal values.

Secure health data flow from citizens and healthcare systems to regulatory authorities and research is in place.

Perspective 4: Available health-related information for optimised treatment, care, prevention, and research

Personal data in electronic health records (EHRs) is used by healthcare providers and researchers for more efficient PM.

Harmonised solutions to ensure data privacy, safety, and security are applied in health-data management.

Optimised treatment and prevention based on personal data benefit citizens, while minimising costs and risks.

Perspective 5: Economic value by establishing the next generation of medicine

A reasonable balance between investment, profit, and shared benefit for the citizen is a reality for PM.

Appropriate business concepts and models are in place for PM.

Telemedicine and mobile solutions promote PM and are of economic value.

New jobs in healthcare systems are created.

Data and technology.

By 2030, digital technology is a ubiquitous enabler of all aspects of society, including the health and well-being of citizens. Attitudes towards digital technology and data sharing have changed, driven by a new generation for whom digital technology and social networks are fully integrated in daily life. These citizens are more empowered to control their health data than those of previous generations, and thus more engaged in healthcare decisions and data sharing for research. Adequate regulatory frameworks and data management protocols for the protection of personal rights are compliant with international state-of-the-art standards addressing data security, accessibility, storage, and curation.

Comprehensive personal health data is, in 2030, available through EHRs. The widespread use of wearable devices and apps allows continuous and real-time tracking of health parameters and behaviours, which is complemented by biomarker technology. The global efforts to understand genomic variation in millions of individuals allow the definition of individual genomic risk profiles associated with common diseases, placing greater emphasis on prevention. Other levels of biological information, including epigenomics, proteomics, and metabolomics complement genomic-risk estimates and provide monitoring tools for individuals at risk for disease. Data generation is continuously evolving, requiring innovative and flexible information and communication technology (ICT) solutions to address the needs of PM models for data storage, management, access, safety, and sharing. Interoperability and harmonisation concepts are embedded in healthcare and research systems through more homogeneous data collection tools. Significant investments in artificial intelligence methods by 2030 lead to novel and efficient integration and interpretation of multilevel data from a wide range of sources. Finally, creative and trustworthy ICT solutions are available to support clinical decisions by healthcare providers at the point-of-care.

Inter-sectoral synergies.

In the ICPerMed vision for 2030, strong synergies between healthcare and research are crucial for the application of PM. Large volumes of routine healthcare data provide a rich source of material for research, allowing patient stratification and the definition of individual profiles and supporting adapted clinical trials. A close alignment between healthcare providers, researchers, and patients, together with improved flexibility of healthcare systems, enables end user-driven biomedical and clinical research and supports the rapid assimilation of research results by the clinic. The healthcare systems of 2030 support research to strengthen the evidence base of novel PM strategies, effectiveness, and value.

Other parameters influencing health outcomes, including lifestyle and behaviour, socio-economic status, employment, and environmental exposure are integrated with personal health and biomarker data. Acknowledging the impact of policies from other sectors enables valuable inter-sectoral synergies, particularly for health promotion and disease prevention.

In 2030, synergies with the private sector are driven by the need for rapid technological progress, along with novel business opportunities and models. PM drives innovation, particularly in areas such as digital technology, biomarker detection, and the development of molecular-targeted drugs. Through close cooperation with the pharma industry, data from clinical trials is available to the medical community, improving patient access to innovative medicines. Health technology assessment clarifies the true value of technologies, incentivizing PM.

Healthcare system reforms.

By 2030, the primary focus of healthcare has shifted from treatment to risk definition, patient stratification, and personalised health promotion and disease prevention strategies of particular value for ageing societies. Optimisation of healthcare systems until 2030 reflects this change. Economic sustainability and societal benefits of PM are clear and integrate a societal perspective. Economic analysis is at a systemic level, integrating unemployment, social-care systems, new risk-sharing methods, and the entire life cycle of PM approaches. This broader societal perspective is underpinned by shared ethical values and equity of access for all, including marginalised sectors and under-served populations. In 2030, adequate reimbursement models are in place to support this more equitable approach, and consider the long-term value of innovative technology-based approaches.

Significant investments in technological infrastructure and digital platforms until 2030 maximize the enormous economic value of public ownership of data and create the need for new skills and novel professional profiles. Health professionals trained in digital technologies, biomarker examination, and data analysis are members of multidisciplinary teams that make shared clinical decisions. Healthcare systems use flexible working models to accommodate individual needs and incorporate the rapid turnover of technological and scientific innovations streaming from research, and bidirectional data accessibility is facilitated by networking and data-sharing platforms.

Education and literacy.

Major changes in medical and other healthcare provider curricula (e.g. pharmacists, nurses, and therapists) result in a new generation of informed, empowered, engaged, and responsible healthcare providers by 2030. There is a strong focus on digital literacy and the skills needed to interpret biomarker information. The value of multidisciplinarity in clinical and healthcare decisions is routinely used in practice. Given the fast turnover of technologies and their potential impact on healthcare, lifelong education and training is essential for healthcare providers. Conversely, professionals with non-clinical backgrounds have a better understanding of healthcare and clinical issues, facilitating interactions amongst clinical teams.

For the citizen, health data education and literacy in PM, including ethical, regulatory, and data-control issues, is provided through schools and specific literacy programs. Improved PM literacy is complemented by interfaces capable of providing required rigorous information on demand while preserving the patient-clinician interaction.

In 2030, healthcare managers and policy makers have ample evidence of the benefits of PM to citizens and healthcare systems. This enables the establishment of political frameworks to tackle effectiveness, efficiency, equity, and ethical issues underlying the development and implementation of PM approaches.

Conclusions
PM is not so much a paradigm change as the evolution of medicine in a biotechnology and data-rich era. This development requires extensive adjustments in the way healthcare is provided, including new skills for healthcare professionals and novel tools for delivery. The ICPerMed vision reflects such an evolution. It was supported by consulting European and international experts, covering key sectors, who provided feedback on the opportunities and challenges of PM and highlighted specific concerns and possible solutions [2].

ICPerMed supports coordinated research directed towards the progressive implementation of PM and has previously developed an Action Plan [3], defining research activities to stimulate the adoption of PM in healthcare. Leveraging the Action Plan, ICPerMed members have been successful in establishing PM research and healthcare programs and actions in their own countries and regions [4]. The European Commission already supports many initiatives consistent with the presented vision and, together with ICPerMed, is committed to expanding its efforts globally. The core perspectives of the ICPerMed vision and transversal issues presented herein can further orient policy makers and guide the healthcare community in their planning of future programs and activities for PM implementation. ICPerMed will continue to act as a communication platform for existing and future initiatives and organisations related to PM, paving the way towards this vision of PM in 2030.

Source: https://translational-medicine.biomedcentral.com/articles/10.1186/s12967-020-02316-w

------end---------

WHAT IS PERSONALIZED MEDICINE?

What is precision or personalized medicine?
Would a teenage boy buy the same clothes as his grandmother? Probably not. But when they get sick, they’re likely to receive the same medical treatment, despite their many differences. And so will everyone else.

That’s because even the world’s best scientists and doctors don’t fully understand yet how different people develop disease and respond to treatments. The result is a "one-size-fits-all" approach to medicine that is based on broad population averages. This traditional practice often misses its mark because each person’s genetic makeup is slightly different from everyone else’s, often in very important ways that affect health.

The advent of precision medicine is moving us closer to more precise, predictable and powerful health care that is customized for the individual patient. Our growing understanding of genetics and genomics — and how they drive health, disease and drug responses in each person — is enabling doctors to provide better disease prevention, more accurate diagnoses, safer drug prescriptions and more effective treatments for the many diseases and conditions that diminish our health.

Tailoring health care to each person’s unique genetic makeup – that’s the promising idea behind precision medicine, also variously known as individualized medicine, personalized medicine or genomic medicine.

What are the benefits of precision medicine?
Throughout history, the practice of medicine has largely been reactive. Even today, we usually must wait until the onset of diseases and then try to treat or cure them. And because we don’t fully understand the genetic and environmental factors that cause major diseases such as cancer, Alzheimer’s and diabetes, our efforts to treat these diseases are often imprecise, unpredictable and ineffective.

The drugs and treatments we devise are tested on broad populations and are prescribed using statistical averages. Consequently, they work for some patients but not for many others, due to genetic differences among the population. On average, any given prescription drug now on the market only works for half of those who take it.

Personalized medicine, because it is based on each patient’s unique genetic makeup, is beginning to overcome the limitations of traditional medicine. Increasingly it is allowing health care providers to:

shift the emphasis in medicine from reaction to prevention
predict susceptibility to disease
improve disease detection
preempt disease progression
customize disease-prevention strategies
prescribe more effective drugs
avoid prescribing drugs with predictable side effects
reduce the time, cost, and failure rate of pharmaceutical clinical trials
eliminate trial-and-error inefficiencies that inflate health care costs and undermine patient care.
How is JAX connecting genetics to medical practice?
The interactions among hundreds of genes and gene networks, along with external factors such as diet and exercise, determine our biological traits — hair color or cholesterol levels, for example — and our health status. Systems genetics is a comprehensive approach to studying and understanding this biological complexity.

Mining vast databases with powerful statistical tools, geneticists, mathematicians, physicists and computer scientists are generating new insights into how genetic complexity drives health and disease.

The Jackson Laboratory has deep expertise in systems genetics that gives us a distinctive edge in the quest to understand disease. This capacity also brings us closer to personalized medicine that’s targeted to each individual’s unique genetic composition.

Our research helped explain why some patients did not respond as expected to Gleevec, an effective drug for some forms of leukemia. It also identified an additional drug that can be used with Gleevec to potentially help these patients.

Our systems genetics group is mapping the dozens of genetic interactions that give rise to type 1 diabetes. Others are deciphering the gene network that controls levels of HDL, the "good" cholesterol, in our blood. And researchers in our Aging Center are untangling the web of genetic and environmental factors that make us grow old.

We also house and maintain the Mouse Genome Informatics database, the world’s most comprehensive collection of mouse genetic data. This global resource is essential to understanding genetic complexity not only in mice, but in humans, who are 95-98% genetically similar to mice. 


------end---------

What is precision or personalized medicine?
Would a teenage boy buy the same clothes as his grandmother? Probably not. But when they get sick, they’re likely to receive the same medical treatment, despite their many differences. And so will everyone else.

That’s because even the world’s best scientists and doctors don’t fully understand yet how different people develop disease and respond to treatments. The result is a "one-size-fits-all" approach to medicine that is based on broad population averages. This traditional practice often misses its mark because each person’s genetic makeup is slightly different from everyone else’s, often in very important ways that affect health.

The advent of precision medicine is moving us closer to more precise, predictable and powerful health care that is customized for the individual patient. Our growing understanding of genetics and genomics — and how they drive health, disease and drug responses in each person — is enabling doctors to provide better disease prevention, more accurate diagnoses, safer drug prescriptions and more effective treatments for the many diseases and conditions that diminish our health.

Tailoring health care to each person’s unique genetic makeup – that’s the promising idea behind precision medicine, also variously known as individualized medicine, personalized medicine or genomic medicine.

What are the benefits of precision medicine?
Throughout history, the practice of medicine has largely been reactive. Even today, we usually must wait until the onset of diseases and then try to treat or cure them. And because we don’t fully understand the genetic and environmental factors that cause major diseases such as cancer, Alzheimer’s and diabetes, our efforts to treat these diseases are often imprecise, unpredictable and ineffective.

The drugs and treatments we devise are tested on broad populations and are prescribed using statistical averages. Consequently, they work for some patients but not for many others, due to genetic differences among the population. On average, any given prescription drug now on the market only works for half of those who take it.

Personalized medicine, because it is based on each patient’s unique genetic makeup, is beginning to overcome the limitations of traditional medicine. Increasingly it is allowing health care providers to:

shift the emphasis in medicine from reaction to prevention
predict susceptibility to disease
improve disease detection
preempt disease progression
customize disease-prevention strategies
prescribe more effective drugs
avoid prescribing drugs with predictable side effects
reduce the time, cost, and failure rate of pharmaceutical clinical trials
eliminate trial-and-error inefficiencies that inflate health care costs and undermine patient care.
How is JAX connecting genetics to medical practice?
The interactions among hundreds of genes and gene networks, along with external factors such as diet and exercise, determine our biological traits — hair color or cholesterol levels, for example — and our health status. Systems genetics is a comprehensive approach to studying and understanding this biological complexity.

Mining vast databases with powerful statistical tools, geneticists, mathematicians, physicists and computer scientists are generating new insights into how genetic complexity drives health and disease.

The Jackson Laboratory has deep expertise in systems genetics that gives us a distinctive edge in the quest to understand disease. This capacity also brings us closer to personalized medicine that’s targeted to each individual’s unique genetic composition.

Our research helped explain why some patients did not respond as expected to Gleevec, an effective drug for some forms of leukemia. It also identified an additional drug that can be used with Gleevec to potentially help these patients.

Our systems genetics group is mapping the dozens of genetic interactions that give rise to type 1 diabetes. Others are deciphering the gene network that controls levels of HDL, the "good" cholesterol, in our blood. And researchers in our Aging Center are untangling the web of genetic and environmental factors that make us grow old.

We also house and maintain the Mouse Genome Informatics database, the world’s most comprehensive collection of mouse genetic data. This global resource is essential to understanding genetic complexity not only in mice, but in humans, who are 95-98% genetically similar to mice. 



------end---------

Advances in computational modelling for personalised medicine after myocardial infarction

Myocardial infarction (MI) is a leading cause of premature morbidity and mortality worldwide. Determining which patients will experience heart failure and sudden cardiac death after an acute MI is notoriously difficult for clinicians. The extent of heart damage after an acute MI is informed by cardiac imaging, typically using echocardiography or sometimes, cardiac magnetic resonance (CMR). These scans provide complex data sets that are only partially exploited by clinicians in daily practice, implying potential for improved risk assessment. Computational modelling of left ventricular (LV) function can bridge the gap towards personalised medicine using cardiac imaging in patients with post-MI. Several novel biomechanical parameters have theoretical prognostic value and may be useful to reflect the biomechanical effects of novel preventive therapy for adverse remodelling post-MI. These parameters include myocardial contractility (regional and global), stiffness and stress. Further, the parameters can be delineated spatially to correspond with infarct pathology and the remote zone. While these parameters hold promise, there are challenges for translating MI modelling into clinical practice, including model uncertainty, validation and verification, as well as time-efficient processing. More research is needed to (1) simplify imaging with CMR in patients with post-MI, while preserving diagnostic accuracy and patient tolerance (2) to assess and validate novel biomechanical parameters against established prognostic biomarkers, such as LV ejection fraction and infarct size. Accessible software packages with minimal user interaction are also needed. Translating benefits to patients will be achieved through a multidisciplinary approach including clinicians, mathematicians, statisticians and industry partners.

Source: https://heart.bmj.com/content/104/7/550

------end---------

Stratified, precision or personalised medicine? Cancer services in the ‘real world’ of a London hospital
Abstract
We conducted ethnographic research in collaboration with a large, research-intensive London breast cancer service in 2013–2014 so as to understand the practices and potential effects of stratified medicine. Stratified medicine is often seen as a synonym for both personalised and precision medicine but these three terms, we found, also related to distinct facets of treatment and care. Personalised medicine is the term adopted for the developing 2016 NHS England Strategy, in which breast cancer care is considered a prime example of improved biological precision and better patient outcomes. We asked how this biologically stratified medicine affected wider relations of care and treatment. We interviewed formally 33 patients and 23 of their carers, including healthcare workers; attended meetings associated with service improvements, medical decision-making, public engagement, and scientific developments as well as following patients through waiting rooms, clinical consultations and other settings. We found that the translation of new protocols based on biological research introduced further complications into an already-complex patient pathway. Combinations of new and historic forms of stratification had an impact on almost all patients, carers and staff, resulting in care that often felt less rather than more personal.

Introduction
Stratified medicine is a term that has been widely used since the 1990s in relation to genomics and subsequently other fields of biology. It is a form of medicine that sorts a population into the most biologically appropriate groupings to determine the optimal therapeutic response. However, this approach can also be described both as personalised and as precision medicine. The former term has been adopted by NHS England for their forthcoming Strategy (2016) and the latter in the USA in President Obama's Precision Medicine Initiative (2015). Both these government initiatives stress how such developments will replace the previous ‘one size fits all’ or ‘trial and error’ approach to medicine and health care in the same way as proponents of stratified medicine. Precision medicine builds on the finer sub-classification of disease to add repeated monitoring of disease markers to enable recursive tailoring of treatment to individual response. Personalised medicine can be used to incorporate both the more precise biological stratification and a holistic approach, which accords a role to patient participation and preference (Cribb and Owens 2010). As Tutton (2012, 2014) has emphasised, consideration of the dual biological and social aspects of health care has a long history in the UK, which may explain why the term ‘personalised medicine’ is most commonly used. In this article, we use stratified medicine as a cover term as it is more inclusive of other forms of biological differentiation which pre-date the newer terms; we use the terms precision and personalised medicine only when they are specifically intended.

Stratified medicine is strongly supported by the UK government through funding for the life sciences and drug development as well as the National Health Service (NHS). Thus, the UK Medical Research Council's 2014–2019 research strategy emphasises the possibilities of stratified, personalised and precision medicine and the Secretary of State for Health announced Genomics England in 2013, explaining:
The UK will become the first ever country to introduce this technology in its mainstream health system – leading the global race for better tests, better drugs and above all better, more personalised care to save lives.

(Genomics England 2013)
In order to explore the translation of stratified medicine into existing systems and the possibilities of a more personalised care, we focused on cancer services where some diseases are now subdivided into several biological groupings. For example, 10 or more types of breast cancer were described in 2012 (Curtis et al. 2012), confirming that breast cancer is a heterogeneous disease at the genetic level. These types respond to different treatments, and experimental and translational cancer research are woven together with standard cancer care in ways that have become more complex since the ‘-omics revolution’. (Keating and Cambrosio 2007, 2011, 2012a, 2012b)

Breast tumours are staged according to their size and anatomical spread, and graded by histological appearance; molecular typing improves estimates of prognosis and the likely benefits of different therapies. Subtyping includes histological grade, the expression of hormone receptors (oestrogen and progesterone) and the amplification status of the HER2 gene. Additional molecular tests may be performed to distinguish subtypes that might benefit from different therapies, and combination therapies are used to address the intransigent issues surrounding drug resistance. Yeo et al. emphasise the improved survival rates associated with new targeted therapies and conclude, ‘We are on the brink of an era of diverse molecular stratification of breast cancer, and the development of increasingly personalised medicine’ (Yeo et al. 2014: 4).

What counts as stratified medicine is contested. Tamoxifen, a hormonal therapy initially licensed in the UK to treat advanced breast cancer in 1972, is not commonly included because it is not based in genomics. It has been used widely since the 1980s for oestrogen receptor positive (ER+) cancers but, as 80% of breast cancers are ER+, further profiling is also required to inform treatment strategies. Similar problems arise with even the best-known examples of a stratified (genomic) approach. About 15% of breast cancers have amplification of the HER2 gene (Yeo et al. 2014) and trastuzumab (brand name Herceptin), a monoclonal antibody, is routinely given to improve disease-free survival. The use of HER2 as a biomarker has become a key stratifying diagnostic, which is intrinsic to decisions about therapy. This biomarker reflects somatic change in the tumour and is targeted by trastuzumab, but it is not inherited by transmission of a germ-line mutation, as is the case with BRCA1 and BRCA2 and so its status in the new stratified medicine has also been disputed (Hedgecoe 2005).

Since the draft sequence of the Human Genome Project, scholars and commentators have explored the hope, promise and anticipation as well as the scepticism surrounding personalised genomics and its institutionalisation in health care. Social research has explored the role of speculation and of expectations that might bring the future into the present along with the necessary collaborations, resources and infrastructure to deliver this new medicine. Some have explored this in relation to increasing the markets for treatments (Gabe et al. 2012), introducing the notion of surplus health (Dumit 2012), including the promise of modern genomics in relation to the increasing prominence of bio-sociality and labour market opportunities in outsourced clinical trials (Fortun 2008, Sunder Rajan 2006). Others have looked at health care in relation to the move from ‘bench to bedside’, observing that knowledge and practice travel in more than one direction, enrol companions along the way and produce, at least from the perspective of researchers and industry, unanticipated results (Hedgecoe 2006, 2008). These contributions have illuminated developments in industry and marketing, including the growth of online genetic testing, new companion diagnostics associated with targeted therapies (‘theranostics’) and shifting medical aetiologies. Less sociological attention has been devoted to the mundane business of integrating new practices of medicine within existing services, although Hedgecoe (2004, 2008) has explored the use of pharmacogenetics in the clinic, including genetic tests.

Some research-intensive units, where statified medicine is developed and implemented, have below-average ratings in patient experience surveys (see for example Quality Health 2013). We undertook a small pilot evaluation of patients’ experience in a large, research-intensive cancer unit. We analysed survey responses from 777 patients and carried out 25 hours of observation in the services, observing and talking to 28 patients, 14 companions and 10 staff. We were struck by the complexity of patients’ journeys, involving multiple and repeated visits to different elements of the service for detailed diagnosis, monitoring and treatment. Problems arose when the ‘system’ did not work in drawing all these elements together and patients were left feeling vulnerable and wondering, ‘who is thinking of me when I am not here?’

On the basis of this pilot, we hypothesised that stratified medicine might promote fragmentation and depersonalised care while at the same time delivering more biologically precise treatment. We further hypothesised that this situation resulted from medical practices that amplified concurrent developments intensifying the stratification of the workforce and the market. We therefore designed a study to examine how recent developments were embedded in a large London breast cancer service in the light of sociological attention to the -omics revolution as well as the translation of new medical knowledge and practices.

Methods
We conducted an ethnographic study to explore the translation of stratified medicine to the ‘shop floor’ of cancer services in a research-intensive London hospital. Following the initial pilot, we researched breast cancer, the largest tumour group, combining observation with interviews from 2013 to 2014. Staff and patients contributed to the study design, and the study included interviews of patients and staff about their experiences. The study had ethical and local regulatory approval (London, City and East Research Ethics Committee 12/LO/0685, Imperial College London Joint Research Office).

The study was advertised through staff meetings, posters and leaflets in the hospital site and two local non-government organisations (NGO). We focused our observations on a large outpatient department (clinic) that acted as a hub, bringing together many specialised staff, patients, carers, researchers and NGO workers such as Macmillan Cancer Support staff. This clinic also provided the main site for recruiting research participants to interview. Patients were informed about the study by clinical staff and introduced to researchers on site or asked to contact the research team by e-mail or phone. Eligible patients (≥ 18 years of age and receiving treatment or follow-up care for breast cancer) were provided with information about the study by clinical staff and through publicity, and those who volunteered to participate in interviews were asked for their written consent. They were then invited to complete two interviews, 3–6 months apart. Patients who were interviewed were also asked to name people who had been central to their care, ‘key individuals’ (who may have been companions, family, friends or NGO or healthcare staff), and to approach them with invitations to participate in the study. Staff were also recruited through advertising posters and staff meetings. These other participants (key individuals) were asked for their consent in the same way as the patients but to a single interview. We interviewed the patients, carers and staff about their own experiences of the cancer services using minimal structured prompts from a topic guide.

Observations and interviews were carried out by four researchers (SD, LM, CS and HW), who kept extensive field notes of the observations and conversations. The interviews were audio-recorded, transcribed and organised using NVivo. Data were coded and checked for consistency and comprehensiveness by four members of the team, who also cross-referenced interview material to field notes by date and theme so as to construct a single data set.

Key themes were identified and discussed in monthly team meetings before coding, enabling a shared foundation for continuing fieldwork. Where appropriate, material was also discussed and checked with staff in the breast cancer service. The integration of these varied data enabled fuller interpretation: interview material was extended by observations of participants across different settings, while fieldwork notes were elaborated in the light of individual views and explanations. The analytic approach is common to ethnographic studies insofar as it frames what people say they do in relation to what they are observed to do (Bernard 2006), acknowledging the influence that social positions will have on differing perspectives of a wide range of clinical, administrative and other activities.

Results
We observed 75 half-day sessions in the cancer services; most were in a large outpatient department (OPD, clinic) but we also spent time in the chemotherapy unit and in multidisciplinary team meetings. In total 33 patients were recruited and 23 were re-interviewed between February 2013 and April 2014.1 Re-interviews allowed us to check our original information and to extend our understanding of the patient's journey in a situation of increasing rapport. We also interviewed 23 key individuals, again with their written consent. Most of these 79 interviews lasted 50–70 minutes and took place in the hospital or offices of a neighbouring NGO; two were conducted at the participant's home. Patients participating in interviews ranged in age from 38 to 79 years (median: 58.7 years) and most were white (n = 26). Most were being treated for the first occurrence of breast cancer (n = 22) and had begun treatment fewer than 5 years previously (n = 23). Key individuals participating in interviews included eight family/friends; 14 healthcare staff: nurses, doctors and ancillary staff; and one NGO worker.

We found that the terms, stratified, precision and personalised medicine were used rarely. In order to assess the translation of stratified method into routine care, therefore, we begin with the meetings at which these terms were introduced and discussed before turning to understandings of the patient pathway or journey. We use ‘HER2’ as an exemplar of the new approach in clinical consultations that we observed.

Stratified medicine: the promise of genomics
There is so much going on, like building cathedrals in the Middle Ages. Molecular science is so exciting.

(Oncologist, 2013)
Direct reference to stratified medicine was observed spontaneously only in meetings, some of which included events for patients and the public. In these meetings, developments in biology provided the foundations of a vision that would lead to better treatment and care, indicating how references to biology implicate other matters (Hacking 2007). An oncologist and cancer scientist explained the future for a new centre at one such event, pointing to facilities for genome sequencing and drug testing. He emphasised how critical this multidisciplinary perspective had been to the development of new drugs and methods of detection, as indicated by our paraphrased field notes:
New translational research from genome studies of the code is critical…. This is very important because cancer is strictly Darwinian. People die not because they're not sensitive to treatment… 90% of women will respond. Women die however because tiny numbers of cells in any cancer are dislodged into the blood stream, and mutations in DNA overcome the blockage imposed by the drug. Now the drugs exist to kill 99.9% of sensitive cells but the remaining 0.1% cells survive. It's not the drugs that create mutations; they were there from the beginning. Previously, you needed a lump of tissue to sequence a genome. Soon you will have the whole genome sequenced commercially from a single cell.

The speaker described how selective drugs (unlike standard chemotherapy) inhibit specific proteins in the metabolic pathways associated with cancers. However, a small number of pre-existing mutations will be drug resistant and so the therapy will select for their growth. Later on, the speaker provided an illustration from work on BRCA proteins, underscoring the importance of heterogeneity in cancers. In answer to a question, he explained that inhibition of the enzyme that helps mend broken bits of DNA should have led the cancer cells to die. However, they grew out with this inhibitor because the sub-clone had been there from the beginning, and so the cancer cell survived. He continued:
Should we add extra drugs to the cocktail? Now, we guess and we use three or four drugs at the outset. It will take another five years, perhaps a total period of 15 years, to develop combination treatments at the outset that are more than guesswork. Cancer is so clever but we will be able to overcome it eventually so that it will become more like diabetes. We'll sequence in a blood test for an emerging clone. [He describes the current situation and continues] If we can, we'll turn cancer from a very frightening and aggressive illness to a chronic illness that we monitor before the lumps ever appear. This is more than a vision.

(Oncologist, 2013; paraphrased notes from meeting)
This promissory vision that will turn breast cancer into a more benign condition diagnosed by a blood test and treated before ‘lumps’ form has been extensively analysed in relation to contemporary developments in biocapital as well as the sociology of speculation (see Dumit 2012, Fortun 2008, Gabe et al. 2012, Sunder Rajan 2006: see also Good 2001). It is important to appreciate, too, how this optimism enrols staff as well as patient participation, and motivates career choices and subsequent practice. We found that many healthcare workers responded to the new science with reflections on their own encounters with illness. Subsequently, some specialised in cancer care or research, while others volunteered to work with patients. Another inspiring account from an oncologist, again for non-specialists, indicates the attraction of this atmosphere; here speaking of the earlier introduction of tamoxifen in the way that oncologists speak of trastuzumab or similar drugs today:
I became interested in endocrine therapy when I qualified…. [A] patient, who had been there for 6 months with mental health issues, was physically sick. This turned out to be breast cancer but no-one had examined her. In those days, there were no physician oncologists and oncology was a Cinderella specialty… And so I became interested in oncology and especially in breast cancer where there was a new drug on the block called tamoxifen. This we gave to her and she got better. The breast cancer melted away and she recovered mentally too because the hormone that the cancer cells were producing was gone. And she returned home and to a normal life…. There is so much going on, like building cathedrals in the Middle Ages. Molecular science is so exciting.

(Oncologist, 2013; paraphrased notes from meeting)
The presentation of a new biology that promised a better future was typical of a wide range of meetings we attended, and this vision is congruent with the developing NHS England Strategy for Personalised Medicine.

Stratified medicine: oncology out-patients
Stratified medicine was rarely mentioned in the clinic although individuals commonly spoke of genetics and, with probing, patients referred vaguely to improved outcomes. Some spoke with relief about an initial HER2 positive diagnosis since they had confidence that trastuzumab would help, even though the prognosis is intrinsically worse than for other types of breast cancer. A senior nurse appeared equally nonplussed and said during a discussion of stratified medicine:
I didn't know what you were talking about until your colleague explained the emphasis on genomics. Medicine here and nursing just as much have always been stratified. This is nothing new.

(Field notes 2014)
On being questioned, the oncologists explained how stratified medicine was embedded in everyday practice and as ordinary as previous efforts to classify patients (Hedgecoe 2004). In this context, newly stratifying diagnostics joined older forms, linking trastuzumab and tamoxifen within the same routines. It was hard therefore to identify the specific forms of stratified medicine that derived from -omics and we selected HER2 as an exemplar of the new approach.

Patients spoke less of their particular type of cancer than of waiting. At least 2 days a week, the large OPD had three oncology clinics in breast cancer running simultaneously, involving approximately 10 doctors, five outpatient nurses, a specialist lymphoedema nurse with her own clinic, clinical nurse specialists (CNS), two phlebotomists, three reception staff and a volunteer greeting patients. Throughout our fieldwork, up to a hundred patients congregated for long periods in this OPD and interacted with these as well as other hospital and university employees, and NGO workers such as Macmillan Cancer Support staff. Patients receiving treatment might spend most of the day at the hospital, waiting to have their bloods taken, waiting to see an oncologist, waiting for their script to be made up in the pharmacy and waiting for a chair in the chemotherapy suite. They might also do all this work and leave without receiving the planned treatment but with a further appointment on the horizon. In such circumstances, some patients asked whether the delay would affect the course of a cancer that would not wait. In the context of a busy urban NHS service, waiting has many inflections: inefficiencies and bureaucratic inflexibility; triage; and also a form of care among those waiting and working together. Occupying physically the OPD, patients commented both on their appreciation of an NHS that belonged to them and also on their humiliation as they were defined, contained and put on hold by the system (Day 2015, Livingston 2012). Patients contrasted the inefficiencies of the system almost universally with what they considered the uncommon kindness, admirable skills and excellent care on the part of individual members of staff, especially with reference to their clinical expertise.

In clinical consultations, patients’ perspectives on their type of cancer occasionally surfaced during interactions with doctors; we therefore provide examples from clinical consultations. A doctor told a patient who was positive for HER2 that she had metastases on her lungs: ‘yours is an unusual case; your cancer has spread while having Herceptin. That was very unlucky’. This patient was not eligible for the new treatment trial because it would probably give her too many side effects and the consultation concentrated on resolving what might be an effect of the cancer and what might be a side effect of treatment or perhaps yet another issue. Another woman learned that she had not responded to treatment. She was taken off trastuzumab after five cycles as ‘she’ (that is, her cancer) was progressing and she had a tumour in her chest. The specialist registrar said that this very unusual: ‘Most people respond’, to which her patient replied that she wanted to be told that she'd got 20 years’ remission. An elderly woman with metastatic cancer asked this same doctor if trastuzumab could be causing her rhinitis. She explained first that she could not leave the house without tissues and then asked if the doctor could please check the tumour. It had been aching and she had been coughing. She had various side effects and, in her words, saw a spine doctor, a diarrhoea doctor and a kidney doctor, between them managing cancer in her spine, lung, spleen and kidney. She attended this OPD every 3 weeks although, she said, ‘I have never been ill’. Having lived with cancer and its treatments for many years, this woman declined further chemotherapy as, she said, ‘it only has 26–28% success anyway’. She explained to the doctor that her sister had ‘nursed her husband to death through his cancer and the chemo had only made things worse’. After some discussion, doctor and patient decided to wait for results of tumour markers before doing anything further.

These brief examples show that cancers, therapies and people do not always behave as expected, and failures caused surprise. Regular monitoring enabled interventions to be tailored recursively to a changing mix of cancer, treatment, side effects and other issues, and the new biomarkers and treatments were subject to a process of testing through trial and error just like the old. Misunderstandings were common, as the following example on the importance of monitoring indicates.

An oncologist explained the close attention given to repeated test results in the clinic one day, perhaps for my (SD) benefit and perhaps, too, for the benefit of a patient's granddaughter. During the consultation, the granddaughter, a healthcare professional, asked pointed questions to which the doctor responded; ‘first are the blood markers, second the scanning and in her case she'd had an X-ray and a spinal CT [computed tomography] scan, third the breast examination’. The marker was CA15–3, which half the patients had and which was also the best possible marker. Hers was stable: ‘if you have more of them, you've got more cell division’. The doctor said of the X-ray:
[W] ell, perhaps it's a bit misleading as it was done 6 weeks before we began treatment and so it [the shadow] would have grown before the treatment [could have] worked.

Bone scans could also mislead as they flared up when they were healing; ‘it's not a great test to order, especially if you see a junior doctor next time and they don't know this’. On examination, the doctor concluded that her tumour was softer and much less defined than it had been. He looked back over the scans and wondered if she even had a shadow in her breastbone; there was no confirmation from the CT scan, no cancer in her spine or rib, and so he was not at all convinced by this shadow in the sternum.
Did she [the patient] want to look at the scans? ‘The scans are so accurate nowadays and [of] such good resolution that they pick up everything…. [The older woman asked her granddaughter to look instead.] You can see how small this possible shadow is in relation to the rest here’. The patient was confused by the quantity of information but she heard her doctor conclude, ‘no, the lump isn't growing and all is under control’. She was not interested in the results and concluded, when she was told that she would most likely be on that treatment for the standard 2 years, ‘well, at least that means I'll be living for another 2 years’.

(Field notes 2013)
Monitoring often led to more tests in the effort to achieve greater precision, informing decisions about the next step. The sequence of steps describes a patient pathway and existing procedures have become more complex with the introduction of new practices of stratified medicine.

Stratified medicine in the clinical pathway: implications for the division of labour
The epistemic, political, and economic status of the cancer patient within protocols and the protocol-production process has been a recurring theme for both patients and practitioners, especially when the time comes to choose which road to take – which path to follow or decision to make – in that embedded series of protocols commonly referred to as the therapeutic process.

(Keating and Cambrosio 2007: 215)
Continued monitoring can modify treatment: cancers resist or adapt and so treatment must co-evolve to stratify and calibrate disease differently over time. The new medicine is embedded in trials that span the world and have introduced protocols that dictate procedures seen as pathways (Berg 1998, Timmermans and Berg 1997). Reciprocally, as Keating and Cambrosio (2007: 215) assert, ‘All patient pathways converge sooner or later on a protocol’. They continue:
Presently, even though few adult cancer patients actually participate in clinical trials, virtually all of them are diagnosed, treated, and advised according to a protocol, be it a routine or an experimental protocol.

The concept of pathways arose in the 1980s. Pathways not only plot the steps to caring for and treating patients but also a range of other requirements: they guide costing and rationing, workflow and performance management as well as clinical practice (Rotter et al. 2010). From a service perspective, the pathway is a standard built from evidence-based practice which is addressed to groups of patients, conditions or molecular activities (strata) that are both more and less than individuals (Zuiderent-Jerak et al. 2012). For most staff, pathways were peopled with abstract as well as particular patients, along with their most predictable variations, who figured in a host of requirements that were glossed simply as standards. Some of these standards referred to clinical protocols and others to government requirements that set a clock ticking to a prescribed timeline and provided the basis for calculating costs and payments. For example, a ‘14-day target’ specified the approved limit for referral from a primary care physician with suspicions of cancer to an appointment in a specialist clinic; and service providers would be sanctioned if they missed this target. For patients, by contrast, the pathway represented their own journey through treatment and, in interview, few appeared to know what would or should happen next. For example, one woman spoke of the period after surgery:
This test, next test, and I had to go under that machine.. … For a while, I was coming in nearly every other day…. When you come back, you have to go to here. You need to go and have a scan. Then they phone you: ‘Oh, you have got to go and have this’. I spent every other day here…. Every day, someone's, ‘Go here, go there’.

The distance between views of the pathway helps to explain why no-one we interviewed could map the whole pathway in concrete terms onto the particular hospital site.

Staff were aware that the expansion of treatment might provide ‘adaptive’ or ‘tailored’ care and improve outcomes but they were also concerned that developments would promote differentiation of the workforce, which could affect patient care negatively. As several staff explained, stratified medicine requires specialist expertise distributed along the pathway. The extended taxonomy of breast cancers understandably lengthens a hospital line: a patient needs more tests and monitoring to find the right treatment; analysis requires more biostatistical expertise and new kinds of trials. You cannot test a stratified medicine on an unstratified population and so fewer patients are eligible for each trial; drugs are tested in combination and biological targets constantly move. At the same time, they explained, elements of the pathway can be delegated to non-specialists. The new biological medicine's protocols outline fixed steps to follow according to the best evidence and so they allow less qualified staff to simply follow instructions where processes are not yet automated, freeing experienced clinical labour to deal with problems that arise.

These observations suggest that the new biological stratification is one important factor promoting the increasing stratification of the workforce. Moreover, we observed that specialist clinical labour was directed increasingly towards the complex technical requirements of management. Services supporting the breast cancer pathway, which ideally run alongside clinical care, were provided more often by autonomous NGOs such as Macmillan Cancer Support, by volunteers and by family members.2 In practice, the increasingly elaborate division of labour can make it very difficult for patients to receive and staff to deliver the appropriate care, and any common standard (Bowker and Star 1999) for the patient pathway is in danger of unravelling. The separation between some support and medical services indicates the importance of a third form of stratification that might address the problems of integrating an increasingly fragmented pathway, at least in the view of service leads who were responsible for improving services.

Stratified or personalised care?
The doctors do not seem to understand that it is not answers but assistance that we seek.

(Patient, 2014)
In the UK at least, stratified medicine is seen as a step along the way to a fully personalised approach although, during fieldwork, references to the latter became scarce in response to the difficulties that we have described. Nonetheless, personalisation is the best known of the three terms for the new biology, as well as for other practices in health and social care. Personal budgets in social and some health care, for example, present the service user as a customer who can shop around the emerging market to choose the best package of care.3
Patients expressed frustration when they were ‘lost’ or ‘missed’ because of a lack of integration along their complex pathway. A patient might need to see a surgeon, radiologist, clinical oncologist, plastic surgeon, chemotherapy nurse and radiographer, and some complained that they rarely saw the same person twice. Although the UK NHS has long contained markets in care, it is only recently (2012–2013) that the UK Health and Social Care Act opened them to all willing providers. As a consequence, cancer services in some regions were put out for tender. The UK's main cancer charity that supports patients orchestrated one such bidding process. According to The Guardian newspaper, Macmillan Cancer Support's advisory role was grounded in the desire
to overhaul cancer care in the county to make it more joined-up after some patients complained that they were “getting lost in the system, having to repeat themselves all the time, and that care [was] not always factoring in their personal circumstances.”

(Campbell 2014, our italics.)
The charity was criticised for its possibly unwitting ideological support of privatisation when a number of firms attended their briefings. Macmillan's chief executive responded with the claim that the new programme would offer ‘an integrated approach’ and ‘[b]y appointing one organisation to take responsibility for managing the whole cancer care journey, we can demand truly seamless care’ (Devane 2014).

The problem of navigating complex systems has been made more acute by the requirements of stratified medicine with its increasing specialisation and monitoring. Support along the patient pathway has traditionally been delivered by a CNS, who meets patients at the diagnosis and offers them support during treatment. However, the increasing complexity of the pathway and a growing caseload had made this role more challenging. Survey results show that having a CNS correlates with a better patient experience (Quality Health 2013). Our research participants valued supportive relationships with CNS but some patients were not sure who this person was or whether they had a named CNS at all. One, for example, could not identify a specific CNS, referring instead to a ‘cast of thousands’ involved in her care. Another, with metastatic disease, looked blank when asked but her husband wondered if one of three particular nurses might be a CNS. All three were ‘very good most of the time’, ‘anything that's wonky, they sort it out’, but when one of them went on holiday, the couple found they were unable to reach the named alternative support. This particular couple also explained that they had been happy to liaise with the consultant's secretary, who could resolve problems when the system did not function well, but her job had disappeared during restructuring (Ward and Day 2013).

The allocation (or subcontracting) of navigation and some supportive care to non-clinical staff and external bodies such as charities constitutes a further form of stratification, leaving clinical care with the more technical and protocol-driven elements. Research participants, staff and patients alike, expressed disquiet at the impersonal nature of many clinical interactions in the same way as Macmillan and many bemoaned the lack of ‘generalist’ expertise that could hold the pathway together. Clearly, these problems have not been caused directly by the translation of biological developments into health care; complaints about the impersonal nature of bureaucracies are pervasive. However, the practices of stratified medicine have an elective affinity with both the outsourcing of particular support services and occupational specialisation, suggesting that an indirect relationship exists between developments in these different forms of stratification. As far as clinicians and patients were concerned, the spaces for holistic care have become more and more restricted.

In this context, it is worth noting a paradox recognised by a range of participants: the most personalised medicine in the sense of both experimental biology and holistic care (Tutton 2012, 2014) was found at the limits of protocol-governed treatment where there was simply no biological evidence and only minimal market demand. We encountered patients with advanced disease who were receiving palliative treatment to control their symptoms or slow down disease progression. Chang4 fell in this group and her views changed over the course of our year's fieldwork. Initially, she explained that her chances of recovery were slim because there were no treatments for her ‘triple negative’ status. An ex-nurse, she was clear that she would refuse treatment if it made her feel worse but, after some time, agreed to chemotherapy for secondary tumours. As she explained, ‘first they burnt my brain and now, there are just a few strands [of hair] here and there’. Her double vision had returned. ‘Never mind’, she said, ‘this hat keeps me warm and with the scarf too’. Now, she thought, she would stop treatment only if tests showed that the tumours were growing: there would be ‘nothing left to do’. She spoke of a referral to a hospice and for morphine syrup but said it was too early; she was not ready. Discussing the all-important results, Chang agreed that she, not the doctors, would decide:
[D]octors hope; they want to carry on treatment but they are very good… and yes, I will not have treatment unless it is working. What is the point?

Chang was on the last cycle of the treatment when we [SD] met again and then, she said, ‘it is currently a full stop’. A few days later, she joked about which doctor she would see with a fellow patient, Yuna, in the OPD. Yuna said her doctor had told her there was no answer. Chang retorted, ‘the doctors do not seem to understand that it is not answers but assistance that we seek’. Was Chang asking when the interventions to track and respond to biological pathways would cease and when someone would offer her the help she needed? If dying and living are experienced together (Das 2015), at what point should the biologically defined pathway end and how?

Although we were focusing on breast cancer, elements of the cancer pathway were shared and we met Chris during our pilot study receiving treatment for a different tumour. A few months later when we met again by chance, I [SD] asked about the treatment and Chris shrugged. It was not clear, he said, and added, ‘I'm on the last shot. Anything from now on is experimental’. Noting that ‘10 years ago, I wouldn't have had a chance’, he acknowledged that there was no clear evidence but a number of drugs to try. Chris was positive about his experimental treatment. To be treated as a unique ‘case’ meant that there were no validated biomarkers or therapies but only personalised care. This personalisation was attuned to a stratum of one but Chris, unlike Chang, considered experimentation a form of responsiveness that might encompass the biological, the financial, the bureaucratic or any other facet of what was always also his singular position. If Chang saw the biological ‘personal’ in opposition to a more holistic or social care, Chris considered them to be closely connected. He might have said, as Kit did in Iain Banks's (2013: 159) The Quarry, ‘I know Guy's cancer is not contagious; you can't catch it off him…. That's the thing about cancer; it's all yours – it's entirely, perfectly personalised’. Paradoxically, it is only when a stratum of n = 1 is defined that the biologically personalised joins the socially personalised medicine.

Discussion
We have described how stratified medicine is experienced as it is introduced in a large research-intensive cancer unit. While it is not possible to isolate the impact of translational medicine from other developments in breast cancer care through an ethnographic approach, we have identified a number of key findings. Firstly, we observed how clinicians and scientists embrace the new molecular medicine with great optimism, anticipating the successful transformation of cancer management using precise diagnosis, monitoring and therapy. Secondly, stratified medicine was seen to have placed additional strains on the service through its requirement for a highly skilled workforce and a meticulously integrated patient pathway that, in the context of budget constraints, were difficult to deliver. Highly skilled staff have moved increasingly to back office functions such as laboratory analysis, the research and testing of algorithms and the continuing development of protocols and they have been replaced in frontline functions by less qualified staff following the protocols of the new medicine. This leads into the third point; this recalibration of staff roles has enabled hospitals to trim budgets and carry on, but staff and patients alike reported increasing fragmentation and particular difficulties in coordinating the steps along a pathway. Finally, we show how measures to improve coordination and navigation, with the introduction of new roles and some external providers, do not always work, with the result that some patients describe care that is far from personalised.

If the requirements of the new medicine were not sufficiently exacting in the UK today, its realisation is complicated still further by largely contingent developments that may amplify the processes that we have observed. Elements of government, industry and research are intent on bringing a ‘better’ medicine to the patient. Personalised medicine has been considered variously in relation to New Labour, neoliberal reforms and the new austerity (Cribb and Owens 2010, European Alliance for Personalised Medicine 2013, Savard 2013) but, with its many associations, it can coincide with a wide range of regimes. New legislation mandating the tendering of services to ‘all willing providers’ enables the outsourcing of parts of the pathway. These developments, in turn, mean that services will be able to sustain the technical and biological work required by the new medicine. Stratified medicine may promise to create a person-centred hub in place of the Fordist line that patients described, but interventions towards this end already threaten to bring the pathway to a stop. Future outsourcing might resolve problems in the pathway and initiate alternative directions but it could equally intensify the direction of current developments so that staff and patients alike find themselves in pieces, scattered along the pathway and struggling to put the parts together.

Stratified medicine is contrasted favourably with a previous and less desirable ‘all-comer’ medicine. All-comer medicine fails to treat effectively because it treats uniformly and ‘one size’ is intended to ‘fit all’. The modernist notion of health care for all, cradle to grave, seems inappropriate to new biologies, burgeoning expertise and consumer preferences. In the UK, this all-comer, also known as ‘empirical’ (Hingorani et al. 2013), medicine has been practiced in a national service that met needs through centralised planning associated with bureaucratic inflexibilities and relatively effective price control. Having evolved over the last 60 years to include all manner of quasi-public and quasi-private service providers, the NHS is considered by some commentators to be too unwieldy to deliver personalised medicine, that is, a set of practices that holds patients at the centre of a pathway receiving treatment that will work on their type of cancer.

Participants in this particular cancer service were unclear about the contours of a better medicine that might integrate care with treatment and improved outcomes. Many spoke of elements from a pre-genomic, as well as the genomic, era and emphasised the importance of a form of care that delivered an inclusive service. Even though the traditional NHS form of stratification was compromised by its association with a callous bureaucracy and a one-size-fits-all form of cancer care, patients strongly supported the admission of all-comers and they emphasised how thrilled they were to be welcomed and treated themselves. However, the relations between an inclusive service on the one hand and improved outcomes on the other remained hazy. The ‘better’ market form on offer was equally tainted by association with an inappropriate or unethical profit motive and, at times, a similarly vague association between market efficiency, on the one hand, and better outcomes, on the other, was implied.

Even though participants were unclear about the path to a better medicine, several expressed concern about future lines of exclusion that would replace valued public realms, in which the NHS continues to occupy a central place, with thoroughgoing market principles (Day 2015). In the 21st century, valuation creates biological markets through inevitably evaluative processes of sorting that are currently bundled under the rubric of an -omics revolution. This revolution points to a seemingly impossible leap in cancer care where the individuation of biologies and customer preferences will coincide with numerous other, potentially divergent measures tracking costs, governance requirements, safety, participation on the part of both staff and patients, and, of course, health outcomes in a single pathway. How can value become equally an outcome and an experience, a notion of inclusion or equity and a price that government, insurers or others will pay?

Conclusion
Stratified, precision and personalised medicine constitute a field of overlapping and shifting meanings that appeal to notions of evidence, new expertise and person-centred care as well as subgroup medicine. Personalised medicine, conceived in terms of a welcome attention to the person of the patient, which is exquisitely attuned to individual genetic and – by extension – social factors, is perhaps the best known of these three terms and it is also the most fluid and contested. It remains an aspiration and, in the meantime, stratified medicine refers to and produces new forms of disease that inevitably combine with previous ones.

In one research-intensive, large, urban centre from 2013 to 2014, the translation of stratified medicine into routine care led to a combination of practices that promoted less rather than more integrated, personalised and seamless care. This result may describe a temporary phase of healthcare delivery and it cannot be attributed solely to the new biology. Nonetheless, the empirical consequences of new medical practices and the elective affinities that we have outlined between developing forms of stratification in medicine, the market and workforce suggest that the translation of -omics into standard care will have similar, unanticipated effects in other UK settings.

Expectations and the promise of biological developments are an important theme in the sociological literature. However, the focus of previous studies has tended to be on the big players – in science, industry and government, or on particular perspectives within the translational pathway. By observing and integrating the perspectives of the many different players involved with stratified medicine in health care we have shown that high expectations of the new stratified medicine are widely shared by participants who are often considered to have distinct views and interests, with health care staff on the one side and patients with their families and friends on the other. Furthermore, speculation about a better future is embedded in everyday interactions within the service. In this context, the mundane challenges we have discovered to the introduction of stratified medicine may prove difficult to reconcile with widespread optimism.

Source: https://onlinelibrary.wiley.com/doi/full/10.1111/1467-9566.12457
------end---------
Personalised medicine in Canada: a survey of adoption and practice in oncology, cardiology and family medicine

ABSTRACT
Introduction: In order to provide baseline data on
genetic testing as a key element of personalised
medicine (PM), Canadian physicians were surveyed to
determine roles, perceptions and experiences in this
area. The survey measured attitudes, practice,
observed benefits and impacts, and barriers to
adoption.
Methods: A self-administered survey was provided to
Canadian oncologists, cardiologists and family
physicians and responses were obtained online, by
mail or by fax. The survey was designed to be
exploratory. Data were compared across specialties
and geography.
Results: The overall response rate was 8.3%. Of the
respondents, 43%, 30% and 27% were family
physicians, cardiologists and oncologists, respectively.
A strong majority of respondents agreed that genetic
testing and PM can have a positive impact on their
practice; however, only 51% agreed that there is
sufficient evidence to order such tests. A low
percentage of respondents felt that they were
sufficiently informed and confident practicing in this
area, although many reported that genetic tests they
have ordered have benefited their patients. Half of the
respondents agreed that genetic tests that would be
useful in their practice are not readily available. A lack
of practice guidelines, limited provider knowledge and
lack of evidence-based clinical information were cited
as the main barriers to practice. Differences across
provinces were observed for measures relating to
access to testing and the state of practice. Differences
across specialties were observed for the state of
practice, reported benefits and access to testing.
Conclusions: Canadian physicians recognise the
benefits of genetic testing and PM; however, they lack
the education, information and support needed to
practice effectively in this area. Variability in practice
and access to testing across specialties and across
Canada was observed. These results support a need
for national strategies and resources to facilitate
physician knowledge, training and practice in PM.

INTRODUCTION
. for the sweet ones [treatments] do not benefit
everyone, nor do the astringent ones, nor are all the
patients able to drink the same things. (Hippocrates)
Personalised medicine (PM), the tailoring of medical
treatment or prevention to the individual characteristics
of each patient, has been enabled by recent advances in
molecular biology.1 Research in the ‘-omic’ sciences has
resulted in improved understanding of the relationships
between genes, proteins and disease, providing more
tools for PM2e5 and driving a shift in medical practice.6
Evidence of this ‘shift’ includes a 66% increase in
cancer-related genetic testing in Ontario between 2002
and 2008,7 and the facts that 10% of FDA approved
drugs include pharmacogenomic information on their
labels8 and genetic testing is recommended or required
for at least 11 FDA approved drugs9 and for 10 Health
Canada approved drugs (based on a review of drug
labelling using the Health Canada Drug Product Data base). A number of applications of PM based on genetic
testing are currently in use.10 Pharmacogenomics, the
optimisation of drug therapy based on genetic infor mation, has been applied to improve clinical outcomes
or reduce side effects and adverse events.11 12 Targeted
therapeutics, used in combination with companion
diagnostics, has been particularly successful in
improving treatment for cancer.13 14 Finally, PM is being
used to assess disease risk, facilitating prevention and
early detection.15
As a result of these developments, PM has become an
increasingly important topic for physicians, healthcare
organisations and the public.16 17 There is widespread
debate concerning the intended and unintended conse quences of PM for the quality and cost of healthcare;
many scientific and medical leaders expect PM to increase
the quality of healthcare and reduce overall healthcare
costs.13 18 19 A few studies have assessed the adoption of
genetic testing and its impact on the role and practice of
physicians in Canada.20e24 These studies focused
primarily on the adoption of genetic tests for the diag nosis and treatment of cancer within Ontario’s healthcare
system, and recommended physician education, public
education and improved coordination of healthcare
delivery and genetic testing services. In order to facilitate
medical and continuing professional education in PM in
Canada, it is essential to have a baseline understanding of
current knowledge, attitudes and practices.
The present pan-Canadian survey of practicing oncol ogists, cardiologists and family doctors was designed to
provide baseline data on genetic testing as a key element
of PM in Canada regarding attitudes, state of practice
and barriers to adoption. Three specialties were targeted
in the survey: cardiologists and oncologists as they
experience higher volumes and greater need for
personalised genetic testing, and family physicians as
they are usually the first point of contact for patients and
are often involved in screening for risk of disease.

METHODS
Ethics approval was received from IRB Services to survey
a sample of Canadian physicians (oncologists, cardiologists
and family physicians) regarding their knowledge, training
and practice in genetic testing and PM.
Physician contact information was obtained from
a third party for 859 oncologists and 1165 cardiologists
from across Canada. A weighted sample, based on
population, of family physicians (n¼2334) from Cana dian provinces was randomly selected from contacts with
email addresses. The self-administered survey was avail able in French and English and distributed by mail, fax
and email during the period 26 May to 15 September
2010. Respondents submitted their responses online, by
mail or by fax. Survey candidates were contacted with up
to four reminders to encourage participation. The
survey questions related to demographic information,
training, practice, knowledge and education in PM
based on genetic testing, the nature and extent of
practice in this area, and the benefits of PM and barriers
to its adoption. Questions were developed based on the
authors’ knowledge of genetic testing and PM. A draft of
the survey questions was developed based on this
knowledge and a review of the literature of previous
surveys conducted in other jurisdictions.25 26 This draft
survey was subsequently reviewed by 11 physicians (five
oncologists, three cardiologists and three family physi cians) and their feedback was incorporated into the
final survey. The survey’s design was informed by how
new technologies or innovations are adopted in practice
and a diffusion of innovations framework was consid ered.27 The survey solicited physicians’ knowledge of,
attitudes towards and practice of personalised genetic
testing to understand the relative advantages, compati bility, ease of implementation and system response to
adoption of personalised genetic testing. This is an
initial application of this framework to the Canadian
context.
Vovici software28 was used for the online survey
administration, allowing for both open-ended and close ended questions, and menu creation for selection of pre determined answer options for close-ended questions. All
questionnaires were reviewed for completeness. The data
entry protocol included separate quality review of each
survey against the entered data to ensure accuracy. Survey
results were analysed using STATA software v 11.0.29
This study was designed to be exploratory and
included analyses based on descriptive statistics and
bivariate associations. Inferential analyses were not
pursued. Answers to survey questions were compared
according to medical specialty and region or province.
Fewer responses were received from the Atlantic prov inces, Saskatchewan, Manitoba and Alberta compared to
Ontario, Quebec and British Columbia. Data from the
Atlantic provinces (Nova Scotia, New Brunswick,
Newfoundland and Prince Edward Island) were
combined and data from Saskatchewan, Manitoba and
Alberta were combined.

Due to the small number of responses for certain
questions, results with more than a 5% probability of
occurring by chance were excluded. Pearson c2 test
statistics were calculated to determine whether differ ences according to medical specialty, region or province
were statistically significant.
RESULTS
Respondent profile
A total of 363 physicians provided responses to the
survey (8.3% overall response rate). Physicians not
providing direct patient care (n¼16) or not practicing
family medicine, cardiology or oncology (n¼6) were
excluded. Thus, the respondent group retained for the
analysis comprised 341 active physicians with an adjusted
response rate of 9.7%.
Of the respondents, 43%, 30% and 27% were family
physicians, cardiologists and oncologists, respectively.
Thirty-three per cent of the respondents practiced in
Ontario, 20% in Quebec, 24% in Manitoba, Saskatchewan
and Alberta, 14% in the Atlantic provinces and 9% in
British Columbia. Of the cardiologist and oncologist
respondents, 73% and 79%, respectively, held academic
appointments, compared to 41% of family physician
respondents. One-third of survey respondents were in the
46e55 age range. The average time since completion of
training was 12 years for participating oncologists,
18 years for cardiologists and 22 years for family physi cians. Family physician respondents reported working
predominantly in offices or clinics, cardiologists
predominantly in academic health science centres,
community hospitals and private offices/clinics, and
oncologists predominantly in academic health sciences
centres. Respondents from all specialties were repre sented in each geographical area as shown in figure 1.
Attitudes and perceptions
Respondents were asked a series of questions about their
attitudes and perceptions regarding the usefulness of
genetic testing in the context of PM, as an indicator of
physicians’ openness to the adoption of PM. The
majority of respondents agreed that knowing a patient’s
genetic profile can influence treatment decision-making
(83%) and importantly, can improve patient outcomes
(70%). However, only 51% of respondents agreed that
there is sufficient evidence to support ordering genetic
tests. The perception of the usefulness of genetic testing
was similar across specialties and provinces as no
significant differences were observed (figure 2).
State of practice
Respondents’ current levels of practice and knowledge
of genetic testing and PM were also assessed. The results
indicate that oncologist respondents are practicing more
PM, with 59% reporting having ordered a genetic test in
the past month compared to only 22% of general prac titioners and cardiologists. Oncologists also reported
feeling more informed, more able to interpret test
results and more comfortable discussing results with
patients compared with other specialties (figure 3).
Overall, only 21% of respondents agreed that they are
sufficiently informed about PM and 29% agreed that
they are able to interpret the results of genetic tests.
Thirty per cent of respondents agreed that they are
comfortable discussing test results with patients. These measures appear to be consistent across provinces
(figure 3). The survey also assessed physicians’ percep tions of the impact of genetic testing on their patients.
Of the respondents, 40% agreed that their patients have
expressed fears of discrimination based on genetic
testing and 37% reported that their patients are asking
them about genetic testing and PM. Similar reports of
patients expressing fear of discrimination were observed
across specialties (figure 3); however, more oncologists
(50%) reported that patients are asking about
PM compared to 30% of cardiologists and 32% of
general/family physicians (figure 3).
Impacts and benefits
Respondents were asked a series of questions about the
impact and benefits of genetic testing for their practices.
Most respondents reported that genetic tests they
ordered were to identify a genetic predisposition or risk
factor (60% agreed vs 20% disagreed) and that these
tests influenced patient treatment plans (54% agreed vs
18% disagreed). Many also reported that genetic tests
they ordered increased the therapeutic benefit for
patients (42% agreed vs 19% disagreed). Comparing
across specialties (figure 4, panel A), oncologist
respondents were more likely to agree that tests they had
ordered had influenced treatment plans (67% agreed)
compared to other specialties (c2 p¼0.006). Note that
for the purpose of this study, ‘ordering’ means either
requisitioning a test directly or facilitating access
through another healthcare professional, such as
a medical geneticist or other specialist (56% of respon dents reported that they are responsible for ordering
genetic tests for their patients and 31% reported that
a geneticist is responsible for ordering tests for their
patients).
Barriers to adoption
Respondents were asked to indicate what they perceive
as the main barriers to their practice of genetic testing
and PM. A list of 13 barriers (table 1) was provided. The
top five cited barriers were: lack of clinical practice
guidelines, limited provider knowledge, attitudes and
awareness of benefits, lack of evidence-based clinical
information, the cost of testing and a lack of time and
resources to educate patients.
Access to testing
With regards to access to appropriate genetic testing for
their patients, 50% of respondents agreed that tests
which they believe would be useful in their practice are
not readily available, 48% indicated that the cost of
genetic tests is a main barrier to the use of PM and 33%
indicated that the length of time it takes to obtain results
is an important barrier to the use of PM, as the results
may not be received in time to help make treatment
decisions. Compared to other specialities, oncologists
identified the time it takes to obtain results as a barrier
to practice (59%) more often than other specialties
(figure 5, panel A). In general, these measures relating to access to testing varied across provinces, possibly
reflecting differences in access to genetic testing across
Canada (figure 5, panel B).
Physician education
Most respondents reported (figure 6) having no formal
undergraduate (92%) or graduate training (89%) in
genetic testing and PM. Interestingly, 73% of respon dents have attended university lectures or engaged in
self-study and 75% would like more continuing educa tion in this area. More oncologists reported having
graduate training in this area (27%) compared to the
other specialties (c2 p¼0.0001).
DISCUSSION
Attitudes, impacts and benefits
The results of this study indicate that Canadian physi cians responding to the survey are optimistic about the
promise of PM, and open to its use. The majority of
respondents agreed that genetic testing as a component
of PM can influence treatment plans (83%) and improve
outcomes (70%). This is consistent with a recent survey
of molecular oncology testing in Ontario, where it was
reported that molecular oncology testing is expected to
become increasingly prevalent in all areas of diagnosis,
prognosis and treatment in the foreseeable future.21
Similar findings from another Canadian survey30 and
a study of over 10 000 physicians in the US25 also indicate
widespread awareness among physicians of the current
value and potential impact of PM. The positive percep tions found among Canadian physician respondents may
facilitate efficient and appropriate adoption of PM into
practice.
Patient engagement has been identified as a possible
factor in physicians’ attitudes towards adopting new
practices.26 31 Thirty-seven per cent of respondents
reported that their patients were enquiring about
genetic testing and PM. Physicians also reported that
patients expressed fear of discrimination based on
genetic testing (figure 3). Although Canadian law does
not specifically prohibit genetic discrimination, a level of
protection is provided by the Canadian Human Rights
Act (Art. 3) and the Personal Information Protection
and Electronic Documents Act. Steps have been taken to
strengthen these protections. In April 2010, Bill C-508,
an act to amend the Canadian Human Rights Act to
specify genetic discrimination, was introduced into
parliament.32 Few respondents indicated that patient
anxiety concerning test results is a barrier (table 1). This
is consistent with a recent US study of more than 2000
individuals which found no post-test anxiety or adverse
outcomes in individuals who received comprehensive
genetic profiling.33
State of practice
This study showed that oncologists are practicing more
in this area (figures 3 and 4) and are leading in terms of
adoption of PM among the specialties surveyed. With
regards to access to testing, it was found that this and
other measures of the state of practice across the prov inces varied (figure 5). This variability in practice and
access across Canada may be due to differences in access
to testing services, funding and the interpretation of the
evidence or perception of benefits from province to
province. It has been suggested that decision-making
related to predictive genetic testing is ad hoc and vari able across Canada and that a coordinated national
approach is needed.23 Recommendations have been
proposed for a coordinated approach to the adoption
and funding of genetic testing in Ontario.34 Work in this
area is critical to ensuring equitable access and
improving parity of healthcare across Canada. A coor dinated strategy and implementation across the country
may be challenging given the disparate provincially
funded and controlled healthcare systems in Canada.
Barriers to adoption
A lack of medical guidelines was identified by respon dents (61%) as the predominant barrier to adoption,
indicating a need for the development of best practices
and guidelines to support the implementation of PM.
Sharing of best practices as well as genetic testing and
pharmacoeconomic information across provincial
healthcare systems is also likely necessary to support
efficient and cost-effective national implementation of
PM.
Of the respondents, 62% agreed that medical infor matics will be critical to delivering PM. Indeed, vast
amounts of data will be generated with widespread
adoption, and an IT infrastructure for collection,
storage, analysis, interpretation and reporting will be
needed.35e37 Furthermore, decision support tools,
including electronic medical records, will be needed to
facilitate interpretation and point-of-care decision making. This may pose a significant barrier in Canada
where IT infrastructure and electronic medical record
implementation is targeted for completion only in
2015,38 significantly later than in other OECD nations.
Surveys of Canadian21 22 and US physicians25 have
reported the need for physician education for the
successful adoption of PM. These studies found that
a majority of physicians lack the education, training and support necessary for successful adoption. The present
study supports these findings. Furthermore, respondents
indicated that they are actively pursuing more informa tion, with 73% engaging in self-study. These data support
a need for formal and continuing physician education in
this area. A 2010 survey of 90 medical schools in the US
and Canada found that 80% have begun to incorporate
pharmacogenomic training into their curricula;
however, approximately 60% considered this instruction
at their school to be ‘poor’ and more than 80% were not
considering increasing the level of instruction within the
next 3 years.39
Physicians’ perceptions and knowledge of the evidence
supporting the clinical and analytical validity of genetic
tests for PM are obviously important for its adoption.
Canadian and US studies have demonstrated that current
physician knowledge, real-world data and guidelines
relating to PM have often been insufficient for appro priate adoption,40 even where testing is recommended or
publicly funded.41 42 In the present study, 51% of
respondents agreed that there is sufficient evidence to
order genetic tests for PM. These results suggest either
a need for better physician education or a need for
additional supporting evidence for PM implementation.
Most likely both factors are at play. Further supporting
the need for more research was the finding that 53% of
respondents cited the need for evidence-based clinical
information as a main barrier to their use of genetic
testing. Translational research is needed to provide more
robust data for evaluating clinical utility and best prac tices for adoption and implementation within Canada’s
healthcare system. Furthermore, resources that provide
physicians with easy access to accurate and current
information would certainly facilitate appropriate and
efficient adoption of PM.
Conclusions
In the absence of baseline data on provider knowledge
and the practice of PM in Canada, our study fills this
important gap by providing a foundation upon which we
can build. Canada is lagging behind other jurisdictions
which have more resources in place to support PM,
including those that facilitate provider and public
understanding. PM based on genetic testing is currently
being practiced in Canada across specialties and prov inces. Many physician respondents recognise its benefits
and appear to be open to its adoption. They report that
patients are asking them about genetic testing and PM;
however, most physician respondents are not confident
in discussing genetic testing and PM with their patients.
This may not be surprising considering the overall lack of
formal education in the field among surveyed physicians,
as well as the limited time and resources available for
physicians to study this subject. These study results also
indicate variability in practice and access to genetic tests
across Canada among those surveyed. In addition, the
study results point to the need for pan-Canadian strate gies and resources that facilitate healthcare provider
knowledge, training and practice at the undergraduate
and graduate levels, and through targeted continuing
professional education interventions.
Soaring healthcare costs across industrialised coun tries are not sustainable. A few PM pioneers are paving
the way towards demonstrating that these new molecular
tests may result in better care at lower costs. Indeed, the
history of innovation across many industries such as the
computer, telecommunications, higher education,
transportation and many other sectors has shown that
previously inaccessible and expensive products and
services can be made more accessible at lower cost.43
Hence, as we strive for better healthcare, PM and the
new models required for its full implementation present
an unavoidable challenge and perhaps an opportunity to
transform our healthcare system into one adapted to the
21st century. measures appear to be consistent across provinces
(figure 3). The survey also assessed physicians’ percep tions of the impact of genetic testing on their patients.
Of the respondents, 40% agreed that their patients have
expressed fears of discrimination based on genetic
testing and 37% reported that their patients are asking
them about genetic testing and PM. Similar reports of
patients expressing fear of discrimination were observed
across specialties (figure 3); however, more oncologists
(50%) reported that patients are asking about
PM compared to 30% of cardiologists and 32% of
general/family physicians (figure 3).
Impacts and benefits
Respondents were asked a series of questions about the
impact and benefits of genetic testing for their practices.
Most respondents reported that genetic tests they
ordered were to identify a genetic predisposition or risk
factor (60% agreed vs 20% disagreed) and that these
tests influenced patient treatment plans (54% agreed vs
18% disagreed). Many also reported that genetic tests
they ordered increased the therapeutic benefit for
patients (42% agreed vs 19% disagreed). Comparing
across specialties (figure 4, panel A), oncologist
respondents were more likely to agree that tests they had
ordered had influenced treatment plans (67% agreed)
compared to other specialties (c2 p¼0.006). Note that
for the purpose of this study, ‘ordering’ means either
requisitioning a test directly or facilitating access
through another healthcare professional, such as
a medical geneticist or other specialist (56% of respon dents reported that they are responsible for ordering
genetic tests for their patients and 31% reported that
a geneticist is responsible for ordering tests for their
patients).
Barriers to adoption
Respondents were asked to indicate what they perceive
as the main barriers to their practice of genetic testing
and PM. A list of 13 barriers (table 1) was provided. The
top five cited barriers were: lack of clinical practice
guidelines, limited provider knowledge, attitudes and
awareness of benefits, lack of evidence-based clinical
information, the cost of testing and a lack of time and
resources to educate patients.
Access to testing
With regards to access to appropriate genetic testing for
their patients, 50% of respondents agreed that tests
which they believe would be useful in their practice are
not readily available, 48% indicated that the cost of
genetic tests is a main barrier to the use of PM and 33%
indicated that the length of time it takes to obtain results
is an important barrier to the use of PM, as the results
may not be received in time to help make treatment
decisions. Compared to other specialities, oncologists
identified the time it takes to obtain results as a barrier
to practice (59%) more often than other specialties
(figure 5, panel A). In general, these measures relating to access to testing varied across provinces, possibly
reflecting differences in access to genetic testing across
Canada (figure 5, panel B).
Physician education
Most respondents reported (figure 6) having no formal
undergraduate (92%) or graduate training (89%) in
genetic testing and PM. Interestingly, 73% of respon dents have attended university lectures or engaged in
self-study and 75% would like more continuing educa tion in this area. More oncologists reported having
graduate training in this area (27%) compared to the
other specialties (c2 p¼0.0001).
DISCUSSION
Attitudes, impacts and benefits
The results of this study indicate that Canadian physi cians responding to the survey are optimistic about the
promise of PM, and open to its use. The majority of
respondents agreed that genetic testing as a component
of PM can influence treatment plans (83%) and improve
outcomes (70%). This is consistent with a recent survey
of molecular oncology testing in Ontario, where it was
reported that molecular oncology testing is expected to
become increasingly prevalent in all areas of diagnosis,
prognosis and treatment in the foreseeable future.21
Similar findings from another Canadian survey30 and
a study of over 10 000 physicians in the US25 also indicate
widespread awareness among physicians of the current
value and potential impact of PM. The positive percep tions found among Canadian physician respondents may
facilitate efficient and appropriate adoption of PM into
practice.
Patient engagement has been identified as a possible
factor in physicians’ attitudes towards adopting new
practices.26 31 Thirty-seven per cent of respondents
reported that their patients were enquiring about
genetic testing and PM. Physicians also reported that
patients expressed fear of discrimination based on
genetic testing (figure 3). Although Canadian law does
not specifically prohibit genetic discrimination, a level of
protection is provided by the Canadian Human Rights
Act (Art. 3) and the Personal Information Protection
and Electronic Documents Act. Steps have been taken to
strengthen these protections. In April 2010, Bill C-508,
an act to amend the Canadian Human Rights Act to
specify genetic discrimination, was introduced into
parliament.32 Few respondents indicated that patient
anxiety concerning test results is a barrier (table 1). This
is consistent with a recent US study of more than 2000
individuals which found no post-test anxiety or adverse
outcomes in individuals who received comprehensive
genetic profiling.33
State of practice
This study showed that oncologists are practicing more
in this area (figures 3 and 4) and are leading in terms of
adoption of PM among the specialties surveyed. With
regards to access to testing, it was found that this and
other measures of the state of practice across the prov inces varied (figure 5). This variability in practice and
access across Canada may be due to differences in access
to testing services, funding and the interpretation of the
evidence or perception of benefits from province to
province. It has been suggested that decision-making
related to predictive genetic testing is ad hoc and vari able across Canada and that a coordinated national
approach is needed.23 Recommendations have been
proposed for a coordinated approach to the adoption
and funding of genetic testing in Ontario.34 Work in this
area is critical to ensuring equitable access and
improving parity of healthcare across Canada. A coor dinated strategy and implementation across the country
may be challenging given the disparate provincially
funded and controlled healthcare systems in Canada.
Barriers to adoption
A lack of medical guidelines was identified by respon dents (61%) as the predominant barrier to adoption,
indicating a need for the development of best practices
and guidelines to support the implementation of PM.
Sharing of best practices as well as genetic testing and
pharmacoeconomic information across provincial
healthcare systems is also likely necessary to support
efficient and cost-effective national implementation of
PM.
Of the respondents, 62% agreed that medical infor matics will be critical to delivering PM. Indeed, vast
amounts of data will be generated with widespread
adoption, and an IT infrastructure for collection,
storage, analysis, interpretation and reporting will be
needed.35e37 Furthermore, decision support tools,
including electronic medical records, will be needed to
facilitate interpretation and point-of-care decision making. This may pose a significant barrier in Canada
where IT infrastructure and electronic medical record
implementation is targeted for completion only in
2015,38 significantly later than in other OECD nations.
Surveys of Canadian21 22 and US physicians25 have
reported the need for physician education for the
successful adoption of PM. These studies found that
a majority of physicians lack the education, training and support necessary for successful adoption. The present
study supports these findings. Furthermore, respondents
indicated that they are actively pursuing more informa tion, with 73% engaging in self-study. These data support
a need for formal and continuing physician education in
this area. A 2010 survey of 90 medical schools in the US
and Canada found that 80% have begun to incorporate
pharmacogenomic training into their curricula;
however, approximately 60% considered this instruction
at their school to be ‘poor’ and more than 80% were not
considering increasing the level of instruction within the
next 3 years.39
Physicians’ perceptions and knowledge of the evidence
supporting the clinical and analytical validity of genetic
tests for PM are obviously important for its adoption.
Canadian and US studies have demonstrated that current
physician knowledge, real-world data and guidelines
relating to PM have often been insufficient for appro priate adoption,40 even where testing is recommended or
publicly funded.41 42 In the present study, 51% of
respondents agreed that there is sufficient evidence to
order genetic tests for PM. These results suggest either
a need for better physician education or a need for
additional supporting evidence for PM implementation.
Most likely both factors are at play. Further supporting
the need for more research was the finding that 53% of
respondents cited the need for evidence-based clinical
information as a main barrier to their use of genetic
testing. Translational research is needed to provide more
robust data for evaluating clinical utility and best prac tices for adoption and implementation within Canada’s
healthcare system. Furthermore, resources that provide
physicians with easy access to accurate and current
information would certainly facilitate appropriate and
efficient adoption of PM.
Conclusions
In the absence of baseline data on provider knowledge
and the practice of PM in Canada, our study fills this
important gap by providing a foundation upon which we
can build. Canada is lagging behind other jurisdictions
which have more resources in place to support PM,
including those that facilitate provider and public
understanding. PM based on genetic testing is currently
being practiced in Canada across specialties and prov inces. Many physician respondents recognise its benefits
and appear to be open to its adoption. They report that
patients are asking them about genetic testing and PM;
however, most physician respondents are not confident
in discussing genetic testing and PM with their patients.
This may not be surprising considering the overall lack of
formal education in the field among surveyed physicians,
as well as the limited time and resources available for
physicians to study this subject. These study results also
indicate variability in practice and access to genetic tests
across Canada among those surveyed. In addition, the
study results point to the need for pan-Canadian strate gies and resources that facilitate healthcare provider
knowledge, training and practice at the undergraduate
and graduate levels, and through targeted continuing
professional education interventions.
Soaring healthcare costs across industrialised coun tries are not sustainable. A few PM pioneers are paving
the way towards demonstrating that these new molecular
tests may result in better care at lower costs. Indeed, the
history of innovation across many industries such as the
computer, telecommunications, higher education,
transportation and many other sectors has shown that
previously inaccessible and expensive products and
services can be made more accessible at lower cost.43
Hence, as we strive for better healthcare, PM and the
new models required for its full implementation present
an unavoidable challenge and perhaps an opportunity to
transform our healthcare system into one adapted to the
21st century.


Source: https://bmjopen.bmj.com/content/1/1/e000110.short
------end---------
Treatment of Pain in Cancer: Towards Personalised Medicine

Abstract
Despite increased attention to cancer pain, pain prevalence in patients with cancer has not improved over the last decade and one third of cancer patients on anticancer therapy and half of patients with advanced disease still suffer from moderate to severe pain. In this review, we explore the possible reasons for the ongoing high prevalence of cancer pain and discuss possible future directions for improvement in personalised pain management. Among possible reasons for the lack of improvement are: Barriers for patients to discuss pain with clinicians spontaneously; pain measurement instruments are not routinely used in daily practice; limited knowledge concerning the assessment of undertreatment; changes in patients’ characteristics, including the ageing of the population; lack of significant improvement in the treatment of neuropathic pain; limitations of pharmacological treatment and lack of evidence-based nonpharmacological treatment strategies. In order to improve cancer pain treatment, we recommend: (1) Physicians proactively ask about pain and measure pain using assessment instruments; (2) the development of an optimal tool measuring undertreatment; (3) educational interventions to improve health care workers’ skills in pain management; (4) the development of more effective and personalised pharmacological and nonpharmacological pain treatment.
Keywords: cancer pain; prevalence; barriers; undertreatment
1. Introduction
Despite increased attention to cancer pain, pain prevalence in cancer patients has not significantly changed over the last decade compared to the four decades before. Back in 2007, a systematic review about the prevalence of pain in cancer comprising 40 years (1966–2005), with 52 articles included, and based on almost 20,000 patients, made clear that over 1/3 of patients on anticancer therapy suffered from moderate to severe pain and almost half of the population receiving palliative care had moderate to severe pain [1]. The update of the review on the prevalence in cancer 10 years later showed improvement in attention for pain in cancer patients: A 35 percent increase in papers on the prevalence of pain in cancer were published in the period between 2005 and 2014 compared to the 40 years before. In addition, the methodological quality of these papers increased from only 1 out of 3 being of acceptable quality to over half, and the number of included patients tripled [2]. Unfortunately, pain prevalence did not change and still almost half of the population receiving palliative care experienced moderate to severe pain (Table 1).
Table 1. Prevalence of pain in patients with cancer.
Table
Although textbooks often associate malignancies with either a high risk (bone, pancreas, oesophagus) or with a low risk of pain (lymphoma, leukaemia, soft tissue) [4,5], it is not clear what evidence these statements are based on. A cohort study of end-stage cancer patients [6] concluded that cancer pain was not restricted to specific tumour locations. Reyes-Gibby et al. studied patients under anticancer treatment and reported high prevalences of moderate to severe pain in patients with head and neck, gastrointestinal, and breast malignancies [7]. Nevertheless, the type of cancer was not shown to be a predictor of pain prevalence, although patients with gastrointestinal, lung, breast, other haematological, and “other” malignancies had a significantly higher risk of moderate to severe pain than prostate cancer patients [8].
The results of systematic literature reviews raise important questions: Why does pain prevalence not decrease despite inevitably more research on cancer pain? The latter not only resulted in an increased understanding of cancer pain mechanisms, such as pain due to bone metastases [9], but also in the development of new drugs for cancer pain, such as the rapid onset opioids for breakthrough pain.
Do we still neglect to ask our cancer patients if they experience pain, which as a result might lead to undertreatment? Or might we ask the question if cancer patients and/or their pain has become more complicated over the years or we do not have the tools to treat them properly? In this review, we discuss the possible reasons for the ongoing high prevalence of cancer pain and point at possible future directions for improvement in casu personalised medicine for patients with cancer experiencing pain.
2. Pain Prevalence: Reasons for Lack of Improvement in Cancer Pain
2.1. Patients do Not Report Pain Spontaneously
A recent study confirmed that cancer patients are still uncomfortable with discussing many of their symptoms in the clinical setting, even when the symptoms are bothersome [10]. Although lack of energy and pain were most often discussed, only about 50% of patients intended to discuss these bothersome symptoms with their physician. Well known patient barriers towards reporting pain, such as misconceptions about analgesic use (fear of adverse effects, addiction, tolerance, and lowered immunity caused by pain medicine), concerns about pain communication (unde-reporting of symptoms to avoid distracting physician from providing cancer treatments), and a belief that pain is inevitable and uncontrollable, are still very prevalent [11]. A study of 135 patients with audio-recorded consultations revealed that patients’ active communication about pain (patients asked questions, stated preferences about pain-related matters, and expressed concerns) made physicians change their pain management regimen, which could lead to better pain control [12]. However, a systematic review of the effectiveness of patient-based educational interventions to improve cancer-related pain concluded that improvement of pain was only seen in less than one third of the studies and in less than 20% of all patients included [13]. The patient-based educational interventions as described (Oldenmenger 2018) varied widely in content and intensity. Furthermore, secondary significant differences were noted in favour of the experimental arms in pain knowledge, relieving barriers, medication adherence, and self-efficacy. From this, it was concluded that standardised patient training programmes addressing clinician–patient communication might possibly lead to better cancer outcomes.
For now, the first step is for the physicians to acknowledge the fact that many patients experience cancer-related pain and proactively ask for pain and pain-related complaints explicitly.
2.2. Measurement of Pain
The measurement of pain is an issue in itself and it was shown that only 7 to 43% of physicians use the numeric rating scale (NRS) or visual analogue scale (VAS) in their practice, whereas multidimensional questionnaires for pain assessment are rarely used [11]. Moreover, 30% to 51% of physicians thought that patients exaggerated their pain in order to attract attention [14]. Today, we still do not measure pain routinely, except for research purposes. In a recent study in the Netherlands, in only 0.1% of almost a thousand patients visiting an oncology outpatient clinic was an NRS score of pain intensity recorded [15]. A year later, after introducing the new nationwide cancer pain guidelines in the Netherlands, there was still no mention of pain in the vast majority of patients, and an NRS was found only once in the patient records [16]. A reference to pain in the medical record of patients was found in 50%, 28%, and 21% in respectively academic, large peripheral, and small peripheral hospitals. It is not known why health care professionals are reluctant to consequently measure pain in patients with cancer. Possibly, the lack of knowledge and confidence to administer analgesia effectively [17] and/or underestimating the influence of pain in daily life [18] contribute to poor pain assessment in patients with cancer.
2.3. Assessment of Undertreatment
To date, the assessment of undertreatment is not a daily practice. The pain management index (PMI) [19] is a helpful tool to assess undertreatment. First, a pain score and treatment score are calculated: Pain score: Mild = 1 (NRS 0–4), moderate = 2 (NRS 5, 6), severe = 3 (NRS 7–10); treatment score: Non-opioid = 1, weak opioid = 2, strong opioid = 3. Next, the pain score is subtracted from the treatment score. A positive result, i.e., a positive PMI, is said to indicate that the patient is adequately treated, whereas a negative PMI indicates undertreatment. It should, however, be noted that the PMI probably underestimates undertreatment. A patient with severe pain and strong opioids, for example, will not receive a negative score on the PMI (NRS = 0), but this patient is definitely not adequately treated. Furthermore, the sensitivity of PMI scores of < −1 and < 0 for predicting pain interference were only 0.16 and 0.37, and the corresponding specificities were 0.95 and 0.71, respectively. This implies that a negative PMI does not always indicate inadequate pain management [20].
The NRS is likely not the best instrument to assess a patient’s pain. Patients determine whether their pain is ‘controlled’ by whether or not they are able to maintain relationships with family or friends and perform activities which are important to them. The NRS does not necessarily reflect these aspects [21]. Last, but not least, the PMI does not include antineuropathic comedication. For now, however, as the NRS is the best available instrument for the assessment of pain, it is questionable if solid conclusions about the undertreatment of pain in patients with cancer can be made. For future studies and daily practice, the global assessment of the response or pain relief is an easy to use, patient-centered help to assess the adequacy of pain treatment.
2.4. Undertreatment
During the last decade, two meta analyses on undertreatment in cancer pain were published. Both concluded that although 30% of patients were still not adequately treated, an improvement over time could be observed [22,23]. The undertreatment of patients was found to be worse in low-income countries and in noncancer hospitals [22,23].
A review of the knowledge and attitudes of healthcare professionals towards cancer-related pain concluded that there is a continuing deficit in professional knowledge regarding cancer pain but also that many of the studies that investigated the effect of educational programmes on healthcare professionals’ knowledge, attitude, and/or practice failed to show an improvement in knowledge, attitude, and/or practice [17]. A discrepancy between self-assessment and actual practice in cancer pain management by medical oncologists, indicating that specialised physicians are not fully aware of their knowledge deficiencies, has been reported [24]. Furthermore, a recent study by Martens et al. noted that with long-term geriatric care, doctors base their choices in opioid prescriptions almost exclusively on personal experience and are barely influenced by guidelines [25]. From this, we conclude that new strategies for teaching healthcare professionals are needed. For instance, improved pain education in undergraduate medical and nursing curricula might help to overcome these problems and introduce guidelines for the treatment of pain in cancer patients at an early stage in teaching and training [26].
2.5. Pain and/or Patients More Complex?
2.5.1. Pain Characteristics
Cancer pain can be classified in different ways: By its origin (somatic, neuropathic, or visceral), by its duration (acute or chronic), and by its pattern (continuous and incident or fast and short breakthrough pain).
The treatment of cancer pain is more complex when it comes to neuropathic pain and pain with breakthrough. Neuropathic pain, incident pain, and pain intensity are independently associated with the amount of days necessary to achieve stable pain control [27]. Cancer patients with neuropathic pain are most prone to being treated insufficiently. In a recent study among 892 cancer patients, 40% of the patients with moderate to severe pain also showed neuropathic pain symptoms, causing increased interference with daily activities. However, only 8% of these patients were treated with co-analgesics [28]. Patients with neuropathic cancer pain have significantly greater analgesic requirements and have a reduced performance status. These cancer patients with neuropathic pain report worse physical, cognitive, and social function [29]. Patients with breakthrough cancer pain (BTcP) reported significantly higher pain intensity scores than patients without BTcP and they reported an impaired quality of life (QoL) [30]. The prevalence of both BTcP and (partly) neuropathic pain has not changed over the years [31,32,33,34,35,36] (Figure 1).
Cancers 10 00502 g001 550Figure 1. Prevalence (in %) of different pain modalities in cancer, as reported by various studies.
Over the last 10 years, significant progress has been made in the field of BTcP, both with respect to the definition of BTcP as well as its treatment. Although there are still no universally accepted diagnostic criteria for BTcP, the definition of the task group of the Science Committee of the Association for Palliative Medicine of Great Britain and Ireland [37] is now widely accepted: BTcP is ‘a transient exacerbation of pain that occurs either spontaneously, or in relation to a specific predictable or unpredictable trigger, despite relatively stable and adequately controlled background pain’. The use of this definition allows comparisons of different studies.
The USA already approved oral transmucosal fentanyl citrate (OTFC) for the treatment of BTcP in 1998. Europe followed in 2002 [38]. Since then, many rapid onset opioids (ROOs), all transmucosal fentanyl products, have found their way to the market place. These ROOs follow the pattern of BTcP: Fast and short.
Current BTcP guidelines almost invariably endorse the preferential use of ROOs in breakthrough pain. However, the scientific evidence to support the current guidelines remains at a low grade [39].
Much less progress has been made in the treatment of neuropathic pain. It has been established that only a minority of patients with neuropathic pain receive co-analgesics, and that the available adjuvant analgesics have moderate numbers needed to treat (NNT). The pooled NNT of trials with tricyclic antidepressants (TCA), serotonin–noradrenaline reuptake inhibitors (SNRI), and gabanoids are 4–5, 5–7, and 6–12, respectively [40].
In conclusion, a modest success of a better treatment of BTcP is observed, and at the same time, a lack of improvement in treatment success in cancer patients with neuropathic pain. These findings cannot fully explain the fact that the overall prevalence of pain in cancer patients did not considerably change over time.
2.5.2. Patient Characteristics
Due to increasing life expectancy and ageing of the population, the prevalence of cancer and cancer treatment in patients over 70 years of age is expected to further increase. Age is associated with and partly influences clinical decisions and outcomes in different ways. Age influences how quickly patients are referred to specialised care. Older patients are often underrepresented in clinical trials, and the number of comorbidities increase and treatment outcomes of the cancer are less favourable [41]. On the other hand, a younger age, in addition to psychological distress and the already mentioned neuropathic pain and incident pain, is independently associated with more days to achieve stable pain control [27].
More and more chemotherapy has been prescribed to more and more vulnerable patients over the last 10 years, but whether this has influenced the prevalence of cancer pain is not known. Hence, the present cancer pain polulation significantly differs from the cancer patient population of of a few decennia before. This also implies additional difficulties in the treatment of pain as, for instance, medication is generally not studied in elderly people over 70 years of age, and in this group, comorbidities like heart, lung, liver, and kidney disease, and comedication can cause frequent and/or unexpected side-effects. For example: Clinically important renal impairment is common in old age, and even 99% of patients aged 85 and over have a moderate or severe reduction in the glomerular filtration rate (GFR) [42]. A reduced GFR has significant implications for, amongst others, the choice of strong opioids. The two major metabolites of morphine, morphine-3-glucuronide (M3G) and morphine-6-glucuronide (M6G), are excreted renally [43]. The accumulation of M3G and M6G is already present with mild renal impairment and, in particular, the accumulation of M3G is associated with neurotoxic adverse effects, like hyperalgesia, allodynia, myoclonus, seizures, and delirium [44,45]. Although no recommendations could be formulated on the preferred opioid in patients with renal impairment, based on pharmacokinetics and clinical experience, fentanyl, alfentanil, and sufentanil are mostly recommended [46].
It should be stressed that comorbidities can be associated with pain as well. In the elderly, pain is frequent: 65% suffer from osteoarthritic back pain, around 40% of musculoskeletal pain, 35% peripheral neuropathic pain, and 15–25% from chronic joint pain [47].
In conclusion: Although pain characteristics did not change over time, the cancer patients with pain did all the more. The increasing complexity of patients with cancer may at least partly account for the lack of success in reducing pain prevalence in patients with cancer.
2.6. Interventions to do Better?
The question remains of, if we just cannot treat cancer pain better than we do now, it is because we just do not have the proper tools to do better. In the last decade, we got access to many different opioids and co analgesics. The disappointing numbers needed to treat for co-analgesics were already mentioned.
2.6.1. Pharmacological Interventions
A review of the effectiveness of the use of the WHO pain ladder in cancer pain (Figure 2) showed adequate pain relief in 20–100% of the patients [48].
Cancers 10 00502 g002 550Figure 2. WHO analgesic ladder in cancer pain. Abbreviation: WHO, World Health Organization.
This range in adequate pain relief is quite wide, and depends on the definition of success. When an NRS of <4 is used to define success, 50–90% of the patients can be treated successfully with the use of the WHO pain ladder [32,49].
Although the European Association for Palliative Care proclaimed the WHO ladder essential in treating pain in cancer, they also stated that “there is a shocking lack of evidence to support clinical practice and guidelines at the present time” [50]. Although the WHO ladder is a valuable and easy to follow treatment algorithm to use in the treatment of cancer pain, there is still room for debate. The updated national guideline “treatment of pain in cancer” in the Netherlands recommends prescribing paracetamol as “as needed” medication instead of maintaining the “around the clock” advice with the start of strong opioids (step 3), according to the ladder. Opposed to the positive randomised controlled trial (RTC) of Stocler [51] indicating that adding paracetamol to strong opioids improves pain control and wellbeing, four RCTs [52,53,54,55] and one crossover study [56] failed to demonstrate a beneficial effect of maintaining around-the-clock paracetamol. In a prospective study, 53% of patients wanted to stop taking paracetamol, 18% wanted to continue with regular paracetamol medication as before, and 29% wanted to take paracetamol as needed [57]. A recent Cochrane review concluded: “There is no high-quality evidence to support or refute the use of paracetamol alone or in combination with opioids for the first two steps of the three-step WHO cancer pain ladder” [58].
Another problem with the pharmocological treatment of pain in cancer patients is that numerous studies and meta-analyses up until now have shown no clear benefit in pain relief for one opioid over the other. As a result, there is no consensus on the choice of a strong opioid to start with at step 3 of the WHO ladder [59,60,61,62]. Recent studies tried to make opioid therapy more patient-tailored to improve efficacy by including patient characteristics. For example: Pharmacokinetic studies have shown that drug exposure and/or metabolism significantly differ based on the presence of polymorphisms in genes coding for pharmacokinetics genes. Many potential genetic markers have been described, and the importance of genetic predisposition in opioid efficacy and toxicity has been demonstrated in knockout mouse models and human twin studies. Furthermore, a shortlist of 10 genes that are the most promising markers for clinical has been proposed [63]. The most solid evidence of clinically relevant gene variations on the analgesic treatment with opioids is available for CYP2D6, COMT, SLC22A1, and a genetic variant, OPRM1. So far, however, the Clinical Pharmacogenetics Implementation Consortium guidelines provide CYP2D6-guided therapeutic recommendations to individualise treatment only with tramadol and codeine. Guidelines for other opioids are lacking [64].
As of now, further optimisation of patient/opioid couples is investigated in a different way. Recently, the superiority of methadone over fentanyl in the treatment of neuropathic pain in patients with head-and-neck cancer was shown [65]. These findings are in line with the hypothesis that methadone has a dual mechanism of action, not only interfering with the u-opiate receptor but also with the NMDA-receptor. The latter is known to be specifically involved in neuropathic pain [66]. With this study, the clinical success, defined as a >50% improvement, was significant higher with methadone treatment at 1 week. Demographic characteristics may be important in the prediction of treatment success, as we were able to show that therapy success, again defined as an over 50% pain reduction at one week, was positively associated with the use of methadone, the presence of neuropathic pain, and duration of pain in months [67]. Treatment success was negatively associated with the age of the patient in years. The inclusion of these four characteristics into a prediction model resulted in an area under the curve of almost 82 percent. In line with these findings, Corli and colleagues [68] assessed the role of demographic characteristics, pain features, comorbidities, and ongoing therapy on the lack of efficacy in the treatment of pain in cancer and on the occurrence of severe adverse drug reactions. Liver metastases and the presence of BTpC were found to increase the risk for nonresponse. Conversely, a high basal pain intensity significantly decreased the same risk.
Apart from the differences between pharmacokinetics and pharmacodynamics in ligand–receptor binding as well as pharmacogenetics, strong opioids differ in the routes of administration and effectiveness. Oral, transdermal, subcutaneous, and intravenous routes of drug delivery are possible for each individual opioid, and this plays an important role in the patients’ choice and wishes.
In conclusion: There is a need for more effective pain treatments and pain treatment should be personalised using pain and patient characteristics and, in the near future, genetic profiling in order to decrease the prevalence of pain in patients with cancer.
2.6.2. Nonpharmacological Interventions
Already many decades ago, Dame Cicely Saunders introduced the term ‘‘total pain’’ to characterise the multidimensional nature of a patient’s cancer pain experience to include the physical, psychological, social, and spiritual domains. Based on this view and definition, the use of nonpharmacological interventions in addition to pharmacological strategies has been increasing. Nonpharmacological interventions can be divided into two groups: Interventions that aim for patient empowerment and interventons aimed at providing comfort for patients.
Patients struggle with misconceptions about pain medication, concerns about pain communication, and beliefs about the inevitability and uncontrollability of cancer pain [69]. Many interventions have been developed and evaluated in order to improve pain control and support the self-management of patients. Interventions focused at empowerment of the patient incude addressing knowledge about pain, pain medication, side effects, and alternative methods to control pain. Other interventions target problem-solving skills and communication skills [69]. Unfortunately, none of the interventions dealing with patient empowerment achieved the desired effects on different outcome measures so far, and the questions remain about the optimal format as well as the content as the combination of intervention components [70].
In cancer pain, many complementary interventions to provide more comfort for the patient are described: Aromatherapy, with or without reiki and therapeutic touch or massage, massage therapy as such, music therapy, art therapy, and electromyographic biofeedback-assisted relaxation [71]. Unfortunately, most studies are of (very) low quality and show inconsistent results on these same nonpharmacological interventions [71]. Other barriers to applying nonpharmacological interventions are the need to hire external professionals with specific training, and the financial cost for institutions. Properly designated studies are needed to define the place of these interventions in the personalised treatment of cancer pain.
3. General Considerations Concerning “Cancer Pain”
Over the last years, the question of whether “cancer pain” is an entity by itself and whether we do not create a “sham distinction” is now known better as a “distinction without a difference“. When pain is experienced by a patient with cancer, this pain is often referred to or indicated as cancer pain. Although this terminology is fairly simple and might suggest easy and straitghforward treatment or treatment algorithms, an indication of “cancer pain” is incomplete and much more complex. For instance, a patient suffering from pain due to bone metastases is familiar with a different type of pain compared to a patient suffering from chemotherapy-induced neuropathic pain. Nevertheless, both types of pain are included within the indication “cancer pain”. Therefore, the indication “cancer pain” should no lomger be used; instead, we should base the indication on the the actual pain syndrome the patient with cancer is suffering from. This might improve the understanding of the underlying pain syndrome, and could assist in the selection of the most optimal drug treatment.
4. Conclusions
The prevalence of pain in patients with cancer has remained unchanged over the last decennia despite increased understanding of cancer pain mechanisms and the development of new drugs for cancer pain. One third of cancer patients on anticancer therapy and half of patients with advanced disease still suffer from moderate to severe pain. We discussed various reasons for this lack of improvement: The absence of spontaneuously reporting of pain by patients, insufficient measurement of pain and assessment undertreatment by clinicians, complex pain characteristics, changes in characteristics of patients with cancer, and limitations of interventions, including pain medication. From this, we conclude that the clinician must finally really start measuring pain in cancer patients at all visits and find ways to decrease the “unskilled and unaware” status in health care workers. Currently, educational efforts have limited effects and new and/or earlier interventions have to be sought. Additionally and last, but certainly not least, research into new and more personalised pain treatment must go on, in genetics as well as in pharmacological and nonpharmacological treatment.

Source: https://www.mdpi.com/2072-6694/10/12/502/htm
------end---------

Personalised medicine for multiple sclerosis care

Abstract
Treatments with a range of efficacy and risk of adverse events have become available for the management of multiple sclerosis (MS). However, now the heterogeneity of clinical expression and responses to treatment pose major challenges to improving patient care. Selecting and managing the drug best balancing benefit and risk demands a new focus on the individual patient. Personalised medicine for MS is based on improving the precision of diagnosis for each patient in order to capture prognosis and provide an evidence-based framework for predicting treatment response and personalising patient monitoring. It involves development of predictive models involving the integration of clinical and biological data with an understanding of the impact of disease on the lives of individual patients. Here, we provide a brief, selective review of challenges to personalisation of the management of MS and suggest an agenda for stakeholder engagement and research to address them.
Personalised medicine for multiple sclerosis: is it needed?
Personalised medicine is currently in vogue. It has been championed as a foundation for future medicine, based particularly on examples drawn from cancer and rare inherited metabolic diseases.1 Individualised, genetic-based diagnoses have had considerable impact, for example, in licencing of imatinib for Philadelphia chromosome–positive chronic myeloid leukaemia and trastuzumab in HER2-positive breast cancer.2 Whole exome or whole genome sequencing promises further impact on diagnosis and management of rare diseases.3
But, is it needed for those of us caring for people with multiple sclerosis (MS)? There is no question but that MS care is an area that should be the focus of innovation in the model for healthcare delivery. MS is a ‘syndrome’ with a wide variation in clinical presentation, disease course and responses to treatments. While unpicking this heterogeneity may not have been crucial 20 years ago, when limited treatment options were available, with an increasing number of available treatments and a more empowered and informed patient population, it is central everyday practice now. Because the medicines all are relatively expensive, better ways of making most effective use of them are needed (Figure 1(a)). They also have different mechanisms of action and a wide range of efficacy and relative risk. A newly diagnosed patient and his or her neurologist therefore must make complex decisions for treatment initiation, treatment choice and treatment escalation. The neurologist needs to define and communicate clearly to patients and payers:
•	
What factors contribute to the expression of MS and its relative risk without treatment?
•	
Who should be treated?
•	
When should they be treated?
•	
How can medicine risk and benefit best be balanced?

Figure 1. (a) A simplified illustration of the potential cost-effectiveness of treatment monitoring for routine interferon neutralising antibody (NAb) testing of MS patients on IFN treatment. The mean cost of relapse treated in the UK National Health System is £3586.4 The cost of NAb testing is about £50. It was assumed that the NAb test provides an actionable index discriminating potential efficacy of the IFN treatment from lack of any impact and that patients recognised to have NAb would be switched to another, equally effective disease modifying treatment of similar cost. Recognition of the NAb therefore leads to savings accruing from avoidance of potential relapse costs associated with loss of efficacy of the IFN (change in relapse rates with loss of efficacy × cost of relapse = 0.52 × 3586 = £1864).5 The potential savings per test is the product of savings from avoidance of relapse and probability of a positive test (1864 × 0.45 = £840). (b) A graphical illustration of the dependence of net benefit of any treatment on baseline risk for a patient. The benefit of treatment was assumed to be proportional to the risk of disease progression (relapse). An ‘ideal treatment’ – one in which there is no cost or risk of adverse events – should benefit all patients, with an absolute benefit (measured, for example, in adjusted quality of life years) that increases with disease risk. However, in practice, treatments also can have a negative impact on patients, either as cost or adverse events. Here, for simplicity, the negative impact is assumed to arise only from adverse effects of treatment and the risk and impact are assumed to be the same for all patients. Estimation of the net benefit to a patient includes consideration both of the impact of treatment to reduce disease activity and the impact of adverse events associated with medicine use. While patients with higher baseline risk from the untreated disease will receive greater net benefit (area defined by grid lines to the right), those at lower risk from their disease may experience net harm (punctate area marked to the left). A goal of personalised medicine is to match the benefit/risk profile of a medicine and the baseline disease risk to optimise the net benefit for a patient.
The patient and the payer both need to be confident that neurologists’ answers to these questions are well founded. These questions can be answered with reference to clinically meaningful characteristics of sub-groups of patients. However, this alone does not ensure that the needs of all individuals are optimally addressed. The goals of personalised medicine are to establish robust, evidence-based approaches to addressing the questions for each patient.
Recent advances in the management of other complex diseases offer real cause for optimism that these goals are achievable. Rheumatoid arthritis (RA) provides an illustrative example. Individual differences easily captured in the clinic define different disease courses and can be used prospectively in the selection of treatments. Epidemiological studies showed that patients with high body mass index have worse outcomes from RA and may respond more poorly to tumour necrosis factor inhibitors (TNFi), whereas the efficacy of abatacept and tocilizumab is unaffected.6 Smoking also adversely affects TNFi outcomes, but has less or no effect on the efficacy of rituximab and tocilizumab. Patients expressing an anti-citrullinated peptide antibody biomarker are more likely to have a progressive, destructive course7 with greater disease activity and radiological damage8 and a worse response to anti-TNF medications,9 although a better response to rituximab and abatacept.10
Where are we now?
The questions posed for personalised medicine for MS now can be distilled into three challenges:
•	
Precision diagnosis;
•	
Predicting treatment response;
•	
Personalised monitoring to progressively update this prediction.
Briefly reviewing progress towards realising these provides a way of benchmarking the current state of the art.
Diagnosis couples clinical presentation with conventional laboratory tests and imaging (and, in some cases, cerebrospinal fluid (CSF) examinations),11 to reject alternative possible disorders, to define the probable MS syndrome and, in some cases, to recognise the distinct sub-syndrome of primary progressive MS. Auto-antibody testing supports further diagnostic precision that defines distinct diseases among the idiopathic demyelinating disorders. Neuromyelitis optica spectrum disorders (NMOSD) can now be identified by serum antibodies against aquaporin4 (AQP4-IgG) with high specificity12,13 and moderate sensitivity.14 NMOSD have a distinct clinical course to MS and do not respond (or may worsen) with interferon (IFN) treatment.15 Patients with myelin oligodendrocyte glycoprotein (MOG) antibody-associated demyelination also have a unique clinical, radiological, and therapeutic profile.16 These examples illustrate well how the integration of clinical and biomarker data can be used to provide specific prognostic and therapeutic advice to individual patients.
However, where we still fall far short of the ideal is in establishing prognosis, which also should be part of precision diagnosis. Because all treatments carry costs – in financial terms or as a reduction in quality of life or risks to future health – the net benefit of treatment involves balancing the expected natural history of an individual’s disease, or baseline risk, against the effects of treatment17 (Figure 1(b)). Patients differ widely in their baseline risk of untreated disease progression. The net benefit of any treatment needs results from the interaction of treatment efficacy and likelihood of adverse events against baseline risk. It is this net benefit that is central to estimates of clinical effectiveness.
Considerable community effort has been addressed to the identification of prognostic factors for estimation of the baseline risk, but the precision with which this can be done still is limited. For example, magnetic resonance imaging (MRI) measures of disease activity (by gadolinium contrast enhancement) or T2-hyperintense lesion load are important prognostic measures for prediction of risk of clinically definite disease after first symptoms.18 Large, single clinical centre–based studies additionally have highlighted interactions of MRI measures with age and sex in determining risk of progression.19 Lesion distribution and clinical presentation appear to be independent predictors of medium term prognosis.20 Epidemiological studies suggest that other phenotypic (e.g. obesity, serum vitamin D), exposure (e.g. sunlight) and lifestyle factors (e.g. smoking) impact on prognosis.21 However, we are not aware of models that define their quantitative interactions with individual susceptibilities. For example, is the impact of smoking, low vitamin D and obesity meaningfully higher in people carrying the DRB1501 allele, or with early presentations of disease? Genotype alone has not yet been shown to contribute significantly to disease severity risk.22 In the absence of single, highly predictive markers, personalisation will depend on clusters of markers in multivariate models.
There still are few specific indices to guide the timing of treatment beyond evidence from trials that early treatment delays short-term clinical progression.23 Choice of initial treatment also does not have a good evidence base, other than the lack of benefit (or worsening) that has been found with IFN and other conventional disease modifying treatments24 in patients with progressive onset disease or NMOSD. While not fully evidence-based, patients with a higher baseline risk likely will receive greater net benefit from any treatment. Information concerning the relative efficacy of medicines is limited due to the small number of head-to-head clinical trials and the limitations of inference even when comparisons of the pivotal trials of individual agent efficacies are made using formalised meta-analytic structures.25 One of the most promising approaches to gathering evidence concerning relative clinical effectiveness is through real-life data aggregation in multi-centre consortia, such as MSBase.26 Generally, choices regarding medicine use in clinical practice are framed in terms of a hierarchy of efficacy and risk for treatments based on data from their pivotal clinical trials (which were intended to demonstrate efficacy, rather than comparative effectiveness). Decision-making then represents the balancing of these data against estimates of relative disease severity for any given patient. Patient and neurologist specific factors of preference and access also play a role. However, while there may be general guidelines, there is not a general consensus regarding the criteria and methods for arriving at the balance of evidence for an individual patient.
Although pharmacogenomics has been of limited use thus far in the personalisation of MS, this could change, particularly if the medical community moves towards more routine genotypic profiling. Individual characteristics determining drug metabolism (e.g. hepatic and renal function for elimination of IFN) may guide choice of drug in less common situations in patients with comorbidities.27 Differences in ethnicity may alter drug absorption or metabolism.28 There is preliminary evidence for possibly meaningfully significant effects of specific markers in some individuals for IFNβ.29–31 For example, one study reported the discovery and validation of an intronic variant in SLC9A9 gene as a predictor of response to treatment (P <5 × 10−8).32
Currently, in the absence of strongly predictive prospective markers, treatment monitoring plays a major role. In fact, unquestionably, the currently best developed example of personalised medicine in MS is for safety monitoring of natalizumab treatment.33 This model combines titres of anti-John Cunningham (anti-JC) virus antibody, treatment duration and previous history of immunosuppressive therapy in order to stratify patient risk of progressive multifocal leukoencephalopathy (PML). Baseline risk assessment and monitoring with treatment rapidly became the standard of practice as the manufacturer and regulators worked together to define a way of keeping this powerful treatment available once PML was recognised as a complication. An international pharmacovigilance effort developed by the manufacturer rapidly led to validation of a clinically practical approach to personalisation of risk and subsequent monitoring.
Monitoring for effectiveness is more challenging, in no small part because the target outcome (ultimately, the accrual of fixed disability) is less easily defined. Nonetheless, there are examples that are widely, if not universally, employed. Neutralising antibody levels for IFN34 and for natalizumab35 explain a proportion of poorer efficacy of these medications. More generally, T2 lesion increases and brain volume reduction on treatment are predictive of longer term clinical efficacy, at least at a group level.36 The latter, combined with clinical measures of disease activity (relapse frequency or progression of fixed disability), already are incorporated into treatment escalation decisions in many clinical centres, although specific criteria for a switch in treatment are not generally agreed. While the availability of large datasets is a major confound, this also is a consequence of lack of standardisation of MRI field strength, criteria for lesion identification and software for brain volume change measures. Possibly even more sensitive markers of sub-clinical disease activity are emerging, for example, with monitoring of CSF or serum neurofilament light chain (Nf-L) concentrations.37 In a 15-year follow-up study, higher levels of CSF Nf-L at baseline were associated with greater disability progression in relapsing-remitting multiple sclerosis (RRMS) patients.38 Changes in concentrations while on treatment also have been linked to treatment response.39 Additional markers are being explored actively, but most of the profusion of reports based on studies with smaller populations have later failed replication; Kroksveen et al.40 recently reported that from 188 proposed CSF MS biomarkers, only 10 (5%) have been successfully validated.
What needs to be done?
We can summarise these concepts in a vision of what personalised medicine for MS could be: defining the disease suffered by an individual patient in personal, clinical and biological terms that then guide informed decisions on management jointly by the patient and his or her neurologist. These decisions would be based on an evidence base summarised for each patient as a personalised, quantitative estimate of an untreated prognosis and the potential for benefit and harm of different medicines. Decision criteria would be defined based on these estimates and a qualitative appreciation for patient preferences and expectations. Together, this evidence also would provide a solid substrate for cost-effectiveness models to support a reasoned debate about how outcomes are valued and healthcare spending is allocated.
We are still far from practising a truly personalised medicine for MS. Routinely getting the ‘right drug to the right patient at the right time’ will demand coordinated efforts across the MS community. A refocusing of the research agenda is needed. Those involved in MS care, industry, healthcare administrators and patients need to work together to develop the following:
1.	
Large, representative datasets. Clinical trial data can provide a basis for large datasets shared by the community. Sharing of anonymised, individual subject-level data from clinical trials should become the standard. As a pragmatic step towards full transparency of individual data with all clinical trials reports, release of these data with approvals should be required. The academic medical community must lead the way with release of anonymised single-subject data with their own reports. These latter data, particularly when well curated and describing real-life patient care experience incorporating both clinician- and patient-reported measures, will be at the core of future developments for personalised medicine. Many patient characteristics potentially contribute to clinical outcomes; a comprehensive patient profile is needed. As defined in this way, the datasets will be dynamic, growing and evolving in a ‘learning health system’. Creation of these datasets will enable lessons of clinical practice to contribute rapidly to better care.
2.	
Agreement on outcomes meaningful to patients. The personalisation agenda is predicated on the potential to define health objectives important to the recipient of care, rather than in terms of measures that are clinically convenient or simply easily implemented. This will demand improved capture of the patient experience. Approaches that incorporate data on the environment, lifestyle and employment of individual patients, as well as clinical outcomes, are promising.41
3.	
Validated models predictive of the behaviour of individual patients. Models that explain data are not enough: to be clinically useful, models also need to be able to predict future outcomes for individuals in a quantitative framework that can provide a basis for joint decision-making between a patient and his or her neurologist. Key to this is not just the point estimate of risk but also an expression of the confidence with which this is estimated. These models need to be dynamic and able to evolve for more accurate and precise risk estimations as new data become available. Their principles also need to be transparent and described in ways that allow patients and payers, as well as neurologists, to understand and trust them.
4.	
Decision criteria agreed between all relevant stakeholders. What levels of outcome prediction are needed to guide changes in management and with what confidence do they need to be defined? These criteria also must be able to evolve with changes in the way societies and individuals value alternative outcomes, shifting opportunity costs and differences in expectations as new treatment options become available. The principles underpinning these criteria also need to be shared transparently, to encourage dialogue and their use in stimulating innovation in therapeutics. An encouraging new development is work to evaluate the impact of changes in these clinical measures on aspects of life important to patients, such as employment.42
5.	
Enabling of wide access to tools and approaches for personalisation. Shared best practice across the widest community supports equality of access to medicines and will encourage the most rapid growth in data to improve practice. Harmonisation of assessments and intervention strategies is an important part of data collection actionable for care improvements. Demonstrations of cost-effectiveness will be critical to the sustainability of best practice.
Ethical and social considerations
All of the relevant stakeholders – neurologists, patients, industry, regulators and payers – need to be involved in guiding this research and translation agenda if the vision is to be realised. The value propositions for each party are highly interdependent. For example, industry will be unable to drive new drug development with a personalised (or stratified) vision for marketing if regulators do not make it clear that developing the case for a stratified population will be accepted as meaningful or if healthcare payers are not willing to accept the need for the changes needed to enable wide access with individually optimised choices of drugs for MS. Practical clinical use of personalised medicine ultimately will demand that the approaches are cost-effective and accessible.
A key element that cannot be neglected is development of an ongoing dialogue between representatives from all of the stakeholder groups regarding social and ethical issues that arise with personalisation. As trade-offs in public and personal finances always will need to be made, how much does society value ensuring the health of all of its members? What level of uncertainty regarding possible benefit from a medicine will justify withholding – or not supporting – access for some people? Conversely, what level of confidence in the potential occurrence of a serious adverse will justify withdrawing or restricting use of a medicine? How can these decisions be made fairly and reviewed regularly?
Personalised care for MS is not something that can come overnight, although first steps can be taken immediately. Data are still limited and, although models could be built rapidly, ways of incorporating their prospective validation into clinical care are needed. Intentions behind decisions now being made in drug development may not be realised for one to two decades. Regulatory shifts must be made cautiously, with a continued concern for unforeseen consequences in vigilance. Thus, to move forward, stakeholders need to develop a long-term, joint commitment. Focused dialogue leading towards action is needed now if we are to work pro-actively to ensure the best care for people with MS in medical systems that all are experiencing ever-increasing pressures to deliver more for all.
Conclusion
Understanding the heterogeneity of the MS syndrome involves an active process of ‘deconstruction’ to define the biologically distinct diseases included within it and their interactions with individual, patient-specific factors. Coordinated collection and sharing of data, development of predictive models and their progressive evaluation in the care of individual patients will be an essential part of this. At the same time, a much broader range of data on patients will be needed. We should look towards a near future in which care for patients is supported by a comprehensive medical profile including not just clinical data but also that from devices43 and patient-reported outcomes and behavioural, employment and lifestyle data.44 In some instances – with the consent and involvement of the patient – this may include data reflecting personal expectations or areas of concern drawn from non-traditional sources, such as social media or online search sites.45 We believe that ‘personalisation’, if we take its premise as deconstructing the disease heterogeneity to balance benefit and risk optimally in the management of each patient, should be a clinical priority, rather than a clinical ideal. MS care is an area of neurology in which personalised medicine in neurology needs to be championed.

Source: https://journals.sagepub.com/doi/full/10.1177/1352458516672017

------end---------
Cardiovascular models for personalised medicine: Where now and where next?

Highlights
•
Model personalisation requires more than anatomical personalisation.

•
Model uncertainty and sensitivity are important considerations for clinical interpretation.

•
Model verification and validation are critical for trust underpinning clinical decision support.

•
The cardiovascular digital twin will support diagnosis and prognosis by responding continuously to increasing volumes of information collected as the individual goes about their daily life.


Abstract
The aim of this position paper is to provide a brief overview of the current status of cardiovascular modelling and of the processes required and some of the challenges to be addressed to see wider exploitation in both personal health management and clinical practice. In most branches of engineering the concept of the digital twin, informed by extensive and continuous monitoring and coupled with robust data assimilation and simulation techniques, is gaining traction: the Gartner Group listed it as one of the top ten digital trends in 2018. The cardiovascular modelling community is starting to develop a much more systematic approach to the combination of physics, mathematics, control theory, artificial intelligence, machine learning, computer science and advanced engineering methodology, as well as working more closely with the clinical community to better understand and exploit physiological measurements, and indeed to develop jointly better measurement protocols informed by model-based understanding. Developments in physiological modelling, model personalisation, model outcome uncertainty, and the role of models in clinical decision support are addressed and ‘where-next’ steps and challenges discussed.

Previous article in issueNext article in issue
Keywords
Cardiovascular modellingModel personalisationModelUncertainityPhysiological modellingClinical descision support
1. Introduction
This paper is focused on mathematical models of cardiovascular haemodynamics. Each element of the cardiovascular system that is included in the model is represented by one or more equations. Cardiovascular models can be categorised by their purpose and by their dimensionality. An illustration of zero-dimensional, one-dimensional and three dimensional models of an aortic coarctation is presented in Figure 1, and an overview of the purpose of each level of representation is provided in the next section of this paper. Patient-generic models can be used for hypothesis creation, mechanistic understanding, device evaluation or educational purposes. Patient-specific models can be diagnostic, in that they can return characteristic, quantitative measures that might immediately be interpretable to categorise a physiological or pathophysiological state. They can also be predictive: they can forecast how the state will evolve, with or without an intervention. In this position paper, we will focus on the current state-of-the-art and the future challenges when using cardiovascular models for personalised medicine. Figure 2 illustrates some of the considerations that need to be addressed when specifying and deploying a cardiovascular model depending on its purpose. There are several challenges in the integration of modelling into a clinical workflow. These include:
•
Identification of direct inputs to models in data captured in the clinical process (contributing to model personalisation)

•
Identification of direct outputs from models in data captured in the clinical process (contributing to model validation)

•
Personalisation of model input parameters to reproduce model outputs (phenotypes) that are clinically observable in test cohorts. Requires optimisation processes.

•
Association of personalised model parameters with wider data in clinical record (e.g., is resistance and/or compliance associated with age, or body mass index, or co-morbidities such as diabetes). Might include exploitation of machine learning processes for model personalisation and model interpretation.

•
Development of a principled approach to representation of the range of physiological states, from rest through to stress conditions, that an individual patient encounters as they go about their daily lives. The collection of these states for an individual is referred to as their physiological envelope.


Figure 1
Download : Download high-res image (351KB)Download : Download full-size image
Figure 1. Illustration of zero-dimensional, one-dimensional and three dimensional vascular models (courtesy of Massimiliano Mercuri, PhD Thesis submitted to the University of Sheffield 2019).

Figure 2
Download : Download high-res image (806KB)Download : Download full-size image
Figure 2. Considerations in model specifications for cardiovascular simulation (the most important considerations are shown in bold).

The above challenges are specifically about model personalisation, but before this step there are many considerations at the pre-clinical stage. Issues like the selection of appropriate rheological models for blood and generic model and code verification are usually addressed before personalisation is attempted.

2. History
Zero-dimensional (0D), or lumped parameter, models divide the system into compartments within which the fundamental variables are assumed to be uniformly distributed and vary only with time. The governing equations are ordinary differential equations. These models can be used to represent the whole cardiovascular system physiology, or any portion of it. The physiological parameters of pressure, flow and volume can be considered equivalent to voltage, charge and current in electrical analogy models. A comprehensive review of the components of models of this type, including discussion of the basis of the mathematical equivalence between the electrical and haemodynamic concepts, has been published by Shi et al. [1]. These models are readily extended to include the representation of control mechanisms, chemical species concentrations and pharmacokinetics processes. Some of the oldest and most comprehensive systems physiology models are those published by Guyton et al. [2]. More recently these types of models have been integrated with similar 0D systems biology models including the representation of biochemical and electromechanical processes at cellular level. Important early work to include control mechanisms, and to illuminate processes like haemorrhage, was published by Ursino [3]. A very comprehensive set of cardiovascular physiology models, including cellular and cardiac components and including the systems physiology models of Guyton, has been curated and made publicly available through the Cell-ML initiative of the Auckland Bioengineering Institute (ABI) at the University of Auckland [4]. This facility includes tools for the solution of the ODEs and has been one of the most important initiatives of the last two decades in the context of standardisation, documentation, ontological representation, curation and portability of cardiovascular models. There are currently close to one thousand models in the CellML repository, with over 50 circulation models ranging from the simplest with fewer than ten elements to high hundreds.

One dimensional (1D) models, i.e. spatio-temporal models with only one space dimension, are used to describe vascular components in which distribution of quantities along the vessel axis have to be taken into consideration and are essential when wave effects, including transmission and reflection characteristics, are important. The vessels are represented by partial differential equations in time and one spatial dimension. It has been shown formally by Milišić and Quarteroni [5] that in the limit an assembly of 0D models can approximate a 1D system. A comprehensive review of 1D modelling, including the fundamental mathematics and the major published vascular tree models (typically numbering of the order of 100–500 components), has been published by van de Vosse and Stergiopulos [6], complemented by a benchmark study of numerical schemes by Boileau et al. [7]. Often these models are coupled with 0D representations of the heart (variable elastance or single fibre models) to produce closed-loop systems.

The first three-dimensional analyses of cardiovascular components were performed in the early 1980s when it became possible to solve the governing equations of fluid mechanics on large 3D meshes. Most commonly the solution processes were based on finite volume or finite element discretisations of the Navier–Stokes (NS) equations, using custom-written or commercial CFD code. In the earliest applications the 3D domains were often simplified and idealised but over three decades, in parallel with the development of increasingly powerful medical imaging (now often providing time-resolved anatomical and physiological (velocity/flow) data), more and more detailed models were developed and published. More recently some groups used alternative mathematical representations, including for example Lattice Boltzmann formulations [8], [9] that might have advantages for some domains, but overwhelmingly NS solvers have been dominant in cardiovascular applications.

In the late 1990s and early 2000s there was increasing recognition that, as captured in the context of vessel mechanics by 1D models, the cardiovascular fluid domains are not geometrically frozen but are bounded, or separated, by flexible structures (the vessels expand and contract, heart valves open and close). This led to an explosion of 3D fluid-solid interaction models, based on a range of mathematical formulations for the coupling between solid and fluid domains. Stand-out applications for these methods included cardiac mechanics, with one of the first publications by Peskin and McQueen [10], and heart valve mechanics, described by de Hart et al. [11].

Apart from the physics involved and the way it is represented (0D, 1D or fully 3D) also material properties and especially the constitutive behaviour of blood and cardiovascular tissue is determinant for the outcome of the models. For vascular tissues important progress has been made recently [12].

3. Where now?
3.1. The Virtual Physiological Human
A major impetus for computational physiology was provided by the European Commission's Virtual Physiological Human (VPH) initiative, which saw the investment of over 250M€ over an eight year period. The VPH was conceived as a methodological and technological framework that would enable collaborative investigation of the human body as a single complex system. A White Paper, authored by opinion-formers in the community, was published in 2005 [13], and this was followed by the publication of the Roadmap to the Virtual Physiological Human [14], [15], which was based on widespread consultation with the contribution of over 300 active researchers. An overview of the goals of the VPH initiative was published by Hunter and co-workers [16], with an update in 2012 [17].

In the context of vascular applications, a precursor to the VPH was the @neurIST project [18], which featured many of the elements that would become core parts of the community efforts. The focus was on diagnosis and treatment of aneurysms in the cerebral circulation, and one of the drivers was the increased incidence of detection of such aneurysms from advances in medical imaging. A workflow was developed to segment the medical image, to produce a mesh suitable for 3D computational fluid dynamics analysis, to apply appropriate boundary conditions (based on coupling with a circulation model extended to include the neurovasculature) to solve the equations and to extract potential diagnostic measures from the 3D solutions [19]. This European initiative mirrored in several aspects the work of Cebral and his collaborators [20] in the United States. @neurIST also formalised the digital representation of all relevant aspects of the patient data in its Clinical Reference Information Model [21], incorporating over 2300 data items and the beginnings of an ontology for this application. It also produced a prototype clinical decision support system to present information to a clinical end user, integrating model outputs with clinical guidelines.

Some of the largest and most successful projects in the VPH initiative were focused on cardiac mechanics: euHeart [22], [23] Health-e-child [24] and VP2HF [25] were notable examples. All of these were based on exquisitely detailed medical imaging of the heart, coupled with systems physiology models for boundary conditions, and targeted at the primary aims of the VPH; effective diagnosis, evaluation of individual prognosis and representation of the likely effects of potential interventions. Schievano et al. [26] report an important and novel, first-in-man, valve application, and noted that their methodologies ‘challenge the conventional stepwise pathway of bench and animal testing prior to human application, and may be safer and more relevant, potentially reducing the number of animal experiments necessary for testing new medical devices’.

3.2. Model personalisation
Zero-dimensional models with personalised parameters might have clinical utility in their own right in the context of diagnosis and data interpretation. This was explored in an important series of papers by Hann et al. [27], [28] who described processes for the assimilation of clinical measurements, including time-series physiological data, in an optimisation process to personalise parameters in relatively simple systems physiology models. Potential diagnostic application in heart failure was published by Sughimoto et al. [29], who demonstrated the separation of systolic and diastolic dysfunction in heart failure based on the personalisation of elastance parameters in a simple systems physiology model.

The VPH initiative exemplified one critical aspect of model personalisation, namely the description of the individual anatomy based on medical image data. Despite recognition of the importance of the boundary conditions [30], the VPH projects generally had lesser focus on this issue. The projects that were funded often included some rudimentary personalisation of the boundary conditions to match a few clinical measurements. Sometimes measurements of pressure and/or flow at the domain boundaries were integrated explicitly in the 3D models, and sometimes they were used to tune parameters in lower dimensional model representations that were coupled at the boundaries. One of the most important projects to pioneer the latter process in the VPH initiative was the ARCH project in renal dialysis [31], [32], [33], [34].

There is increasing recognition that model personalisation needs to be much more than anatomical personalisation, and the tuning of integrated 3D/0D models to reproduce measured clinical data was a continuous and pervasive theme at the World Congress of Biomechanics in Dublin in 2018 [35]. This was presaged by the observation by Irene Vignon-Clemental, at the International Conference on CFD in Medicine and Biology in Albufeira in 2015 that ‘we are seeing a return to simpler models for clinical interpretation’. Marquis et al. [36] have published a rigorous examination of the personalisation process as applied to a pulsatile cardiovascular model.

Whether personalised 0D models are used independently or as part of multi-scale models, it is often the case, especially in a routine clinical pathway, that physiological measurements that might support a model personalisation process are sparse (e.g., [37]). It is generally true that a personalisation strategy will be most robust (most likely to yield a reliable global minimum of the target cost function) when the number of parameters to be personalised is relatively small, perhaps fewer than ten, and even the most parsimonious systems model has many more parameters. An important element of a personalisation strategy is the identification of the model input parameters to which the target phenotypic outputs are most sensitive. In this context the target outputs include both those that are measured and used for personalisation and those that are used for diagnostic interpretation. In practice sensitivity analysis must be an integral part of a personalisation process.

A special type of measured data is time-series data, when a parameter is measured at multiple time points in the cardiac and/or respiratory cycle. These are often collected in clinical research protocols, and are much richer in information than the extrema (e.g. systolic and diastolic pressure) that are more usually collected in routine clinical pathways. Time series volume and flow data is increasingly available from modern dynamic medical imaging protocols. There has been enormous progress on the mathematical and theoretical underpinning of the process of data assimilation of rich, time series, clinical data. A very promising approach uses unscented Kalman filtering to personalise the parameters in systems physiology models, including in the context of boundary conditions for 3D models [38], [39], [40], [41]. This can be extremely important in situations in which a computational model is used to represent accurately the haemodynamics in the measurement state, perhaps to extract additional parameters that are not directly measured. It might also be very valuable when the aim is to personalise model parameters for subsequent use in simulations of predicted changes under an intervention.

3.3. Recognition of model uncertainty
Clinicians are used to dealing with uncertainty in their decision processes, but often model-based applications return quantitative parameters based on deterministic simulations with no indication of the effects of propagation of uncertainties in the clinical data that underpins the model through to the model measurements and predictions.

One of the major advances for the cardiovascular modelling community in the last decade has been the formal evaluation of model sensitivity and uncertainty [42], [43], [44], [45]. Increasing recognition of the importance of this topic, and the transatlantic community effort to address the challenges, was underlined at the INI Fickle Heart workshop held in June 2019 at the Newton Institute in Cambridge [46].

One of the conclusions from the workshop was that assessments of model sensitivity and uncertainty need to become integral parts of the modelling process, and part of any reported results in a decision support system. In this respect the physiological modelling community has perhaps lagged behind other branches of physics, in many of which measures of error are regarded as essential for data interpretation. Steinman and Pereira [47] present a comprehensive examination of the sources of error and variability in personalised models of cerebral aneurysms.

3.4. Acute v long-term outcome
One of the primary benefits of modelling is its predictive capacity. It is able to predict how a state will evolve both with and without an intervention. Generally, although not without challenges, the prediction of the short-term response to an intervention is massively easier than prediction of the longer-term response. The latter inevitably includes biological remodelling processes that produce profound additional layers of complexity. These remodelling processes represent what is termed phenotypic plasticity, which is the ability of one genotype to produce more than one phenotype when exposed to different environments. We are seeing real progress in the representation of the underlying processes of remodelling of the vascular wall [48], [49], and even some examples in patient geometries [50], but there is much to do to integrate more personalised structural and genotypic data into the modelling process. Our society is increasingly conscious of the benefits of improved lifestyles, and we believe that modelling can play a critical role in understanding the evolution of the phenotype under all types of intervention. Phenotypic plasticity is often measured in large-scale trials, for example of the benefits of exercise, including at public health levels, but modelling has in principle the capacity to promote a causally cohesive understanding of the processes involved. However, it is fair to say that we are currently not capable of modelling the phenotypic plasticity processes associated with, for example, exercise intervention at a sufficient level of detail. A further major challenge is to be able to incorporate the effects of genetic variation on the phenotypic plasticity response as such. Achieving this level of understanding will have profound effects on personalised medicine as we will then be able to predict the health trajectory for a specific individual as a function of intervention regime. Such predictive capacity will most likely substantially influence people’s willingness to adapt to a suggested preventive intervention regime.

3.5. Clinical decision support
A review of the challenges of clinical translation of cardiovascular models was published by Huberts et al. [51]. These included:
•
the identification of the calculations that are of most direct interest to the medical doctor,

•
the identification of the right level of complexity of a model for a particular purpose,

•
the importance of verification and validation,

•
the need to work in a complex legal and regulatory framework.


The importance of the latter points, as well as the recognition by the regulatory authorities of the potential of simulation, is emphasised by the leadership and engagement of the US Food and Drug Administration with the ASME V&V40 initiative [52]. An excellent review of many of the relevant issues is presented by Steinman and Migliavacca [53].

One of the important issues to consider in designing a clinical decision support system is that of resource, both in terms of man-time and compute time. There is always a trade-off between level of automation and robustness, although of course automation can eliminate intra- and inter-observer variability. Compute resource is often not the limiting step in the modern world, although still the execution time for a full fluid-solid interaction analysis might take many days even on high performance computing resource. What is acceptable depends on the clinical scenario. If decisions are taken over a period of days or weeks then a remote service might be appropriate, such as that commercialised by HeartFlow (https://www.heartflow.com/) in the context of coronary fractional flow reserve (FFR), a measure of the capacity to increase flow to an area of the myocardium affected by a diseased coronary artery. This technology, which is approved by, amongst others, the FDA and the National Institute for Health and Care Excellence (NICE) in the UK, is perhaps the most prominent and successful application of model-based predictive clinical decision support in the cardiovascular sector.

If results are required during the course of a single clinical visit or procedure then it is likely that the computations might need to be performed locally, perhaps on the clinical workstation. Such scenarios are commonplace in the clinical environment, and this makes it a fertile area of application for reduced order models because they can yield solutions in timescales that are consistent with this schedule. Reduced order models include lower order (0D or 1D) models, which have underpinned systems physiology models for many years, meta-models, which seek to capture the model behaviour in a simpler model that is fitted to data produced by the full model, and reduced-basis models. The reduced-basis approach, described by Lassila et al. [54], identifies parameters (modes) that are able to represent the model behaviour using a reduced number of degrees of freedom, and shows enormous promise for cardiovascular model application. Gaussian process emulators have also seen an upsurge of interest in the cardiovascular sector in the last five years [55].

3.6. Coronary fractional flow reserve as an exemplar of personalised physiological modelling
Perhaps the most successful penetration of cardiovascular modelling tools into clinical application is in coronary modelling. Coronary fractional flow reserve (FFR) [56] is a measure of the capacity to increase flow to an affected area of the myocardium by removing the blockage caused by a coronary stenosis. It is based on a very simple ratio of the pressure distal to the diseased segment to the proximal pressure. The computation of this parameter exemplifies many of the issues in personalised physiological modelling and its translation to clinical application.
•
The importance of FFR is that it does not simply characterise the local anatomy in isolation. It is a physiological measure. It has long been known that tighter lesions, characterised by a greater percentage blockage of the artery, generally have greater effect on the patient and cause more symptoms. However for the same anatomical blockage, some patients see more benefit from the treatment of the lesion than others. The reason is that the flow to the myocardium is determined not only by the resistance of the artery in which the lesion is observed but also by the resistance of all of the distal arterial and microvascular structures that it supplies. In the simplest representation, for steady flow, there are two resistances in series, representing the diseased artery and the myocardial resistance respectively, and the important question is how significant the resistance of the diseased artery is to the overall resistance. This very simple model leads to the hypothesis that the ratio of distal to proximal pressure, under hyperaemic (maximal flow) conditions, might be indicative of the capacity to restore flow by removal of the resistance in the coronary artery. Measurement of the pressure ratio requires passage of a pressure wire through the lesion, a process that is invasive and not completely without risk. If the coronary artery can be segmented from medical image data, and an estimate of the personal myocardial resistance can be made, then FFR can be computed. This requires the coupling of a local three-dimensional model (or, indeed, a one dimensional model [57], [58]) of personal coronary anatomy to a personalised model of the distal resistance. Morris et al. [59] review the challenges and limitations of the computation of FFR, including the question of the estimation of distal resistance in an individual.

•
Noninvasive computational estimation of FFR is an example of the diagnostic capacity of a model. It produces a measure of the consequence of the disease in the reduction of flow. It is also predictive: the same measure is used to estimate the ratio by which the flow might be improved under an intervention. Because the model describes the system more comprehensively than the simple two-resistor model on which the concept is based, it can more accurately estimate the capacity for flow improvement. Inevitably there is some residual local resistance in the artery after treatment, and this can be simulated in the same way as the diseased artery. It is possible to include a very detailed model of the intervention, for example the deployment of a stent and the interaction with the wall [60], [61], [62], [63], including the contact mechanics in the deformable system where there is the need (determined by an assessment of likely complications), the time, and adequate anatomical and local structural information to justify this degree of sophistication: currently these analyses might be most valuable in the design of stents or of procedures (including strategies for stenting of bifurcations).

•
A major challenge in clinical translation is that very often the model produces measures that are hypothesised to be important, but there simply is not the clinical trial basis to prove the association between the model measurement and clinical diagnosis or outcome. This was very apparent in the @neurIST project outlined earlier. @neurIST produced a series of morphometric, structural and haemodynamic characterisations of a cerebral aneurysm that, based on our understanding of the physics and biology, ought to be associated with the risk of rupture of an individual aneurysm. For example complex, undulating shape, local wall stress concentrations and physiologically abnormal wall shear stress might all be indicative of risk. @neurIST spent 15M€ to develop a comprehensive computational process to estimate indices derived from these parameters, and characterised of the order of 300 aneurysms using these tools. However the incidence of rupture is low, and the investment in the computational workflow was very small relative to what needs now to be invested to cement the associations between these novel indices and clinical sequalae. In contrast, there is a wealth of clinical trial data [64], [65] that has proven that clinical outcome is improved if a coronary fractional flow reserve is used to guide the decision on intervention. This has made computation of FFR a real low-hanging fruit for computational physiology. It did not have to be proved that a new computational measure had value, only that a computational measure could serve as an adequate surrogate for an invasive measure that was already recognised. Several studies have reported this association for models derived from CT [66], [67] and angiographic [68], [69] image data.

•
A further challenge in the invasive measurement of FFR is that, at least in its original concept, for diagnostic interpretation the pressure measurements should be made when the effect of the disease is most apparent – i.e., during hyperaemia. This is induced by administration of a drug, also not without potential drawbacks or complications. If the effect of the drug can be simulated adequately then this can also be included in the modelling process. This also raises the more general issue of simulation of non-rest conditions in all sorts of applications. This was recognised by Marsden et al. [70] over a decade ago, who proposed that respiration and exercise should be incorporated into CFD simulations for realistic evaluation of system performance in congenital heart defects [71], [72], [73], but outside this application there is relatively little literature on the systematic extrapolation of personalised models to multiple physiological states.


3.7. EurValve: applying the lessons learned
The recently-completed EurValve project [74], [75] reflects many of the lessons that we have learned are important for cardiovascular modelling in clinical decision support. The aim was improved clinical decision support for aortic and mitral valve disease. The disease targets were aortic stenosis and mitral regurgitation. In either case the heart works harder to maintain flow. The underpinning hypothesis (echoing the philosophy of coronary FFR) was that it is not the local anatomical severity of the disease that is important but rather its effect on the overall physiology. Left ventricular work and/or peak power might be important diagnostic measures. These parameters can be estimated from a personalised systems physiology model. A prediction of the reduction in these measures associated with an intervention might represent a clinically-relevant quantitative measure of the potential benefit. Furthermore the derived personalised parameters might have diagnostic or prognostic capacity in their own right. An overview of the components of EurValve, highlighting the integration of a core data and compute infrastructure with computational modelling and clinical elements, is illustrated in Figure 3.

Figure 3
Download : Download high-res image (1MB)Download : Download full-size image
Figure 3. Illustration of component interactions in development of a model-based clinical decision support environment.

The current diagnostic process undertaken at the participating clinical centres was elaborated by the clinical partners, cataloguing all of the measurements that are made, under what conditions. A series of tables was assembled to list the parameter, or concept, and its units. A similar table of computational concepts was generated and mapped onto the clinical concepts (i.e. the parameters that were inputs to or outputs from the models were clearly defined and associated with clinical measurements that were used either for model personalisation or for model validation). Snapshots are illustrated in Figure 4. and the complete tables can be found in [79]. This immediately clarified very obvious issues, such as the fact that blood pressure might be measured very many times under variable conditions. Other measurements, for example volume or flow measures, were often made under very different physiological states, and there is a recognised inconsistency between measures made using different imaging modalities.

Figure 4
Download : Download high-res image (752KB)Download : Download full-size image
Figure 4. Snapshot of Concepts from EurValve Information Model [79].

The analysis steps in EurValve exploited many of the methods referenced previously. The most parsimonious model that was able to represent the most important clinical concepts, including cardiac energetics parameters, was identified and a tuning process was developed to personalise the parameters. The primary protocol was based on a characterisation of the valve from 3D computational fluid dynamics analysis based on segmented medical image data. The decision support system, including valve and system characterisation, was designed to be operable within a single clinical visit. The first operation was segmentation of the valve and local portions of the chambers and aorta as appropriate. Then a series of steady state simulations was run (with an open, stenotic, aortic valve or a closed, regurgitant, mitral valve) to characterise the relationship between pressure drop and flow in the appropriate state for the disease process. This characterisation was integrated with the personalised systems model to ‘measure’ the pressure, volume and flow distribution. The total execution time was of the order of fifteen minutes. EurValve also pursued a novel approach that we believe might be the future for integration of sophisticated modelling into clinical decision support systems at the point of care. This exploited Reduced Order Modelling (ROM) technology from ANSYS, using a reduced basis re-formulation of multiple CFD analysis results. The valve and local geometry was parameterised, so that the full 3D geometry could be reconstructed from something of the order of ten or twenty parameters. The solution space was characterised by performing very many simulations across the parameter space and using sophisticated methods to interpolate within this space for any new geometry. With this method the primary computational burden is moved off line, and in this case the simulations were performed on the Prometheus supercomputer in Cracow in Poland. The resulting ROM was installed on a local machine and could return the flow for a given pressure gradient for a new case, as part of the simulation process, in less than one second.

The next step was to decide what simulations to run for an individual. We would argue that the most appropriate boundary conditions for a simulation are not necessarily, indeed not likely to be, those that are measured in the clinic. Most clinical measurements and images are collected in a rest state and/or supine, which might have very little to do with the conditions under which the disease is manifest, especially for cardiovascular conditions in which symptoms are usually associated with exertion. In the EurValve clinical cohort the individuals were monitored, using the Philips Health Watch, over a period of up to two weeks prior to the valve intervention to determine their maximum heart rate as they went about their lives. The model was personalised to the clinical measurements in the rest state, but a process was developed to extrapolate to the exercise state based on published literature for the association between parameters such as LV elastance and heart rate. The model prediction included the reductions in cardiac work and power measures under intervention in both rest and exercise states. There is primary clinical interest in whether these predictions might be reflected in improvement of current outcome measures, such as improvements in the six-minute walk test.

The ultimate goal of EurValve was to produce a comprehensive decision support tool, and this included further components that are out-of-scope for this article but important in the wider context. These included presentation of the clinical guidelines interpreted for the individual and a module for Case-Based Reasoning, so that the user could find similar cases and examine the decisions that were taken and the outcomes. An efficient computational infrastructure supported the routine operation of the modelling process, including model personalisation (integrating heart rate data), characterisation of rest and exercise states, and prediction of the physiological effect of a candidate intervention. Analysis was performed for over 120 individual patients, from three clinical centres. Typical 0D model execution time for one case was under five minutes. Detailed reporting of the results is out-of-scope for this overview article, but a summary of statistics for personalised elastance parameters at rest is presented in Figure 5. It is immediately apparent that there is greater variation in the aortic cohort, and that the values for this cohort are closer to the normal range than those computed for the mitral valve cohort. It remains to be seen whether these integrated model-based characterisations are diagnostic of the severity of the disease or prognostic of outcome, but it has been proven that they can be performed in a relatively routine process in tractable timescales.

Figure 5
Download : Download high-res image (132KB)Download : Download full-size image
Figure 5. Statistics for personalised elastance parameters in left ventricle model for aortic stenosis and mitral regurgitation cohorts (120 individuals in total).

3.8. Where next?
We believe that the fundamental engineering and imaging technology will continue to develop apace. The balancing of the timescales of clinical pathways with computational and quality assurance requirements when presenting clinical decision support is very problem specific. Where the clinical pathway allows there is great merit in a service model in which the clinical team uploads data for remote analysis (already relatively routine in clinical service for radiological reporting of medical image data and now offered by Heartflow for model-based clinical decision support for a coronary application). The potential of reduced-basis ROMs, developed offline using major computational resource but implemented on local infrastructure (including potentially on imaging hardware in the hospital), has already been introduced, and it can be imagined that a new service sector could develop to produce ROMs for all types of clinical applications in the future.

Many of the attempts to personalise system parameters in cardiovascular models have used optimisation or filtering techniques from the engineering community. Most recently there have been increased efforts to deploy machine learning techniques from the artificial intelligence community. Essentially these seek to learn the model from the data rather than to apply a model to compute outputs from inputs. We believe that the integration of these methodologies will produce major breakthroughs in model personalisation, perhaps by using a model to reduce the machine learning challenge and perhaps by learning model parameters from large clinical datasets. This latter function might be particularly useful in situations in which associations exist between observations or parameters, but they are not clearly quantified. An example is the influence of co-morbidities such as diabetes on parameters including microvascular resistance. Although AI and hybrid methods can be extremely powerful, and offer huge potential, it is recognised that the ‘black box’ nature poses additional challenges in the context of verification and validation.

A major deficiency in the whole field of personalised modelling is the capacity to decide on the most appropriate set of simulations for an individual, and the capacity to interpret the results. When designing an aircraft there is a fundamental process of specification of the flight or service envelope1. It is known what challenges that aircraft will be subjected to, and the stresses, strains and fatigue life are evaluated accordingly. We do not seem to have any similar concept of a ‘physiological envelope’ to represent formally the physiological excursions (rest and exercise states) that an individual might make, in what proportions, as they go about their lives. These considerations are included intuitively, and sometimes implicitly or explicitly in clinical guidelines, in the clinical decision process but not in any formal sense in the modelling process. We believe that the characterisation of this envelope (which is a high dimensional physiological data object) will be a significant step forwards in the exploitation of the power of personalised computational cardiovascular models. There is much to do to evaluate how different states accumulate to produce change, for example the difference between intense exercise interval training and more moderate continued exercise, and to develop algorithms to represent the biological processes of remodelling, whether negative or positive. These do exist, but they are not mature. For engineering materials we have cumulative damage rules that can be applied over a duty cycle [76], but generally there is not the data to support physiological system equivalents.

The concept of the digital twin is already becoming reality in applications such as the continuous monitoring, data assimilation and simulation of aircraft engines. We can imagine a future in which a personal digital twin continuously assimilates data streamed from wearable devices [77] and other pervasive instrumentation to produce characteristic and diagnostic measures and to underpin predictive simulations of the effects of all types of interventions, from lifestyle through to medical and surgical options. There are special challenges for clinical application with respect to the consistency and accuracy of medical data. As discussed previously, often the same parameter is measured multiple times and under different protocols: it is recognised that many clinical measurements are very dependent on the details of the modality and of the measurement protocol, which can be different across clinical centres (with variable degrees of calibration), and are often subject to inter- and intra-observer variability. It is not unusual that the modelling community produces excellent and predictive results using data from carefully designed research trials using state-of-the-art measurement technology, but the benefits do not cascade to clinical practice because of the practicalities of routine clinical measurements constrained by economic and time considerations. We would suggest for successful model-based clinical decision support it is imperative that data accuracy and reproducibility, or data certification [78], is considered when developing the processes for interpretation and presentation to clinical end users – and that part of this is the propagation of measurement uncertainty through the predictive models.

The modelling of cardiovascular physiology is just one part of a broader perspective, but it is representative in many ways. The community is moving towards a causally cohesive multiscale and multiphysics representation of human physiology that will capture what we know at a given time, and it will continuously evolve as it is confronted by huge amounts of experimental, genotypic and phenotypic data. This digital twin will run, eat and age. It will integrate enormous amounts of empirical data and physiological insight into a functional and causal whole across scale, space and time, thereby functioning as a highly efficient synthesiser of intellectual capital from various disciplines.

Source: https://ard.bmj.com/content/72/1/3.short

------end---------

Pitfalls and limitations in translation from biomarker discovery to clinical utility in predictive and personalised medicine
Abstract
Since the emergence of the so-called omics technology, thousands of putative biomarkers have been identified and published, which have dramatically increased the opportunities for developing more effective therapeutics. These opportunities can have profound benefits for patients and for the economics of healthcare. However, the transfer of biomarkers from discovery to clinical practice is still a process filled with lots of pitfalls and limitations, mostly limited by structural and scientific factors. To become a clinically approved test, a potential biomarker should be confirmed and validated using hundreds of specimens and should be reproducible, specific and sensitive. Besides the lack of quality in biomarker validation, a number of other key issues can be identified and should be addressed. Therefore, the aim of this article is to discuss a series of interpretative and practical issues that need to be understood and resolved before potential biomarkers become a clinically approved test or are already on the diagnostic market. Some of these issues are shortly discussed here.

Review
Introduction
The strengthening of the robustness of discovery technologies, particularly in genomics, proteomics and metabolomics, has been followed by intense discussions on establishing well-defined evaluation procedures for the identified biomarker to ultimately allow the clinical validation and then the clinical use of some of these biomarkers.

The ability of biomarkers to improve treatment and reduce healthcare costs is potentially greater than in any other area of current medical research. For example, the American Society of Clinical Oncology estimates that routinely testing people with colon cancer for mutations in the K-RAS oncogene would save at least US $600 million a year [1]. On the other side, thousand of papers in the course of biomarker discovery projects have been written, but only few clinically useful biomarkers have been successful validated for routine clinical practice [2]. The following are the major pitfalls in the translation from biomarker discovery to clinical utility:

1.
Lack of making different selections before initiating the discovery phase.

2.
Lack in biomarker characterisation/validation strategies.

3.
Robustness of analysis techniques used in clinical trials.

Each of these details is rarely documented and can dramatically affect the predictive outcome of biomarker results. However, the selection of useful biomarkers must be carefully assessed and depends on different important parameters, such as on sensitivity (it should correctly identify a high proportion of true positive rate), specificity (it should correctly identify a high proportion of true negative rate), predictive value etc. Unfortunately, biomarkers with ideal specificity and sensitivity are difficult to find. One potential solution is to use the combinatorial power of different biomarkers, each of which alone may not offer satisfaction in specificity or sensitivity. Besides traditional immunoassays such as ELISA, recent technological advances in protein chip and multiplex technology offer a great opportunity for the simultaneous analysis of a large number of different biomarkers in a single experiment, which has expanded at a rapid rate in the last decade. However, although many significant results have been derived, one additional limitation has been the lack of characterisation and validation of such technologies. Besides technical characterisation, it also needs quality requirements for correct characterisation of the predictive value of biomarkers. In order to overcome these limitations, some authorities (e.g. Food and Drug Administration (FDA), European Medicines Agency (EMA), European Association for Predictive, Preventive and Personalised Medicine (EPMA), National Institute of Health (NIH)) already set up recommendations, short proposals and minimum information about a variety of bio-analytical experiments that describe the minimal requirements to ensure that the technical performance as well as the predicted value of biomarkers are correct. For example, EPMA tries to outline a number of key issues in research, development and clinical trial studies, including those associated with biomarker characterisation, experimental design, analytical validation strategies, analytical completeness and data managements [3]. Actual paper follows recommendation presented in the ‘EPMA White Paper’ [4]. Current recommendations should serve a set of criteria, which will help to carry on to a high-quality data project. Improvements in the quality outcomes are important because without requirements in the improved selection of biomarkers, correct performance of standardisation and validation, the interpretation of the results as well as the direct comparisons of the predictive value of biomarkers between different research labs or clinical trial studies is not possible. Besides the lack of quality in biomarker selection, a number of other key issues can be identified, which should be addressed in the course of this article. Therefore, the aim of this article is to review and discuss a series of interpretative and practical issues that need to be understood and resolved before potential biomarkers go into the market and become feasible diagnostic tools. The content and structure of the necessary information, as well as potential pitfalls and limitations of biomarker research and validation, are discussed briefly in the next subsection.

Short overview of different kinds of biomarkers
One of the goals of personalising medicine is to use the growing understanding of biology so that patients receive the right drug for their disease, at the right dose and the right time. Although the definitions of personalising vary, they all include the use of different biomarkers driven by a decision-making process in which a diagnostic test is pivotal. Biomarkers include gene expression products, metabolites, polysaccharides and other molecules such as circulating nucleic acids in plasma and serum, single-nucleotide polymorphism and gene variants. Ideal biomarkers for use in diagnostics and prognostics, and for drug development and targeting, are highly specific and sensitive [5]. Biomarkers can also be categorised as pharmacodynamic, prognostic or predictive [6]:

1.
Pharmacodynamic biomarkers indicate the outcome of the interaction between a drug and a target, including both therapeutic and adverse effects [7].

2.
Prognostic biomarkers were originally defined as markers that indicate the likely course of a disease in a person who is not treated [8]; they can also be defined as markers that suggest the likely outcome of a disease irrespective of treatment [9, 10].

3.
Predictive biomarkers suggest the population of patients who are likely to respond to a particular treatment [8, 9].

Predictive biomarkers help to assess the most likely response to a particular treatment type, while prognostic markers show the progression of disease with or without treatment. In contrast, drug-related biomarkers indicate whether a drug will be effective in a specific patient and how the patient’s body will process it. Figure 1 gives an overview of different biomarker categories and types.

Figure 1
figure 1
Clinical biomarkers: categories/types.

Full size image
In Figure 1, the clinical biomarkers for diagnostics determine whether a patient is suitable for treatment with a particular drug (by stratification markers), determine the most effective dose for the patient (by efficacy markers), determine the underlying susceptibility of a patient for a particular side effect or group of side effects (by toxicity markers) or evaluate the course and effectiveness end point of a therapy (by surrogate endpoint markers).

Biomarkers can also be used as surrogate end points (end points that substitute for a clinical outcome such as how a patient feels or functions, or how many patients survive) [9, 11, 12]. Another way of classifying biomarkers is by their role in drug development. Pharmacokinetic or pharmacodynamic biomarkers are involved in early preclinical to phase I studies, and clinical (prognostic, predictive and surrogate) biomarkers play a role in phase II and III trials [10].

The Biomarkers and Surrogate End Point Working Group [13] has defined a classification system that can be used for biomarkers [14]:

1.
Type 0 consists of disease natural history biomarkers that correlate with clinical indices;

2.
Type I tracks the effects of intervention associated with drug mechanism of action;

3.
Type II consists of surrogate end points that predict clinical benefit.

Measurement of different markers (RNA, DNA and/or proteins) needs different diagnostic assays; therefore, different qualification and validation strategies are required.

Pharmaceutical companies are increasingly looking to develop a drug and diagnostic test simultaneously, in a process referred to as drug-diagnostic-co-development so-called companion diagnostic (CDx), to better define the appropriate patient population for treatment. CDx are increasingly important tools in drug development because they lead to the following:

1.
Reduced costs through pre-selected (smaller) patient population;

2.
Improved chances of approval;

3.
Significantly increased market uptake;

4.
Added value for core business (late phase);

5.
Regulatory trend to have CDx mandatory.

The first drug introduced using the personalised medicine paradigm—Herceptin (Trastuzumab; Roche/Genentech, South San Francisco, CA, USA)—has now been on the market for more than a decade. However, the number of drugs marketed alongside CDx remains small (see Table 1).

Table 1 Overview of already approved CDx on the markets
Full size table
Regulatory hurdles have been cited as other main reasons for the slow growth in this area. The differences between the regulatory process in the European Union (EU) and USA and the complexities of the regulatory processes in both regions cause other huge problems for companies. These difficulties affect the preparation of dossiers and their timing and are amplified when considering a CDx project, particularly where more than one company (e.g. pharmaceutical and diagnostic companies) is involved.

Advances in the science underlying drug development have made the discovery of novel biomarkers a real possibility, whilst still challenging, and the use of biomarkers to drive drug development programmes has been increasing steadily over the past decade. Whilst the majority of these biomarkers will not be translated into CDx tests, the growth of biomarker use indicates that the future of the industry will lie in personalised medicine.

As reflected in Figure 2, the search of the scientific literature indicates that many studies report the discovery of different potential biomarkers, but most of them do not meet the criteria of high sensitivity and specificity. The lack of sensitivity and/or specificity leads to a low number of patent application and, in addition to this, to a low number of successful market applications.

Figure 2
figure 2
Overview of the relationship between publications and patenting of biomarkers.

Full size image
If the biomarker used for patient selection is known from the earliest stages of the development process, the process of assay development can begin early, and there will be a selection of diagnostic assay used in clinical trials from an early stage. Biomarkers related to response to therapy are often the result of clinical investigations in patients and may not be available until later in the development programme.

Diagnostic development is undertaken in three stages once a biomarker has been identified. Analytical validation ensures the consistency of the test in being able to measure the specific biomarker. Clinical validity relates to the consistency and accuracy of the test in predicting the clinical target or outcome claimed, and clinical utility relates to the fact that the test should improve the benefit/risk of an associated drug in the selected and non-selected groups. Table 2 describes strategic consideration and implication positions of key stakeholders—regulators, pharma and diagnostics companies, patients, physicians and healthcare providers.

Table 2 Strategic considerations and implications of personalised medicine
Full size table
Case studies of drugs and their companion diagnostics that have been approved over the last 10 years indicate that the number of co-developed products is small. The majority of diagnostic tests available to drive patient selection for particular drugs have been added years after the drug’s approval. However, experience from the EU and USA also indicates that regulators will not approve targeted drugs in the absence of available, relevant diagnostic tests.

Key points to be addressed
According to Issaq et al. [5], the failure in finding high-sensitive and high-specific biomarkers may be attributed to the following factors:

1.
Small number of samples that are analysed;

2.
Lack of information on the history of the samples;

3.
Case and control specimens which are not matched with age and sex;

4.
Limited metabolomic and proteomic coverage; and

5.
The need to follow clear standard operating procedures for sample selection, collection, storage, handling, analysis and data interpretation.

Furthermore, most studies to date used samples with a complex matrix such as serum, plasma, urine or tissue from patients and controls. Another reason for pitfalls in biomarker validation is the usually slow progression of some diseases, requiring high numbers of well-stratified patients who are undergoing long-term treatment when conventional diagnosis and imaging techniques are used. Importantly, there is a lack of sensitive and specific prognostic biomarkers for disease progression or regression that would permit a rapid clinical screening for potential responders and non-responders. Nonetheless, in view of an urgent need for novel therapeutics that have a positive impact on morbidity and mortality of chronic diseases, the field is now moving more quickly towards clinical translation. This development is driven by smart preclinical validation, a better study design and improved surrogate readouts using currently available methodologies and diagnostic techniques. Moreover, upcoming novel biomarkers and diagnostic technologies will soon permit a more accurate and efficient assessment of disease progression and regression.

Considerations before initiating the biomarker discovery phase
Although some biomarkers have been approved by the FDA as qualitative tests for monitoring specific diseases (e.g. nuclear matrix protein-22 for bladder cancer), unfortunately, the majority of found biomarkers (proteins or metabolites) are not sensitive and/or specific enough to be used for population screening. One of the major reasons that proteomic and metabolomic studies over the past decade have failed to discover molecules to replace existing clinical tests is due to errors in either study design and/or experimental execution. Werner Zolg wrote in a review [15] that, before initiating the discovery phase, the first step in the process chain of creating new diagnostic content is to make critical decisions on the sample selection that will directly impact the outcome of the identification process. The very selection of the discovery samples and their degree of characterisation of the material, down to the standard operation procedures on how the samples were acquired and stored, can be decisive for success or failure. By selecting tissue as the discovery material for biomarker identification, one must inevitably choose between cultured cells or specimen directly obtained from patients. There are advantages/limitations to either option.

Consideration on the selection and randomisation of patients for biomarker studies: looking for the ‘ideal’ patients
The optimal selection and randomisation of patients is essential and has to be included in each clinical trial, testing the efficacy of drugs and biomarkers. In particular, given the variant course of disease progression even in well-selected patients with a dominant single aetiology, subjects should be well matched according to factors such as the following lifestyle risk factors: (1) alcohol and tobacco consumption, (2) body mass index, (3) physical activity, (4) signs of the metabolic syndrome or (5) use of (over-the-counter) medications. As in other studies, age and sex should be balanced. In addition, stratification of patients as to their genetic risk of developing a specific disease, (e.g. using a score) will be central to obtaining a balanced randomisation of the placebo vs. the treatment group. These facts alone should significantly reduce the number of patients and the duration of the trial needed to demonstrate a significant reduction of disease progression or induction of regression. Histological end points in proof-of-concept trials will still be required by regulatory authorities, apart from long-term hard end points, such as morbidity and mortality in phase III trials. At present, it is not possible to exactly predict the number of patients and the time on treatment that are needed to demonstrate the clinical benefit of a drug agent or biomarker. This is one major reason that companies have been reluctant to enter this difficult field.

The current state of biomarker discovery
The search of the scientific literature clearly indicates that most published biomarkers are inadequate to replace an existing clinical test or that they are only useful for detecting advanced disease stage, where the survival rate is low. Many molecular or genetic biomarkers have been suggested for the detection of different diseases; however, most of them do not possess the required sensitivity and specificity. Another reason why most proposed metabolomic and proteomic biomarker results that have not progressed from the laboratory to the clinic study is that the majority stopped at the first phase of biomarker discovery. According to other studies [5, 16, 17], there are five phases that a protein or a metabolite has to go through to become a biomarker. Phase I is preclinical exploratory studies to identify potentially useful markers, phase II is clinical assay development for clinical disease, phase III is retrospective longitudinal repository studies, phase IV is prospective screening studies and phase V is control studies [5].

Listed examples of already approved biomarkers in Table 1 show that there are no 100% sensitive and specific biomarkers for different types of diseases to date. A biomarker with a high sensitivity has a low specificity and vice versa. Unfortunately, biomarkers with ideal specificity and sensitivity are difficult to find. One potential solution is to use the combinatorial power of a number of different biomarkers, each of which alone may not offer satisfactory in specificity. For example, Horstmann et al. [18] studied the effect of using a combination of bladder cancer biomarkers on sensitivity and specificity. Although none of the combinations resulted in 100% sensitivity and specificity, the sensitivity improved from 91% (using two biomarkers) to 98% using a combination of four different biomarkers.

Pitfalls and limitations
However there exist different reasons why most potential biomarkers failed in achieving adequate sensitivity and specificity and are not accepted as clinical tests. One main reason is that most biomarkers are dealing with detecting diseases at an early stage in humans that have different age, sex and ethnicity. Other important fact is to find a protein or a metabolite at an extremely low concentration level among thousands of other proteins and metabolites. To improve sensitivity and specificity, there are different strategies: potential solutions are listed as follows:

1.
Improve the assay (e.g. antibody with a higher specificity and/or in combination with a detection conjugate with a higher sensitivity),

2.
Combine several markers,

3.
Check for subpopulations and stratify population (e.g. matched by gender, age, pathology).

The current procedure for the search of biomarkers is dealing with potential errors in the study design that can be avoided in future studies.

Figure 3 gives an overview about two main reasons why most potential biomarkers failed in achieving adequate sensitivity and specificity and are not accepted as clinical tests. One main reason is pitfalls and limitations in biomarker discovery and second main reason is pitfalls in biomarker validation.

Figure 3
figure 3
Pitfalls and failures in biomarker identification.

Full size image
Age, sex and race
Biomarker studies are normally carried out using body fluids or tissues collected from patients and healthy subjects of different ages, sex and race. Using samples from patients and controls that are of different ages and sexes can dramatically influence the results. In a recent study, Lawton et al. used 269 subjects, 131 males and 138 females, to study the effects of age, sex and race on plasma metabolites. The patients were of Caucasian, African-American and Hispanic descent and ranged in age from 20 to 65 years. The subjects were divided into three different age groups; 20–35, 36–50 and 51–65. Using GC/mass spectrometry (MS) and high-performance liquid chromatography (HPLC)/MS, they reported that ‘more than 300 metabolites were detected of which more than 100 metabolites were associated, with age, many fewer with sex and fewer still with race’ [19].

Selection of patients and controls
Patients for biomarker studies should be carefully selected by a specialist (e.g. oncologist for cancer studies or a pathologist for tissue samples) to insure the presence or absence of diseases. Unfortunately, predictive curve values of biomarkers with no or less overlapping of diseased vs. non-diseased cohorts are difficult to find. There exist always more or less overlapping areas between healthy and diseased cohort. The overlapping area allows the analyst to calculate the proportion of patients whose diagnosis was correctly predicted by the model (true positives for sick patients and true negatives for healthy patients) or false negative or false positive values [3].

Generally, the number of patients and control subjects in published studies is very small to give an acceptable statistical value. Also, many of the potential proposed markers have not been confirmed or validated in a high-quality manner. Body fluids and tissues are collected from a group of patients of different disease stages, and results are compared with a group of healthy persons. The effect of a disease stage on sensitivity of a single biomarker should be taken into consideration as mentioned previously because sensitivity improves with increase in disease stage. Grossman [20] adequately summarises the importance of consistency through his observation that ‘the contradicting published reports likely [resulted] from studies testing different patient populations, using different methodologies, and applying different [cut-offs] for a positive test’.

Errors in study execution
Study execution deals with experimental parameters that need to be considered. These parameters include many different variables, such as sample collection, handling and storage, sample comparison, number of samples, sample preparation, methods of analysis and number of replicates.

Sample collection, handling and storage
Samples are collected from a person who passed a physical exam by a physician who determines that the person of interest has a concrete disease or is healthy. Samples (serum, plasma, urine, saliva, tissue etc.) should be collected in freezer-type tubes, immediately snap frozen and stored in a freezer until time of analysis. It is recommended that, for short-time storage (less than 1–2 weeks), storage condition should be at −20°C, and for long-term storage (more than 2 weeks), storage condition should be at −70°C. At the time of analysis, samples should be thawed at 4°C or on ice and prepared according to the selected method of analysis. The history of the sample is very important and may have been obtained from sample storage banks with proper collection, storage information about the stage of disease, medication, pathology, age, gender and condition of patients. A lack of consistency in sample collection and storage can doom a study before any data are even collected.

Direct sample comparison
If this option is chosen, the degree of sample characterisation is critical. It is of importance that the specimens used in the diseased cohort are not simply classified as ‘diseased’ (if possible, together with the stage of the disease) but that a detailed histopathological assessment of the distribution of cell types (e.g. tumour cells, necrotic cells, stroma) in the diseased specimens is carried out [5]. This distribution should be as uniform as possible in all samples, and it should represent the correct disease/healthy state. Otherwise, normalising the analytical outcome becomes very difficult.

Number of samples
The number of samples that have to be placed in the diseased and healthy control groups in order to be compared with a variety of analytical approaches remains a matter of discussion. A minimum of 15 samples in the discovery phase is necessary to get a reasonable representative selection basis for marker candidates. If the number is for practical reasons (resources, cohort and time lines), which is very small (e.g. less than 10 per group), then the observed differences between the two sets of specimens are in danger of being over-interpreted when extrapolated to generalised cohorts. Low sample sizes make the correct identification of those differences increasingly difficult. To overcome these limitations, Zolg [15] recommended running second and third discovery rounds to complement the results of the first round. Ideally, the sample number analysed should not only allow stating the presence or absence of a given protein, but should also give the opportunity to identify trends. Another opportunity is to pool the samples, i.e. to physically combine several of the extracts to create fewer samples, to be put through the entire analytical process. Pooling of samples inevitably leads to a loss of information. The distribution of proteins is averaged by the very pooling process with the prospect that individual proteins are pushed below the detection limit by one member of the pooling cohort not expressing the protein in question. At any rate, somewhere in the selection process, the individual spectrum of proteins has to be established. Therefore, the pooling process just shifts the workload to a later point in the process chain, and very good arguments have to be found to deliberately increase the complexity of the data sets by pooling.

Sample preparation
Preparations of the sample for proteomic and metabolomic analyses prior to analysis are very important and can introduce errors that always will affect the final results [3, 5]. The search for biomarkers in biological samples involves different steps depending on the sample type and if it is analysed for metabolites or proteins. Extraction of metabolites from the blood, urine or tissue required multiple purification and extraction procedures using different solvent systems as discussed by Want et al. [21] and Issaq et al. [5]. It is not always possible to extract or to isolate all the metabolites from a sample with a single solvent since metabolites have different chemical and physical properties and are present in a wide dynamic range of concentrations. The search for a protein biomarker involves extraction of the proteins followed by fractionation, purification, specific enrichment and then analysis by different analytical methods (e.g. 2DE-PAGE, immunoassays, Western blot, HPLC/MS/MS). Analysis of the blood as well as the serum is more complicated than that of urine or saliva as it contains fewer proteins, and high-abundant protein must be depleted prior to analysis. Approximately 99% of the protein content of the blood (both serum and plasma) is made up of only about 20 proteins (http://www.plasmaproteome.org) [22]. While depletion of these proteins will allow the detection of low-abundant proteins, it may remove proteins that are bound to these 20 proteins, resulting in a loss of potentially important information [23]. Tissues are homogenised first followed by metabolites, and proteins are extracted and analysed. Incomplete homogenisation can lead to losses that can affect the accuracy of the results. In addition, one cannot ignore human errors in sample collection, storage, weighing, extraction etc.

Methods of analysis
Choosing the optimal analysis method is critical in biomarker search by proteomics and metabolomics. For example, analysing the plasma proteome involved protein precipitation and solubilisation; therefore, the downstream fractionation method must be either electrophoresis or a liquid-phase method. Unfortunately, studies have shown that the proteome analysis by groups using different methods resulted not only in different numbers of protein identifications, but also in poor overlap between the results [5, 23, 24]. These results prove that the selected method of analysis is an important parameter.

Number of replicates
Sample should be analysed in triplicate and report the mean and standard deviation. Unfortunately, most published proteomic and metabolomic studies only analyse each sample once, which does not permit the deviation from the mean (i.e. the error in the measurement) to be calculated. Proteomic analysis of a biological sample involves different analytical steps in the course of sample preparation. Each one of these steps can introduce an error. Due to difficulties either in sample preparation, in protein preparation or in assay or protein chip hybridisation, the amount of replicas varied from zero to six. Thus, implicating different optimal statistical tests were necessary for the various settings.

Consideration on the improvement of current efficacy readouts by development of non-invasive diagnostic tools
Further improvement is desirable to reduce the number of study patients, trial duration, costs and, most importantly, possible risks for individuals. Thus, new innovative diagnostic techniques are needed that allow an exact assessment of the degree of disease and, more importantly, of the dynamic processes underlying the diseases. Such biomarkers and technologies will have to be specific for the targeted structure, i.e. the cells or key molecules involved in the development of the disease. Ideally, sensitive and specific markers/imaging methodologies will allow a rapid and mechanism-based screening for and efficacy monitoring of treatments. Additionally, there is a need for universal-standardised reporting methods to aid interpretation and comparison of potential clinical biomarker trails. All current non-invasive methodologies (serum markers, serum marker algorithms, contrast imaging etc.) yield a sufficient to excellent diagnostic accuracy for the detection (or exclusion) of an upcoming or current disease.

Regulatory outlook and future aspects
The regulatory landscape for biomarker discovery and validation projects (especially for drug-diagnostic co-development = companion diagnostic) is evolving and getting more important to the upcoming clinical trial studies. In the past few years, available data have been reviewed by FDA and EMA, and experience from some exploratory data submission process was used to create a formal biomarker qualification purpose [25].

Both the FDA and EMA have similar biomarker qualification processes in place that enable research institutes and pharmaceutical companies to obtain advice or qualification of the biomarker in question. In both cases, similar guidance concepts were developed that are very clear on the fact that biomarker qualification does not constitute a review of a diagnostic for commercialisation. Nevertheless, for the future, biomarker qualification submissions are strongly recommended by US and EU authorities and will be more and more required for drug/diagnostic co-development projects in both regions [25]. Further guidance on clinical trial enrichment and internal standard operating procedures for cross-labelling efforts are also expected and will improve the penetration of personalised medicine in clinical practice.

The FDA’s first guideline was finalised in 2005, and it is based on the fact that many clinical trial studies were utilising biomarkers but that these data were often exploratory and that their regulatory submission was not required [25]. However, the US regulatory agency regarded the submission of these data as beneficial for both the industry and the FDA to ensure that regulatory scientists are familiar with and are prepared to evaluate future submissions. This data mainly includes pharmacogenomic information, and the programme is referred to as a voluntary exploratory data submission (VXDS). The success of this VXDS programme has led to the development of a number of new (draft) guidance documents including those related to the biomarker qualification process and to clinical pharmacogenomics in the early phases of drug development. Further guidance on clinical trial enrichment and internal standard operating procedures for cross-labelling efforts within FDA offices is also expected and is continuously under discussion.

Since the FDA’s initial publication, the International Committee on Harmonisation (ICH) has published a guidance relating to pharmacogenomic data (ICH E15) that defines pharmacogenomics, pharmacogenetics, genomic biomarkers, and relevant sample and data coding. Standardised terminology is presented for incorporation in future regulatory documents related to pharmacogenetics and pharmacogenomics. Further ICH guidance, topic E16, on the information required for biomarker qualification was published in 2010. In addition, the FDA has established processes for working jointly with EMA on the review of exploratory information. A review of their experience and the impact of the guidance were published in 2010 [16].

Conclusions
While application of potential biomarkers in preclinical development is far advanced, only a handful have passed clinical trials (see Table 1) and are already commercially successful on the market (see Table 3). Reasons for the pitfalls are manifold, including difficult validation strategies and the usually slow disease progression, requiring high numbers of well-stratified patients undergoing long-term treatment when conventional diagnostic parameters or related end points are used. Importantly, there is a notorious lack of sensitive and specific surrogate biomarkers for disease progression or regression that would permit a rapid clinical screening for potential drug candidates. Nonetheless, in view of an urgent need for new drugs that positively impact morbidity and mortality of different diseases, the biomarker field is now moving more quickly towards clinical translation. This development is driven by thoughtful preclinical validation, a better study design and improved surrogate readouts using currently available methodologies. Moreover, upcoming novel biomarkers and imaging technologies will soon permit a more exact and efficient assessment of disease diagnosis, disease progression and disease regression as already published in other works [26, 27].

Source: https://link.springer.com/article/10.1186/1878-5085-4-7

------end---------

Personalised Medicine: The Promise, the Hype and
the Pitfalls

Personalised Medicine: The Promise,
the Hype and the Pitfalls
Therese Feiler, Kezia Gaitskell , Tim Maughan,
Joshua Hordern
University of Oxford, UK
In engaging critically with personalised medicine and mapping pitfalls which
mark its progress this project aims to stimulate conversations which deal intel ligently with controversies for the sake of consensus. We aim to ask the ethical
questions which will lead to the improvement of healthcare and we take an
open-minded approach to finding answers to them over time. What is or
should be meant by ‘personalised medicine’ is a major theme of this issue.
It is a debate bound up with question of both values in the sense of ethical
reflection and value in the sense of economic return. This editorial discusses
and interrelates the articles of the issue under four headings: the promise and
the hype of personalised medicine; the human person and the communication
of risk; data sharing and participation; value, equity and power. A key intention
throughout is to provoke discourse and debate, to identify aspirations which
are more grounded in myth or hype than reality and to challenge them; and
to identify focussed, practical questions which need further examination.
keywords personalised medicine, promise, hype, person, risk, data sharing,
value, equity, power
The articles which form this special issue of The New Bioethics have arisen from the
project Personalised Medicine: the Promise, the Hype, and the Pitfalls, a collabor ation between the University of Oxford Healthcare Values Partnership (www.health
carevalues.ox.ac.uk) and the MRC stratified medicine consortium in Colorectal
Cancer (S-CORT). The conference which formed the context in which many of
the papers in the volume were initially presented was held in association with
CASMI-Academic — the Centre for Advancement in Sustainable Medical
Innovation — and the University of Oxford’s Centre for Personalised Medicine. As
editors, we express our thanks to these partners and to the funders of the project, the
Sir Halley Stewart Trust and the University of Oxford Wellcome Trust Institutional
Strategic Support Fund (grant number 105605/Z/14/Z); and the funder of the next
phase of the research, the Arts and Humanities Research Council (grant number:
AH/N009770/1). We are also very grateful to the editors of The New Bioethics
for granting us the opportunity to edit this special issue.
A critical approach to ‘personalised medicine’
In engaging critically with personalised medicine and mapping pitfalls which mark
its progress this project aims to stimulate conversations which deal intelligently with
controversies for the sake of consensus. We aim to ask the ethical questions which
will lead to the improvement of healthcare and we take an open-minded approach
to finding answers to them over time.
Nonetheless, a right desire for consensus must reckon with the controversial
nature of the enterprise which has come to be known as ‘personalised medicine’.
There is vigorous if polite debate in journals such as the Lancet about whether the
entire enterprise is ill-conceived and whether the focus of research spending
should be shifted very substantially to public health: some say that the promises
have not been delivered and are unlikely to be delivered (for debate see e.g. Coote
and Joyner 2015; Dzau et al. 2015; Matuchansky 2015). Moreover, even in using
the term ‘personalised medicine’ we are straight into critical work: what is meant
by ‘personalised medicine’? Is not all medicine personalised, in some broad sense?
In order to focus on medicine which operates at the molecular level, should we
not really say ‘stratified medicine’? After all, it is arguable that no medicine
should be promised that is truly tailored to an individual person, not only because
that is generally scientifically and clinically implausible but also because it unkindly
distorts the expectations of patients and the general public and may limit the under standing of the person to characteristics which are biomedically definable.
What is or should be meant by ‘personalised medicine’ is, therefore, a major theme
of this issue. It is a debate bound up with questions of both values in the sense of
ethical reflection and value in the sense of economic return. Indeed, an emphasis
on how values shape decisions made about medical science’s role in society has
become increasingly urgent. A UK House of Lords Science and Technology commit tee report in 2000 made the following observations on what it sees as a more gen erally troubled relation between science and society:
Some issues currently treated by decision-makers as scientific issues in fact involve many
other factors besides science. Framing the problem wrongly by excluding moral, social,
ethical and other concerns invites hostility … Underlying people’s attitudes to science are
a variety of values. Bringing these into the debate and reconciling them are challenges for
the policy-maker. (House of Lords Committee on Science and Technology 2000)
This suggests a rationale for why humanities disciplines such as theology and
philosophy must be involved in the debates alongside social scientists, clinicians
and medical researchers. Values lie at the heart of healthcare in general.

Personalised medicine defined in the sense given below is no different. Humanities
disciplines have both the competence and responsibility to explore and critique
the values which are in play in any given activity. Everybody has values — scien tists, physicians, patients, policy-makers, politicians. Everyone is invested in some
way — whether professionally, as a patient, policy maker or member of the general
public.
Humanities disciplines have the capacity to generate discussion about that invest ment and to press people towards asking important, sometimes challenging ques tions about values. Such modes of enquiry, embedded in and intertwined with the
history of present society and modern medicine, are capable of illuminating implicit
assumptions at work in enterprises such as personalised medicine. For example, in
this specific project, theological reflection on values has played an important role
in framing the questions. The desire to test out a critical juxtaposition of promise
and hype was a theologically informed thought, seeking discernment of how truth fulness, personhood and promises about the future fare in precision medicine
research.
The UK Government’s Chief Scientific Adviser’s report for 2014 emphasised the
need for interdisciplinary engagement in scientific research:
in engagement — as in innovation itself — there are key roles for the creative arts, huma nities and civil society. Some approaches are less formally structured than others, invol ving uninvited as well as invited engagement…diverse forms of public engagement can
supplement, enrich and inform (rather than substitute) the conventional procedures of
representative democracy. (UK Government Office for Science 2014, p. 61)
The report worries about a closed shop in public reasoning about science and argues
instead for the ‘scope for dialogue to be extended to the workplace, the neighbour hood centre, the household and the church’ (UK Government Office for Science
2014, p. 67). On this view, public reasoning about the significance of genomic
healthcare is underpowered unless it involves a critical awareness of key concepts
and debates scrutinised in the humanities.
One interdisciplinary example of this critical literature is a 2015 volume edited by
Joachim Vollman et al. representing a major collaboration funded by the German
government and reflecting a rich debate in German society drawing on many
voices from oncologists to theologians to policy makers. It represents an important
counterpoint to the widespread, largely uncritical acceptance of ‘personalised medi cine’, especially in the Anglophone world (for an exception, Tannock and Hickman
2016). It also finds an echo in the recognition by political and social theorists of no
religion such as Habermas and Zizek that their sources of interpretation of the world
are intellectually shallow unless they pay attention to theological and religious
traditions.
Vollman addresses the vagueness of the term ‘personalised medicine’, suggesting
that three main positions can be identified: ‘(a) PM is not a new concept as medi cine has always been individualised (b) PM is holistic health care centred on the
needs of the individual patient (c) PM is treatment targeted at stratified sub groups’ (Vollmann et al. 2015, p. 9). Later in the volume one definition in par ticular is argued for, after an extensive systematic literature review of 2500
articles which contained 700 different definitions. This definition will be taken as
the point of departure for this volume, if not the final destination for every author
in it:
Personalised Medicine seeks to improve stratification and timing of health care by utilis ing biological information and biomarkers on the level of molecular disease pathways.
(Vollmann et al. 2015, p. 21)
The promise and the hype of personalised medicine
This account is, however, not the first thing the general public might think when they
hear of ‘personalised medicine’. Instead, the focus on the personal in personalised
medicine may generate a mystique of unrealisable promises and expectations
about medicines and treatments perfectly tailored to each individual person. The
promissory nature of medicine in this field is the focus for the articles in the first
section of this volume. The authors focus on how clinical, commercial and other
sectors talk about personalised medicine — what promises are made; to whom
are they made; can they be fulfilled; what drives the making of promises? How is
the potential of personalised medicine to be distinguished from overstatement
amounting to hype, if indeed the distinction between hype and promise can be accu rately applied to the uncertainties which naturally attend the scientific enterprise?
How are we to assess whether hype, if it exists, is good, bad or just necessary in
order to keep the show on the road?
These questions are posed in a context in which a significant gap has emerged
between the promise of progress made by industry, researchers and politicians on
the one hand and the reality of biomarker-based personalised medicine in research
findings and clinical practice on the other. This raises issues of both historical and
current clinical importance. How good are those involved in personalised medicine
at distinguishing between optimism and over-promising (even intentional exagger ation or distortion)? How should the phenomena of hype and promise be best under stood from historical, philosophical, sociological and theological perspectives? Have
population health approaches suffered unduly because of personalised medicine
hype? Are researchers and clinicians more susceptible to imprudent hype around
genomic medicine than patients? How might scientific, clinical and public expec tations be best protected from the effects of unrealistic hype and unfulfilled promises
while not extinguishing realistic hope and appropriate ambition?
In his article, Tim Maughan addresses many of these issues in relation to cancer,
noting that with a few outstanding exceptions, progress has fallen below expec tations because of the challenges of tumour heterogeneity and clonal evolution.
While in both benign and malignant disease, diseases caused by single genetic altera tions are more amenable to precision medicine approaches, most common diseases
are caused by a complex interplay of multiple genetic and environmental factors
making personalised medicine far more challenging. Drawing on his experience as
both a clinician and researcher he observes that the current optimism for personal ised medicine is distorting clinical consultations, resource allocation and research
funding prioritisation. He concludes that a research active clinician must act both as an agent of change and development, and as a communicator of realism. Only a
personalised medicine that includes a sober appreciation of what genomics can
achieve, together with continued focus on the individual as a person not just as a
genome, will contribute to further improvements in health and healthcare.
In a similar vein, Kezia Gaitskell details some of the ways in which ‘personalised
medicine’ has had a positive impact on disease screening and prevention by enabling
more person-specific estimates of risk, and hence more personalised strategies for
screening and risk reduction. But she also outlines potential difficulties and limit ations of this approach and challenges the assumption that ‘personalised’
approaches are necessarily superior for prevention and screening. She argues
especially for a renewed focus in research and practice on established and proven
public health approaches, which typically lose out to high-tech, innovative research.
Steve Sturdy’s contribution acknowledges these two lines of concern but takes its
own approach. First, it queries the appropriateness of focussing analysis via a strong
contrast between hype and promise. He argues that since the former is a necessary
feature of any scientific enterprise, especially one which depends on private commer cial backing, the proper critical focus should really be on how and why particular
kinds of scientific promises are being articulated and circulated in the present. Chart ing the development of the biotechnology sector from the 1970s, when a focus on
molecular biomarkers became prevalent, through to the present day, Sturdy offers
a historical and sociological analysis of personalised medicine with attention to
the way that promissory language linked together the concerns of government
and commercial interests. He observes that personalised medicine has been much
better at fulfilling its promise to reward private investment, whether in the pharma ceutical or biotechnology sector, than at delivering savings to health services and
their funders. Applications that might secure economies in healthcare but undermine
profitability have largely been ignored and the dominant direction of personalised
medicine continues to be towards the production of new and expensive treatments
for small populations of patients. While Sturdy thinks it unlikely that this trajectory
can be sustained much longer, he also proposes, without much hope of success in
light of the overall direction of health policy, that the problem of mounting costs
for diminishing medical returns would be to decouple medical innovation — at
least insofar as it relies on public funding — from the overriding expectation of com mercial profit.
While each of these contributions represents a particular line of critique concern ing the enterprise of personalised medicine, each is also in their own way a construc tive attempt to remedy problems rather than simply describe them. Maughan and
Gaitskell take clinically contextualised approaches to addressing questions of
hype and promise. Sturdy’s approach, by contrast, suggests a very far-reaching
policy approach which may, as he recognises, be impractical considering the inter meshed nature of commercial and public actors in the actual research enterprise.
If Sturdy is right about the unsustainability of the way that personalised medicine
is developing, it would seem crucial that research and policy addresses the
shaping force of the profit-motive in a way which informs the interaction of commer cial and public interests. A suitable policy would have to be realistic about the
sources of innovation, other-regard and selfishness which lie in both private and public sectors. This would have implications for fostering the kind of research
environment which serves the public interest, in which temptations to neglect
large-scale public benefit may overwhelm even the best motivated private actors.
Such an approach would connect well both with Maughan’s vision of the clinician’s
mediating role amidst the factors which shape patient experience and of Gaitskell’s
emphasis on ensuring that population-level public health approaches are not forgot ten in the search for ever more precise approaches to prevention.
Bearing these critical concerns in mind about hype and promise, the papers which
follow explore various pitfalls which may mark the clinical and research trajectory
as it heads down the road of applying stratification or personalisation in healthcare.
These cluster under three main themes:
• The human person and the communication of risk
• Data sharing and participation
• Value, equity and power
The human person and the communication of risk
In this project, we have noted the importance of distinguishing between different
possible meanings of ‘personalised medicine’ and of analysing the experience of per sonalised medicine by patients and healthcare workers within the disciplines of
social sciences and humanities. On the one hand, notions of biological determinism
might shape a certain biomedical, genomic understanding of the person in ‘person alised medicine’. On the other, and preceding this narrow genomic usage but now
re-emphasised as an overarchi
ng category, another sense of ‘personalised medicine’
has focussed on relational aspects of care, individual beliefs and values, shared
decision-making, risk communication and strategies for individualised compliance.
In a related but distinct way, typically emphasised in medical humanities disciplines
such as theology and philosophy, patients are understood as persons whose self knowledge and significance to others are not circumscribed by an understanding
of their genetic profile and risks. Such knowledge and significance may nonetheless
become submerged in healthcare systems as expectations of the predictive and thera peutic powers of genomic technologies come to dominate the hopes of patients and
healthcare professionals alike. An important question sits alongside these issues. It
concerns what social vision of human persons can underpin a political solidarity
in suffering in which stratified medicine might play an integral part. But in order
to realise such a vision in practice and ensure that scientific progress continues to
command public support, it is important to understand public perceptions of risk
and value — of what really matters to patients.
In this context, Rob Horne makes a fundamental claim about the shape of the
field by arguing that 4P medicine should be reconceived as 5P medicine in order
to incorporate the psycho-social dimensions of care. Building on his earlier work
in the field, Horne argues that technological advances enabling us to personalise
medical interventions at the biological level must be matched by parallel advances
in how we support the informed choices essential to patient and public participation.
Since we cannot take participation for granted, medicine must take account of the perceptions and capabilities that shape participation. Using analogies from dysto pian literature and evidence from social sciences, he argues that we need a better
understanding of how people perceive personalised medicine and how they judge
its value and risks. To realise the promise of personalised medicine, 4P medicine
must be personalised at the psychosocial as well as biological dimension —
putting the person into personalised medicine.
Alastair Kent takes a complementary end-user perspective and draws on Genetic
Alliance UK’s evidence from a Citizen Jury to highlight why what matters to patients
should be factored into how personalised, or stratified, medicine develops its tar geted therapies. While it is unlikely that these novel therapies will provide complete
cures, they will address some but not all symptoms of a condition. The key issue,
therefore, is that early engagement with patients and families will ensure that devel opments are targeted at those aspects of a condition which really matter, not just
those that are easy to count. He argues that this will make the development
process more efficient, and improve the likelihood that patients will be able to
access therapies if the development process is successful. The question of what
really matters to patients is therefore crucial to how research in this field and com munication about risk and value proceeds.
Joshua Hordern’s article takes a further step to consider how stratification and
genetic risk shapes a person’s self-knowledge and knowledge of others. His argu ment takes its keynote from theological interpretations of the human condition
and the promises of God and challenges notions of personhood which are overly
dependent on characteristics which are biomedically definable. In particular, he con siders why and how self-knowledge is important to understanding risk by arguing
for four claims. First, he casts doubt on whether genetic knowledge should properly
be called ‘self-knowledge’ when its ordinary effects on self-motivation and behav iour change seem so slight. Second, he identifies various temptations towards a
reductionist, fatalist, construal of persons’ futures through a ‘molecular optic’, temp tations which ought to be resisted. Third, more constructively, he argues that any
plausible effort to support behaviour change must engage with cultural self knowledge, values and beliefs, catalysed by the communication of genetic risk.
For example, while a Judaeo-Christian notion of self-knowledge is distinctively
theological, people’s self-knowledge is plural in its insight and sources. Finally, he
makes the case that self-knowledge is found in compassionate, if tense, communion
which yields freedom from determinism even amidst suffering. Stratified medicine,
he suggests, offers a newly precise kind of humanising health care through societal
solidarity with the riskiest that can yield this kind of self-knowledge. However, he
also notes, with reference to the parable of the lost sheep, that stratification may
also mean that patients who do not fit into the sought after molecular subtypes
experience accentuated suffering and disappointment. This may represent a forgot ten downside of the core theological concern about hype and is a theme taken up at
the end of this editorial to be addressed in future research.
The overriding emphasis for these three articles concerns how the subtle interplay
between senses of ‘personalised’ are actually crucial to the progress of ‘personalised
medicine’ understood in the molecular sense that Vollman et al. define above. The
questions which require further analysis include the risks to care of persons attendant on a focus on biomedical cure, construed at a molecular level; whether it is
possible and beneficial to stratify on the basis of psychological factors as well as bio logical ones; how healthcare professionals might be better trained to pay attention to
the interaction of the communication of genomic information with the values and
beliefs of patients, especially as regards their future happiness; and how the entire
enterprise of personalised medicine might develop a wiser compassion for the
patients as persons whom it seeks to serve.
Data sharing and participation
In the previous section, issues of how the public perceive participation in personal ised medicine were raised. But a distinct set of problems in relation to a participative,
personalised medicine are raised with respect to data sharing. On the level of prac tical policy, there are national and international databases which are not interacting
sufficiently with each other to make as much progress as researchers and patients
desire. The underlying dimension of this policy problem concerns the motivations
for collaboration and participation. The combination of self-interest and com passion for present neighbours and future generations which inspires data sharing
is liable to be undermined by fears of data loss and trust violation. This in turn
may lead to the worry that the pearls of data shared by well-motivated and
willing patients’ consent may be trampled underfoot in the pursuit of profit. The
three articles in this section take distinct approaches to this set of concerns.
Mark Lawler focusses on the challenge of embedding a culture within the scien tific, medical and patient communities that supports the appropriate sharing of
genomic and clinical information in order to maximise its value. He describes the
ethical, legal and regulatory challenges that arise in pursuing this aim. In particular,
he notes the selfish silos which shape research agendas and researchers’ behaviour.
By way of practical solutions that will benefit patients, researchers and society, he
details the work of the Global Alliance for Genomics and Health, a worldwide
coalition of researchers, healthcare professionals, patients and industry partners,
which is developing innovative solutions to support the responsible and effective
sharing of genomic and clinical data.
Anna Middleton’s contribution supports this Global Alliance for Genomics and
Health policy agenda by charting the personal dimension of personalised medicine
through social scientific work on the attitudes, values and beliefs of those who are
affected by technology in the genomics arena. In short, the public are asked ‘how
is the technology working for you?’ To this end, the rationale for an international
attitude study, Your DNA, Your Say, is introduced together with an overview of
the survey and film design. The project uses film to provide background information
and an online survey to gather public views on donating one’s own personal DNA
and medical data for use by others.
Jonathan Montgomery argues at an angle which acknowledges but cuts directly
across the presuppositions of most thinking in this area including that which
shapes the policy approaches of the previous two articles. He notes the use of
ideas about ownership in discussions of data sharing in personalised medicine. Personal health data are thought by many to be ‘theirs’. But paradoxically, person alised medicine (at least in the context of genomics) relies on the aggregation of
private data into a dataset that is held as a form of knowledge commons. This
leads him to explore notions of private and common property that lie behind dis course about data ownership. Having made these explicit, Montgomery is able to
use justifications and jurisprudence of property both to clarify the persuasiveness
and limits of such claims, and also how they differ from other principles that are
at stake in the interplay between individual and collective goods in the delivery of
personalised medicine. His provocative conclusion is that ownership might more
plausibly lie with health professionals than patients. However, in a socialised medi cine system such as the NHS, such professionals are agents of the state and owner ship would lie with the commons rather than any individual. This means that
common rather than private ownership of genomic information may be more
appropriate.
If Montgomery’s line of analysis is correct, then the normal public language and
legal situation regarding genomic data about persons is significantly less plausible
than it might at first seem. Middleton’s and Lawler’s approaches deal with questions
of attitudes and values and, for Lawler at least, the shaping power of the law on the
environment for data sharing, especially on an international level. But following
Montgomery’s argument would have widespread implications for both Middleton’s
and Lawler’s concerns not least by challenging the widespread public belief in ‘my
data’ and, in due course, laws regarding data theft. On Montgomery’s view, it is
plausible to say that the problem of public suspicion about sharing data might be
overcome by a policy which downplayed the idea of data donation and recognised
instead the co-produced nature of the data itself: that although it is still data ‘about
me’ it is properly speaking ‘our data’. This might inspire greater public participation
in data sharing and engagement in its use which would in turn shape the way in
which precision medicine can proceed.
Whatever one makes of Montgomery’s provocative argument, this set of articles
raises broader policy questions: does the idea of co-production really represent a
plausible way of understanding the ownership of personal, genomic information;
what can be done to enable participation in data sharing, clinical trials and research
to be meaningful with respect to the values of participants; what moral arguments
need to accompany legal provisions to enable relevant institutions and individual
scientists to be sharing information better; and finally, what might need to happen
in terms of legal reform to enable progress to be made towards a more plausible
public approach to the ownership and deployment of genomic data?
Value, equity and power
In this final section, papers consider questions of value, equity and power in a time
when pathways to personalised medicine are being forged both in the UK and
abroad. ‘Value’ is used here primarily in the sense of economic value as distinct
from the discussions of ‘values’ and beliefs which appears in papers in earlier sections.
While there is an intuitive attractiveness to the idea that ‘4P’ medicine will be more
cost-effective, there remain questions about affordability, ensuring equality of access, the participatory role of patients, the (re-)alignment of commissioning strategy
and duties of justice towards neighbours, particularly in low- and middle-income
countries. A risk attendant on the development of personalised medicine is that of
social stratification based on ability to pay which in certain circumstances may
entrench or encourage the corruption of power. What does the pursuit of value
mean when both national and global questions of equity are in view?
Muir Gray, Tyra Lagerberg and Viktor Dombrádi note that while precision medi cine through genomic technologies carries huge potential in the treatment of many
diseases, particularly those with a high-penetrance monogenic underpinning, it also
has ethical implications, particularly with regard to value and equity in healthcare.
They define allocative, personal, and technical value (‘triple value’) in healthcare and
how it relates to equity, before discussing key issues surrounding precision medicine
from this perspective. Equity is taken to be implicit in the concept of triple value in
countries that have publicly funded healthcare systems. They argue that precision
medicine may risk concentrating resources on those who already experience
greater access to healthcare and power both at the level of the national society as
well as at the global level. This places particular responsibility in the hands of health care payers, who need to allocate healthcare resources appropriately based on
case-by-case estimates of the value of different genomic technologies. However, clin icians and patients also need to be involved in optimising the undeniable potential of
precision medicine in the healthcare system without reducing equity. Throughout,
their discussion refers to the framework of the NHS RightCare Programme,
which is a national initiative aiming to improve value and equity in the context of
NHS England.
Richard Sullivan and Bishal Gyawali attend primarily to the international and
global questions of equity and value. They argue that the value that new cancer
drugs add is very debatable. Because of the skyrocketing cost of the new drugs,
each new approval represents a multibillion market. However, unlike other
branches of economics, cancer drugs are intricately associated with socio political issues, emotional overlay, public pressure, industry manipulation and
propaganda. They review the value added by new cancer drugs and examine
the socio-political agenda around them with highlights on the increasing gulf
between high-income and low-middle income countries regarding the affordabil ity of these drugs. Finally, they suggest a way forward to address this highly
complex issue.
These articles together raise up the agenda hard questions on a national or inter national level for those who have committed their professional careers or personal
hopes to personalised medicine. The threats of inequity and poor value healthcare
for those least able to stand up for themselves attend the ‘advance’ of personalised
medicine. This raises the question of what responsibility medical researchers actually
have for national and global inequity when control typically lies in the hands of gov ernments, commissioners and payers. To put matters starkly, is personalised medi cine potentially a Trojan horse for a neoliberal research agenda which will only
be available for the rich in many countries? Such a development might put at risk
the socio-political ethos of risk-pooling which underpins the vision of solidarity in
healthcare.

Conclusion: ‘molecularly unstratified’ patients
The aim of this issue of The New Bioethics has been to provoke discourse and
debate, to identify aspirations which are more grounded in myth or hype than
reality and to challenge them; and to identify focussed, practical questions which
need further examination.
A key concern that emerges in this issue through the disciplines of theology, social
science and medical science revolves around ‘molecularly unstratified’ patients,
those who will not be assigned to receive novel, precision treatments. Paying atten tion to these patients is a next logical step in scrutinising the distinction between
hype and reality. What moral, psycho-social and societal issues are important in
relation to such patients and how might they be best addressed?
The clinical significance of being ‘molecularly unstratified’ is that hope-laden
therapeutic options are closed down and allocation to non-targeted or standard
therapeutic treatments, or supportive care become the only path. For the unstratified
patient, what remains is a speculative search for genetic or other disease drivers, a
process much further back in the developmental pathway when compared with
current therapeutic intervention, implying many years or decades of waiting
before a therapeutic could emerge.
For such patients, the particular molecular nature of their cancer becomes the
occasion of both feeling and being left out and left behind, bobbing untidily
about in the wake of technological and scientific ‘advance’. The deflation of
hope among, and the difficulty of identifying next treatment steps for, this
group of persons may have the side-effect of a kind of psycho-social distinction
between unclassified persons and those ‘lucky’ enough to be stratified in some
way.
An issue such as this embodies why examining the promise, hype and pitfalls of
personalised medicine is important. Left unexamined, the distinction between the
stratified and the unstratified may become a factor in depressing expectations of per sonalised precision, hitherto powered by a heady mix of promise and hype. If this
experience of being ‘molecularly unstratified’ is not carefully addressed, a dilution
or at least pronounced heterogeneity in the public’s support for stratified approaches
may develop.
This is but one example of why analysis of the promise, hype and pitfalls which sur round ‘personalised medicine’ is needed. Our hope is that this issue of The New
Bioethics lays the groundwork for addressing these and other challenges as they arise

Source: https://www.tandfonline.com/doi/pdf/10.1080/20502877.2017.1314895?needAccess=true
------end---------


------end---------


------end---------


------end---------


------end---------


------end---------
